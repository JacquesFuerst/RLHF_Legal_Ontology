{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tune reward model from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODOs:\n",
    "\n",
    "#TODO: double-check that labels are not somehow misaligned...\n",
    "\n",
    "#TODO set up cuda usage when running training\n",
    "\n",
    "#TODO: find out good lora config based on paper/ other source\n",
    "\n",
    "#TODO: check if you need to plot \n",
    "\n",
    "#TODO:  Edit data given by Roos such that it gives goed/fout for position...\n",
    "\n",
    "1. Using Legal BERT with a regression head to be able to generate continuous rewards --> ask Roos from which paper that was taken\n",
    "\n",
    "2. LoRA learns the position of the low rank adaptation matrix that is needed to finetune a model of a much higher rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports, setup, and global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4 GPU(s) available.\n",
      "CUDA is available. Using GPU: NVIDIA L40S\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "\n",
    "from utils import parse_ratings\n",
    "\n",
    "\n",
    "#TODO: use GPU, not CPU for training, distribute training properly...\n",
    "# import matplotlib.pyplot as plt\n",
    "# import wandb\n",
    "\n",
    "\n",
    "\n",
    "# load thing\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print(\"CUDA is available. Using GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA is not available. Using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEEDBACK_TO_TRAIN_ON = 'feedback_extraction' \n",
    "FEEDBACK_TO_REMOVE = 'feedback_detection'\n",
    "MODEL = \"nlpaueb/legal-bert-base-uncased\" # DTAI-KULeuven/robbert-2023-dutch-large --> use this for Dutch text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset loading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load human data\n",
    "\n",
    "FILE_1 = \"/home/jacques.furst/development/RAG/flintfiller-precondition-rl/data/human_feedback/human_feedback_b2339fe6-2896-43ab-a9c8-24c8aacfbbd1.csv\"\n",
    "FILE_5 = \"/home/jacques.furst/development/RAG/flintfiller-precondition-rl/data/human_feedback/human_feedback_7b177fba-3ddb-465b-9a25-6f4481eeb492.csv\"\n",
    "FILE_7 = \"/home/jacques.furst/development/RAG/flintfiller-precondition-rl/data/human_feedback/human_feedback_ab53866b-7831-4f33-a628-3b6dbf01ead1.csv\"\n",
    "FILE_9 = \"/home/jacques.furst/development/RAG/flintfiller-precondition-rl/data/human_feedback/human_feedback_a8ec999a-935f-476d-ac5c-f328a1288c7c.csv\"\n",
    "FILE_10_1 = \"/home/jacques.furst/development/RAG/flintfiller-precondition-rl/data/human_feedback/human_feedback_e274815b-f334-4f2c-8cda-3788070d4bee.csv\" # 13:20\n",
    "FILE_10_2 = \"/home/jacques.furst/development/RAG/flintfiller-precondition-rl/data/human_feedback/human_feedback_5a066984-b053-4ae4-97ad-675900d79540.csv\" # 14:39\n",
    "\n",
    "\n",
    "# load synthetic data\n",
    "\n",
    "FILE_SYNTH = \"/home/jacques.furst/development/RAG/flintfiller-precondition-rl/data/synthetic_feedback/synthetic_feedback.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataframes\n",
    "\n",
    "df_1 = pd.read_csv(FILE_1, sep=\";\")\n",
    "df_5 = pd.read_csv(FILE_5, sep=\";\")\n",
    "df_7 = pd.read_csv(FILE_7, sep=\";\")\n",
    "df_9 = pd.read_csv(FILE_9, sep=\";\")\n",
    "df_10_1 = pd.read_csv(FILE_10_1, sep=\";\")\n",
    "df_10_2 = pd.read_csv(FILE_10_2, sep=\";\")\n",
    "df_synth = pd.read_csv(FILE_SYNTH, sep=\";\")\n",
    "\n",
    "df_human = pd.concat([df_1, df_5, df_7, df_9, df_10_1, df_10_2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(929, 13)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = df_human\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. a) Parse ratings to numeric values for MSE Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO: how to parse the ratings properly here, discuss with Roos\n",
    "\n",
    "# def parse_ratings(rating):\n",
    "#     rating_dict = {\n",
    "#         \"Volledig fout\": 0,\n",
    "#         \"Grotendeels fout\": 1,\n",
    "#         \"Grotendeels correct\": 2,\n",
    "#         \"Volledig correct\": 3,\n",
    "#         \"Bestaat niet in ground truth\": 0, #TODO: change\n",
    "#         \"Fout\": 1,\n",
    "#         \"Goed\": 2,\n",
    "#     }\n",
    "#     return rating_dict.get(rating, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# queries = df['query'].tolist()\n",
    "# answers = df['answer'].tolist()\n",
    "# feedback_1 = df['feedback_extraction'].tolist()\n",
    "# feedback_2 = df['feedback_detection'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed feedback for extraction: 0    2\n",
      "1    2\n",
      "2    2\n",
      "3    2\n",
      "4    3\n",
      "Name: feedback_extraction, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df_train[FEEDBACK_TO_TRAIN_ON] = [parse_ratings(feedback) for feedback in df_train[FEEDBACK_TO_TRAIN_ON]]\n",
    "print(\"Parsed feedback for extraction:\", df_train[FEEDBACK_TO_TRAIN_ON][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. b) keep only relevant feedback column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['file', 'frame_ID', 'frame_type', 'frame_text', 'precondition_id', 'precondition_text', 'precondition_position', 'response_text', 'prompt_config_examples', 'prompt_config_chain_of_thought', 'feedback_extraction', 'feedback_detection', 'additional_feedback'],\n",
      "    num_rows: 929\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset.from_pandas(df_train)\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.remove_columns([FEEDBACK_TO_REMOVE])\n",
    "dataset = dataset.rename_column(FEEDBACK_TO_TRAIN_ON, \"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load model with LoRA layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at nlpaueb/legal-bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSdpaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Load the model and the tokenizer\n",
    "model_id = MODEL \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_id, num_labels=1) # num_labels = 1 since we want to prodict a single scalar (the rating)\n",
    "\n",
    "# Comment: Automodel for sequence classification with num_labels=1 already has a regression head\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 443,137 || all params: 109,926,146 || trainable%: 0.4031\n"
     ]
    }
   ],
   "source": [
    "# Define LoRA config\n",
    "#TODO: find out which lora config makes sense for me\n",
    "lora_config = LoraConfig(\n",
    "    r=8,           # Rank of the LoRA matrices (smaller = less memory)\n",
    "    lora_alpha=16, # Scaling factor (higher = stronger adaptation)\n",
    "    target_modules=[\"query\", \"key\", \"value\"], # Apply LoRA to attention layers\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_CLS\"  # classify each answer \n",
    ")\n",
    "\n",
    "# Convert the model to a PEFT (LoRA) model\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()  # Check trainable params (~0.1% of full model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 1067, 223, 207, 580, 210, 1335, 124, 102, 0, 0, 0], [101, 1067, 223, 207, 5601, 190, 580, 213, 207, 1727, 124, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_data = [\"What is the capital of France?\", \"What is the largest capital in the world?\"]\n",
    "tokenizer(sample_data, padding=True, truncation=True, max_length=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Custom Trainer to be used for the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRewardTrainer(Trainer):\n",
    "    def __init__(self, *args, loss_type=\"mse\", weight_strategy=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.loss_type = loss_type  # \"mse\", \"huber\", or custom\n",
    "        self.weight_strategy = weight_strategy  # \"linear\", \"inverse\", or None\n",
    "\n",
    "    def compute_loss(self, model, inputs, num_items_in_batch=None, return_outputs=False):\n",
    "        # Extract labels (ratings) and optional sample weights\n",
    "        labels = inputs.pop(\"labels\").float()  # Shape: (batch_size)\n",
    "        \n",
    "        # Optional: Compute sample weights dynamically\n",
    "        weights = self._get_sample_weights(labels) if self.weight_strategy else None\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits.squeeze()  # Shape: (batch_size) --> logits are the predicted rewards in this case\n",
    "        \n",
    "        # Custom loss calculation\n",
    "        loss = self._compute_custom_loss(logits, labels, weights)\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "    def _compute_custom_loss(self, logits, labels, weights=None):\n",
    "        if self.loss_type == \"mse\":\n",
    "            loss = F.mse_loss(logits, labels, reduction=\"none\") # --> MSE provides precise regression BUT sensitive to outliers\n",
    "        elif self.loss_type == \"huber\":\n",
    "            loss = F.huber_loss(logits, labels, reduction=\"none\", delta=1.0) #--> balances between MSE and MAE for data that has outliers/ noise\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported loss type: {self.loss_type}\")\n",
    "\n",
    "        # Apply sample weights if provided\n",
    "        if weights is not None:\n",
    "            loss = loss * weights\n",
    "            loss = loss.mean()  # Normalize by mean if weights are unnormalized\n",
    "        else:\n",
    "            loss = loss.mean()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def _get_sample_weights(self, labels):\n",
    "        \"\"\"\n",
    "        Generate sample weights based on rating values.\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        if self.weight_strategy == \"linear\":\n",
    "            # Linear weighting (e.g., emphasize extremes)\n",
    "            weights = torch.abs(labels - labels.mean()) + 1.0\n",
    "        elif self.weight_strategy == \"inverse\":\n",
    "            # Inverse frequency weighting (if ratings are skewed)\n",
    "            unique, counts = torch.unique(labels, return_counts=True)\n",
    "            freq = counts.float() / len(labels)\n",
    "            weight_map = 1.0 / (freq + 1e-6)  # Avoid division by zero\n",
    "            weights = torch.tensor([weight_map[(unique == lbl).nonzero().item()] for lbl in labels])\n",
    "        else:\n",
    "            weights = None\n",
    "        \n",
    "        return weights.to(labels.device) if weights is not None else None\n",
    "\n",
    "\n",
    "\n",
    "    def compute_metrics(self, eval_preds):\n",
    "        predictions, labels = eval_preds\n",
    "        predictions = predictions.squeeze()\n",
    "        \n",
    "        # Regression metrics\n",
    "        mse = mean_squared_error(labels, predictions)\n",
    "        pearson = pearsonr(labels, predictions)[0] # Pearson correlation coefficient\n",
    "        \n",
    "        # Threshold accuracy --> \n",
    "        tolerance_acc = (np.abs(predictions - labels) <= 0.5).mean()\n",
    "        \n",
    "        return {\"mse\": mse, \"pearson\": pearson, \"tolerance_acc\": tolerance_acc}\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    #TODO: evaluate whether the plotting should be done or whether it is redundant to add them\n",
    "\n",
    "    # def evaluation_loop(self, *args, **kwargs):\n",
    "    #     output = super().evaluation_loop(*args, **kwargs)\n",
    "    #     predictions = output.predictions.squeeze()\n",
    "    #     labels = output.label_ids\n",
    "        \n",
    "    #     # Generate plots (saved to disk or logged to W&B)\n",
    "    #     plot_distributions(predictions, labels, self.state.epoch)\n",
    "    #     plot_calibration(predictions, labels)\n",
    "        \n",
    "    #     return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: debug if this is truly needed...\n",
    "\n",
    "# add distributioncallback to trianing to evaluate \n",
    "\n",
    "# class DistributionCallback(TrainerCallback):\n",
    "#     def on_evaluate(self, args, state, control, metrics, **kwargs):\n",
    "#         # Get predictions and labels from the trainer's eval loop\n",
    "#         eval_results = trainer.evaluate()\n",
    "#         predictions = eval_results[\"eval_predictions\"]\n",
    "#         labels = eval_results[\"eval_labels\"]\n",
    "        \n",
    "#         # Log histogram to W&B\n",
    "#         wandb.log({\n",
    "#             \"reward_histogram\": wandb.Histogram(predictions),\n",
    "#             \"true_ratings_histogram\": wandb.Histogram(labels),\n",
    "#         })\n",
    "\n",
    "\n",
    "# def plot_distributions(predictions, labels, epoch):\n",
    "#     plt.figure(figsize=(10, 4))\n",
    "#     plt.subplot(1, 2, 1)\n",
    "#     plt.hist(predictions, bins=20, alpha=0.7, label=\"Predicted\")\n",
    "#     plt.title(\"Predicted Rewards\")\n",
    "    \n",
    "#     plt.subplot(1, 2, 2)\n",
    "#     plt.hist(labels, bins=20, alpha=0.7, label=\"True Ratings\", color=\"orange\")\n",
    "#     plt.title(\"True Ratings\")\n",
    "    \n",
    "#     plt.savefig(f\"distributions_epoch_{epoch}.png\")\n",
    "#     plt.close()\n",
    "\n",
    "# class PlotCallback(TrainerCallback):\n",
    "#     def on_evaluate(self, args, state, control, **kwargs):\n",
    "#         predictions = trainer.predict(test_dataset).predictions.squeeze()\n",
    "#         labels = test_dataset[\"ratings\"]\n",
    "#         plot_distributions(predictions, labels, state.epoch)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def plot_calibration(predictions, labels):\n",
    "#     \"\"\"\n",
    "#     Function to check if the sdfs\n",
    "\n",
    "    \n",
    "\n",
    "#     \"\"\"\n",
    "#     bin_means = np.linspace(1, 5, num=5)  # For 1-5 ratings\n",
    "#     bin_centers = []\n",
    "#     empirical_means = []\n",
    "    \n",
    "#     for i in range(len(bin_means) - 1):\n",
    "#         mask = (labels >= bin_means[i]) & (labels < bin_means[i+1])\n",
    "#         if mask.sum() > 0:\n",
    "#             bin_centers.append((bin_means[i] + bin_means[i+1]) / 2)\n",
    "#             empirical_means.append(predictions[mask].mean())\n",
    "    \n",
    "#     plt.plot(bin_centers, empirical_means, marker=\"o\")\n",
    "#     plt.plot([1, 5], [1, 5], linestyle=\"--\", color=\"gray\")  # Ideal line\n",
    "#     plt.xlabel(\"True Rating\")\n",
    "#     plt.ylabel(\"Predicted Reward\")\n",
    "#     plt.savefig(\"calibration_plot.png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. a) Define a custom data collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import DefaultDataCollator\n",
    "\n",
    "# #TODO: only do this if the labels fix does not work for some reason\n",
    "\n",
    "# class RewardDataCollator(DefaultDataCollator):\n",
    "#     def __call__(self, features):\n",
    "\n",
    "#         ratings = [f.pop(\"rating\") for f in features]  # Removes rating from features temporarily\n",
    "#         batch = super().__call__(features)\n",
    "#         # Explicitly ensure rating is included\n",
    "#         print(features)\n",
    "#         # Re-inject ratings into the batch\n",
    "#         batch[\"rating\"] = torch.tensor(ratings, dtype=torch.float32)\n",
    "#         return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Encode dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m     data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdataset\u001b[49m\u001b[38;5;241m.\u001b[39mcolumn_names)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# mao string labels to integers\u001b[39;00m\n\u001b[1;32m     11\u001b[0m dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mmap(convert_label_to_int)  \u001b[38;5;66;03m# Assuming 'text' is the column with the text data\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# TODO: combine all the relevant points from the dataframe\n",
    "\n",
    "\n",
    "def convert_label_to_int(data):\n",
    "    data[\"label\"] = int(data[\"label\"])\n",
    "    return data\n",
    "\n",
    "\n",
    "print(dataset.column_names)\n",
    "# mao string labels to integers\n",
    "dataset = dataset.map(convert_label_to_int)  # Assuming 'text' is the column with the text data\n",
    "\n",
    "print(dataset[\"label\"][:5])  # Check labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comment\n",
    "\n",
    "1. Needed for feedback extraction: precondition_text, response_text, label(rating feedback extraction)\n",
    "2. Needed for feedback detection: precondition_text, precondition_position, response_text, label (rating feedback detection)\n",
    "3. For the precondition position to be found well, it is a crucial for the model to find the precondition text (at least to a recognizable degree) as well, otherwise the precondition is not found at all..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 929/929 [00:00<00:00, 2924.68 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# tokenize queries and answers together to provide proper context to reward model\n",
    "\n",
    "#TODO: get proper columns from the \n",
    "def tokenize_fn(examples):\n",
    "    combined_texts = [f\"{p} {a} {r}\" for p, a, r in zip(examples[\"precondition_text\"], examples[\"response_text\"], examples[\"label\"])]\n",
    "    return tokenizer(combined_texts, truncation=True, padding=\"max_length\")\n",
    "\n",
    "dataset = dataset.map(tokenize_fn, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['file', 'frame_ID', 'frame_type', 'frame_text', 'precondition_id', 'precondition_text', 'precondition_position', 'response_text', 'prompt_config_examples', 'prompt_config_chain_of_thought', 'label', 'additional_feedback', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 929\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split dataset into train, test, eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: do split into train, test and eval sets\n",
    "train_test_split = dataset.train_test_split(test_size=0.3, seed=42)\n",
    "eval_test_split = train_test_split[\"test\"].train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "\n",
    "final_splits = DatasetDict({\n",
    "    'train': train_test_split['train'],\n",
    "    'validation': eval_test_split['train'],\n",
    "    'test': eval_test_split['test']\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train reward model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./lora_checkpoints\",\n",
    "    # evaluation_strategy='steps'\n",
    "    save_strategy='steps',\n",
    "    save_steps=100,\n",
    "    # eval_steps=100,\n",
    "    save_total_limit=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    learning_rate=3e-4,\n",
    "    num_train_epochs=3,\n",
    "    logging_steps=10,\n",
    "    label_names=[\"labels\"],\n",
    "    # report_to=\"none\",\n",
    "    logging_dir=\"./logs\",\n",
    "    \n",
    "    # report_to='wandb',\n",
    ")\n",
    "\n",
    "# Initialize custom trainer\n",
    "trainer = CustomRewardTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    processing_class=tokenizer,\n",
    "    loss_type=\"huber\",  # Try \"mse\" or \"huber\"\n",
    "    weight_strategy=\"linear\",  # Try \"linear\", \"inverse\", or None\n",
    "    # data_collator=RewardDataCollator()\n",
    ")\n",
    "\n",
    "# # add distributioncallback to trainer TODO: only integrate if relevant\n",
    "# trainer.add_callback(DistributionCallback())\n",
    "\n",
    "#train model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store final model parameters\n",
    "model.save_pretrained(\"final_lora_adapters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reload saved LoRA adapter for inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "model = PeftModel.from_pretrained(base_model, \"final_lora_adapters\")\n",
    "model = model.merge_and_unload()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
