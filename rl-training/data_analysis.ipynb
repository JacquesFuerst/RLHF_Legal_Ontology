{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd01ea27",
   "metadata": {},
   "source": [
    "# Data cleanup notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae5f442",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab85d869",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchmetrics.nominal import FleissKappa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from utils import parse_ratings, count_categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4514bf",
   "metadata": {},
   "source": [
    "## 2. Clean up messed up files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bac294e",
   "metadata": {},
   "source": [
    "### Load files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc34f4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Participant 2\n",
    "\n",
    "first_file_2 = \"/home/jacques.furst/development/RAG/flintfiller-precondition-rl/data/human_feedback/human_feedback_7a185ca4-3d37-4487-8aa7-ad9e8f6fe884.csv\" # 13:10\n",
    "second_file_2 = \"/home/jacques.furst/development/RAG/flintfiller-precondition-rl/data/human_feedback/human_feedback_861cdaee-9f2e-4454-8252-d8ff397eb14e.csv\" # 15:48\n",
    "third_file_2 = \"/home/jacques.furst/development/RAG/flintfiller-precondition-rl/data/human_feedback/human_feedback_f60e0e84-2638-44da-871b-4847b751fabb.csv\" # 16:23\n",
    "\n",
    "\n",
    "# Participant 6\n",
    "\n",
    "first_file_6 = \"/home/jacques.furst/development/RAG/flintfiller-precondition-rl/data/human_feedback/human_feedback_48d1eb99-5d33-476a-a1a4-75917aa92e92.csv\" #12:21\n",
    "second_file_6 = \"/home/jacques.furst/development/RAG/flintfiller-precondition-rl/data/human_feedback/human_feedback_b3d2cab5-ffca-467d-bddb-b9e188e5a85a.csv\" # 11:09\n",
    "third_file_6 = \"/home/jacques.furst/development/RAG/flintfiller-precondition-rl/data/human_feedback/human_feedback_c2e282a4-f8e7-4542-95ab-a9a74b6f57e0.csv\" # 13:39"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da94d9db",
   "metadata": {},
   "source": [
    "### Sort out files participant 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c67f99ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "281\n"
     ]
    }
   ],
   "source": [
    "df_2_1 = pd.read_csv(first_file_2, sep=\";\")\n",
    "df_2_2 = pd.read_csv(second_file_2, sep=\";\")\n",
    "df_2_3 = pd.read_csv(third_file_2, sep=\";\")\n",
    "\n",
    "df_2_1 = df_2_1[:186]\n",
    "df_2_2 = df_2_2[186:]\n",
    "df_2_3 = df_2_3[30:]\n",
    "\n",
    "df_2 = pd.concat([df_2_1, df_2_2, df_2_3], ignore_index=True)\n",
    "print(len(df_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e13ec52",
   "metadata": {},
   "source": [
    "### Sort out files participant 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b247c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84\n",
      "168\n",
      "114\n",
      "282\n"
     ]
    }
   ],
   "source": [
    "df_6_1 = pd.read_csv(second_file_6, sep=\";\")\n",
    "df_6_2 = pd.read_csv(first_file_6, sep=\";\")\n",
    "df_6_3 = pd.read_csv(third_file_6, sep=\";\")\n",
    "\n",
    "print(len(df_6_1))\n",
    "print(len(df_6_2))\n",
    "print(len(df_6_3))\n",
    "\n",
    "df_6_1 = df_6_1[:]\n",
    "df_6_2 = df_6_2[84:]\n",
    "df_6_3 = df_6_3[:]\n",
    "\n",
    "df_6 = pd.concat([df_6_1, df_6_2, df_6_3], ignore_index=True)\n",
    "print(len(df_6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e037d8",
   "metadata": {},
   "source": [
    "### Check if there are duplicate rows in the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4f67265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "duplicates_6 = df_6.iloc[:, :-2].duplicated()\n",
    "duplicates_2 = df_2.iloc[:, :-2].duplicated()\n",
    "print(duplicates_6.sum())\n",
    "print(duplicates_2.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18eba8f6",
   "metadata": {},
   "source": [
    "## 3. Load all proper files (except for the ones that were already created)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5517e4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_1 = \"/home/jacques.furst/development/RAG/flintfiller-precondition-rl/data/human_feedback/human_feedback_b2339fe6-2896-43ab-a9c8-24c8aacfbbd1.csv\"\n",
    "FILE_2 = None\n",
    "FILE_3 = \"/home/jacques.furst/development/RAG/flintfiller-precondition-rl/data/human_feedback/human_feedback_6430b6fd-0ddd-4cc6-a4a0-216d5603143e.csv\"\n",
    "FILE_4 = \"/home/jacques.furst/development/RAG/flintfiller-precondition-rl/data/human_feedback/human_feedback_5fa87112-3702-4263-ba81-1779b3b24d16.csv\"\n",
    "FILE_5 = \"/home/jacques.furst/development/RAG/flintfiller-precondition-rl/data/human_feedback/human_feedback_7b177fba-3ddb-465b-9a25-6f4481eeb492.csv\"\n",
    "FILE_6 = None\n",
    "FILE_7 = \"/home/jacques.furst/development/RAG/flintfiller-precondition-rl/data/human_feedback/human_feedback_ab53866b-7831-4f33-a628-3b6dbf01ead1.csv\"\n",
    "FILE_8 = \"/home/jacques.furst/development/RAG/flintfiller-precondition-rl/data/human_feedback/human_feedback_b1ed9f35-7d6a-439c-8a46-089311e8e340.csv\"\n",
    "FILE_9 = \"/home/jacques.furst/development/RAG/flintfiller-precondition-rl/data/human_feedback/human_feedback_a8ec999a-935f-476d-ac5c-f328a1288c7c.csv\"\n",
    "FILE_10 = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b127917",
   "metadata": {},
   "source": [
    "### Load all dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1da622b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                               Duidelijk\n",
      "1                               Duidelijk\n",
      "2                               Duidelijk\n",
      "3                               Duidelijk\n",
      "4                               Duidelijk\n",
      "                      ...                \n",
      "136    Onbestemde positie in ground truth\n",
      "137    Onbestemde positie in ground truth\n",
      "138               Helemaal niet duidelijk\n",
      "139               Helemaal niet duidelijk\n",
      "140               Helemaal niet duidelijk\n",
      "Name: feedback_detection, Length: 141, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df_1 = pd.read_csv(FILE_1, sep=\";\")\n",
    "df_3 = pd.read_csv(FILE_3, sep=\";\")\n",
    "df_4 = pd.read_csv(FILE_4, sep=\";\")\n",
    "df_5 = pd.read_csv(FILE_5, sep=\";\")\n",
    "df_7 = pd.read_csv(FILE_7, sep=\";\")\n",
    "df_8 = pd.read_csv(FILE_8, sep=\";\")\n",
    "df_9 = pd.read_csv(FILE_9, sep=\";\")\n",
    "\n",
    "print(df_1['feedback_detection'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4e952f",
   "metadata": {},
   "source": [
    "### Cast ratings to numeric values to use fleiss kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c2a0026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array(['Niet goed'], dtype=object)\n"
     ]
    }
   ],
   "source": [
    "print(repr(df_2.loc[df_2['feedback_detection'].str.contains(\"Niet goed\", na=False), 'feedback_detection'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05ecf0ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# parse ratings in all dataframes to be able to calculate Fleiss kappa\n",
    "def apply_parse_ratings(df, number):\n",
    "    df['feedback_extraction'] = [parse_ratings(feedback) for feedback in df['feedback_extraction']]\n",
    "    # print(number)\n",
    "    # print(df['feedback_detection'])\n",
    "    df['feedback_detection'] = [parse_ratings(feedback_detec) for feedback_detec in df['feedback_detection']]\n",
    "    # print(df['feedback_detection'])\n",
    "    return df\n",
    "\n",
    "# Apply to each DataFrame\n",
    "df1 = apply_parse_ratings(df_1, 1)\n",
    "df2 = apply_parse_ratings(df_2, 2)\n",
    "df3 = apply_parse_ratings(df_3, 3)\n",
    "df4 = apply_parse_ratings(df_4, 4)\n",
    "df5 = apply_parse_ratings(df_5, 5)\n",
    "df6 = apply_parse_ratings(df_6, 6)\n",
    "df7 = apply_parse_ratings(df_7, 7)\n",
    "df8 = apply_parse_ratings(df_8, 8)\n",
    "df9 = apply_parse_ratings(df_9, 9)\n",
    "#df10 = \n",
    "\n",
    "# Check if any feedback column contains Nan values\n",
    "\n",
    "\n",
    "print(df1['feedback_detection'].isna().any())\n",
    "print(df2['feedback_detection'].isna().any())\n",
    "print(df3['feedback_detection'].isna().any())\n",
    "print(df4['feedback_detection'].isna().any())\n",
    "print(df5['feedback_detection'].isna().any())\n",
    "print(df6['feedback_detection'].isna().any())\n",
    "print(df7['feedback_detection'].isna().any())\n",
    "print(df8['feedback_detection'].isna().any())\n",
    "print(df9['feedback_detection'].isna().any())\n",
    "\n",
    "print(df1['feedback_extraction'].isna().any())\n",
    "print(df2['feedback_extraction'].isna().any())\n",
    "print(df3['feedback_extraction'].isna().any())\n",
    "print(df4['feedback_extraction'].isna().any())\n",
    "print(df5['feedback_extraction'].isna().any())\n",
    "print(df6['feedback_extraction'].isna().any())\n",
    "print(df7['feedback_extraction'].isna().any())\n",
    "print(df8['feedback_extraction'].isna().any())\n",
    "print(df9['feedback_extraction'].isna().any())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d031f767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are the first 50 rows of df1 and df2 equal (excluding last 3 columns)? True\n",
      "Are the first 50 rows of df1 and df3 equal (excluding last 3 columns)? True\n",
      "Are the first 50 rows of df1 and df4 equal (excluding last 3 columns)? True\n",
      "Are the first 50 rows of df1 and df5 equal (excluding last 3 columns)? True\n",
      "Are the first 50 rows of df1 and df6 equal (excluding last 3 columns)? True\n",
      "Are the first 50 rows of df1 and df7 equal (excluding last 3 columns)? True\n",
      "Are the first 50 rows of df1 and df8 equal (excluding last 3 columns)? True\n",
      "Are the first 50 rows of df1 and df9 equal (excluding last 3 columns)? True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Collect the first 50 entries from each DataFrame for each column\n",
    "dfs = [df1, df3, df5, df7, df9]\n",
    "# dfs = [df5, df8]\n",
    "\n",
    "\n",
    "# Compare first 50 rows, excluding the last 3 columns\n",
    "subset1 = df1.iloc[:50, :-3]\n",
    "subset2 = df2.iloc[:50, :-3]\n",
    "subset3 = df3.iloc[:50, :-3]\n",
    "subset4 = df4.iloc[:50, :-3]\n",
    "subset5 = df5.iloc[:50, :-3]\n",
    "subset6 = df6.iloc[:50, :-3]\n",
    "subset7 = df7.iloc[:50, :-3]\n",
    "subset8 = df8.iloc[:50, :-3]\n",
    "subset9 = df9.iloc[:50, :-3]\n",
    "\n",
    "# Check if they are equal\n",
    "are_equal_1_2 = subset1.equals(subset2)\n",
    "are_equal_1_3 = subset1.equals(subset3)\n",
    "are_equal_1_4 = subset1.equals(subset4)\n",
    "are_equal_1_5 = subset1.equals(subset5)\n",
    "are_equal_1_6 = subset1.equals(subset6)\n",
    "are_equal_1_7 = subset1.equals(subset7)\n",
    "are_equal_1_8 = subset1.equals(subset8)\n",
    "are_equal_1_9 = subset1.equals(subset9)\n",
    "\n",
    "# Print results of how equal stuff is\n",
    "print(\"Are the first 50 rows of df1 and df2 equal (excluding last 3 columns)?\", are_equal_1_2)\n",
    "print(\"Are the first 50 rows of df1 and df3 equal (excluding last 3 columns)?\", are_equal_1_3)\n",
    "print(\"Are the first 50 rows of df1 and df4 equal (excluding last 3 columns)?\", are_equal_1_4)\n",
    "print(\"Are the first 50 rows of df1 and df5 equal (excluding last 3 columns)?\", are_equal_1_5)\n",
    "print(\"Are the first 50 rows of df1 and df6 equal (excluding last 3 columns)?\", are_equal_1_6)\n",
    "print(\"Are the first 50 rows of df1 and df7 equal (excluding last 3 columns)?\", are_equal_1_7)\n",
    "print(\"Are the first 50 rows of df1 and df8 equal (excluding last 3 columns)?\", are_equal_1_8)\n",
    "print(\"Are the first 50 rows of df1 and df9 equal (excluding last 3 columns)?\", are_equal_1_9)\n",
    "\n",
    "# Helper function to extract and stack ratings\n",
    "def prepare_data(dfs, column):\n",
    "    # print(column)\n",
    "    data = [df[column].iloc[:50].tolist() for df in dfs]\n",
    "\n",
    "    # print(data)\n",
    "    return torch.tensor(list(zip(*data))) # shape: (50 items, 8 raters)\n",
    "\n",
    "# Prepare data\n",
    "extraction_tensor = prepare_data(dfs, 'feedback_extraction')\n",
    "detection_tensor = prepare_data(dfs, 'feedback_detection')\n",
    "\n",
    "# couont categories to pass into fleiss kappa\n",
    "extraction_categories = [0,1,2,3]\n",
    "detection_categories = [4,5,6]\n",
    "\n",
    "categories_count_extraction = count_categories(extraction_tensor, extraction_categories)\n",
    "categories_count_detection = count_categories(detection_tensor, detection_categories)\n",
    "\n",
    "# print(\"Extraction tensor shape:\", extraction_tensor.shape)\n",
    "# print(\"Etraction tensor:\", extraction_tensor)\n",
    "\n",
    "# print(\"Detection tensor shape:\", detection_tensor.shape)\n",
    "# print(\"Detection tensor:\", detection_tensor)\n",
    "\n",
    "# print(extraction_tensor.isnan().any())\n",
    "# print(detection_tensor.isnan().any())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29a107a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fleiss' Kappa for feedback_extraction: tensor(0.4586)\n",
      "Fleiss' Kappa for feedback_detection: tensor(0.3678)\n"
     ]
    }
   ],
   "source": [
    "# Determine number of classes (assuming all ratings are integers starting from 0 or 1)\n",
    "num_classes_extraction = len(set(extraction_tensor.flatten().tolist()))\n",
    "num_classes_detection = len(set(detection_tensor.flatten().tolist()))\n",
    "\n",
    "# Compute Fleiss' Kappa\n",
    "# TODO: data structure seems fine, but check that you have taken the ame data for every person\n",
    "\n",
    "\n",
    "kappa_extraction = FleissKappa(mode='counts')\n",
    "kappa_detection = FleissKappa(mode='counts')\n",
    "\n",
    "print(\"Fleiss' Kappa for feedback_extraction:\", kappa_extraction(categories_count_extraction))\n",
    "print(\"Fleiss' Kappa for feedback_detection:\", kappa_detection(categories_count_detection))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_cleanup",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
