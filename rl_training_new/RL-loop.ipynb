{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2.7.1+cu126\n",
      "12.6\n",
      "90501\n",
      "True\n",
      "There are 1 GPU(s) available.\n",
      "CUDA is available. Using GPU: NVIDIA L40S\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForCausalLM\n",
    "from trl import PPOConfig, create_reference_model, AutoModelForCausalLMWithValueHead\n",
    "from trl import GRPOTrainer, GRPOConfig\n",
    "from datasets import DatasetDict\n",
    "\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from utils import CustomRewardFunction, LabelPreservingCollator #, ValueHeadModel\n",
    "from ppo_trainer_custom import CustomPPOTrainer\n",
    "import pandas as pd\n",
    "# import sys\n",
    "# import wandb\n",
    "\n",
    "# # Add the parent directory to the Python path\n",
    "# sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# load the relevant devices available on the server\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = os.getenv(\"AVAILABLE_DEVICES\")\n",
    "\n",
    "# Enable expandable CUDA segments\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "print(os.environ[\"CUDA_VISIBLE_DEVICES\"])\n",
    "\n",
    "# import torch\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(torch.backends.cudnn.version())\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "# load cuda\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print(\"CUDA is available. Using GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA is not available. Using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = os.getenv(\"GENERATION_MODEL_NAME\")\n",
    "ALGORITHM = os.getenv(\"RL_ALGORITHM\")\n",
    "REWARD_MODEL = os.getenv(\"REWARD_MODEL_NAME\")\n",
    "REWARD_MODEL_EXTRACTION_LORA = os.getenv(\"REWARD_MODEL_EXTRACTION_LORA\")\n",
    "REWARD_MODEL_DETECTION_LORA = os.getenv(\"REWARD_MODEL_DETECTION_LORA\")\n",
    "RL_TOKENIZATION = \"best_window\"\n",
    "MAX_LENGTH = int(os.getenv(\"RL_MAX_LENGTH\"))\n",
    "STRIDE = int(os.getenv(\"RL_STRIDE\"))\n",
    "PROMPT_DATASET = os.getenv(\"PROMPT_DATASET_CSV\")\n",
    "DETECTION_DIFFERENCE = int(os.getenv(\"DETECTION_DIFFERENCE\"))\n",
    "WEIGHT_EXTRACTION = float(os.getenv(\"WEIGHT_EXTRACTION\"))\n",
    "WEIGHT_DETECTION = float(os.getenv(\"WEIGHT_DETECTION\"))\n",
    "RL_TRAINING_FILES = os.getenv(\"RL_TRAINING_FILES\") + \"_\" + ALGORITHM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load prompt dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_df = pd.read_csv(PROMPT_DATASET, sep=\";\")\n",
    "dataset = Dataset.from_pandas(prompt_df)\n",
    "\n",
    "#TODO: test whether everything is well-separated\n",
    "train_test_split = dataset.train_test_split(test_size=0.3, seed=42)\n",
    "eval_test_split = train_test_split[\"test\"].train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "\n",
    "final_splits = DatasetDict({\n",
    "    'train': train_test_split['train'],\n",
    "    'validation': eval_test_split['train'],\n",
    "    'test': eval_test_split['test']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['prompt', 'precondition_texts', 'precondition_positions'], dtype='object')\n",
      "26\n",
      "['prompt', 'precondition_texts', 'precondition_positions']\n"
     ]
    }
   ],
   "source": [
    "print(prompt_df.columns)\n",
    "print(len(prompt_df))\n",
    "print(dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: do train test eval split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset columns\n",
    "\n",
    "1. prompt\n",
    "2. precondition_text_dict --> key: id, value: text \n",
    "3. precondition_position_dict --> key: id, value: position\n",
    "\n",
    "Think about whether any other components are needed...\n",
    "Need to iterate through all preconditions to get reward, or return several rewards per response and finetune model on each one --> iteration should be good enough, just need to define reward function properly for this..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00, 53.89it/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at nlpaueb/legal-bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load base model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL,  \n",
    "                                             device_map=\"auto\",  # For GPU/TPU acceleration\n",
    "                                             torch_dtype=\"auto\")   # Optimize precision)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load reward model feedback extraction\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(REWARD_MODEL, num_labels=1)\n",
    "reward_tokenizer = AutoTokenizer.from_pretrained(REWARD_MODEL)\n",
    "\n",
    "extraction_model = PeftModel.from_pretrained(base_model, REWARD_MODEL_EXTRACTION_LORA).to(device)\n",
    "# extraction_model = extraction_model.merge_and_unload()\n",
    "\n",
    "detection_model = PeftModel.from_pretrained(base_model, REWARD_MODEL_DETECTION_LORA).to(device)\n",
    "# detection_model = detection_model.merge_and_unload()\n",
    "\n",
    "\n",
    "# Create the custom reward function\n",
    "reward_function = CustomRewardFunction(extraction_model, detection_model, reward_tokenizer, MAX_LENGTH, STRIDE, RL_TOKENIZATION, device, weight_extraction=WEIGHT_EXTRACTION, weight_detection=WEIGHT_DETECTION, detection_difference=DETECTION_DIFFERENCE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoRA config and wnadb init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type='CAUSAL_LM',  \n",
    ")\n",
    "\n",
    "# wandb.init(project=\"RL-preconditions\", name=\"grpo-run-test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRPO Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ALGORITHM == \"GRPO\":\n",
    "\n",
    "    training_args = GRPOConfig(\n",
    "        output_dir=RL_TRAINING_FILES, \n",
    "        logging_steps=1, \n",
    "        gradient_checkpointing=True,\n",
    "        learning_rate=1e-5,\n",
    "        num_train_epochs=10,\n",
    "        weight_decay=0.01,\n",
    "        warmup_steps=5, # TODO:check if this makes any sense at all\n",
    "        logging_dir=\"logs\",\n",
    "        # save_steps=1,\n",
    "        # save_total_limit=2,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        # eval_steps=1,\n",
    "        # batch_size=2,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        gradient_accumulation_steps=3, #TODO: think about whether this is truly necessary\n",
    "        report_to=\"wandb\",\n",
    "        )\n",
    "\n",
    "    # Initialize GRPO trainer\n",
    "    trainer = GRPOTrainer(\n",
    "        model=model,\n",
    "        reward_funcs=reward_function,\n",
    "        train_dataset=final_splits['train'],\n",
    "        eval_dataset=final_splits['validation'],\n",
    "        args=training_args,\n",
    "        # **grpo_config\n",
    "        peft_config=lora_config\n",
    "    )\n",
    "\n",
    "#TODO: maybe get a learning rate scheduler for this...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code from Huggingface TRL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes:\n",
    "\n",
    "1. Do not use SLURM since I am in a single-node multi GPU setting and SLURM would work with scheduled training on a multi node cluster... --> use accelerate instead\n",
    "\n",
    "2. install transformers accelerate deepspeed trl\n",
    "\n",
    "3. DAPO paper for some hyperparameter settings, DeepSeekMath paper for hyperparametersettings is good as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: initialize proper weights here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 26/26 [00:00<00:00, 916.53 examples/s]\n",
      "Map: 100%|██████████| 26/26 [00:00<00:00, 5012.50 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask', 'additional_entries'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#TODO: Prepare dataset here\n",
    "# Need to tokenize to use for PPO\n",
    "\n",
    "def tokenize_and_keep_original(example):\n",
    "    # Tokenize the \"text\" column\n",
    "    tokenized = tokenizer(example[\"prompt\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "    # Keep the original text\n",
    "    # tokenized[\"original_text\"] = example[\"prompt\"]\n",
    "    return tokenized\n",
    "\n",
    "# Apply the function to the dataset\n",
    "tokenized_dataset = dataset.map(tokenize_and_keep_original, batched=True)\n",
    "\n",
    "# Create label column for this to be handled properly in PPO Trainer\n",
    "def create_label(example):\n",
    "    return {\"additional_entries\": (example[\"prompt\"], example[\"precondition_texts\"], example[\"precondition_positions\"])}\n",
    "\n",
    "tokenized_dataset = tokenized_dataset.map(create_label)\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([\"prompt\", \"precondition_texts\", \"precondition_positions\"])\n",
    "print(tokenized_dataset[0].keys())\n",
    "# print(tokenized_dataset[0][\"prompt\"])\n",
    "\n",
    "#TODO: do train test split on this\n",
    "\n",
    "#TODO: test whether everything is well-separated\n",
    "train_test_split_PPO = tokenized_dataset.train_test_split(test_size=0.3, seed=42)\n",
    "eval_test_split_PPO = train_test_split_PPO[\"test\"].train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "\n",
    "final_splits_PPO = DatasetDict({\n",
    "    'train': train_test_split_PPO['train'],\n",
    "    'validation': eval_test_split_PPO['train'],\n",
    "    'test': eval_test_split_PPO['test']\n",
    "})\n",
    "\n",
    "# use own data collator that does not pad label column\n",
    "data_collator = LabelPreservingCollator(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value model and ref_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:01<00:00,  1.97it/s]\n",
      "WARNING:root:A <class 'peft.peft_model.PeftModelForCausalLM'> model is loaded from 'mistralai/Mistral-7B-Instruct-v0.3', and no v_head weight is found. This IS expected if you are not resuming PPO training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): MistralForCausalLM(\n",
      "      (model): MistralModel(\n",
      "        (embed_tokens): Embedding(32768, 4096)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x MistralDecoderLayer(\n",
      "            (self_attn): MistralAttention(\n",
      "              (q_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "              (v_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "            )\n",
      "            (mlp): MistralMLP(\n",
      "              (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "              (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "              (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "            (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (rotary_emb): MistralRotaryEmbedding()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=4096, out_features=32768, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "<class 'trl.models.modeling_value_head.AutoModelForCausalLMWithValueHead'>\n",
      "True\n",
      "['T_destination', '__annotations__', '__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_apply', '_backward_hooks', '_backward_pre_hooks', '_buffers', '_call_impl', '_compiled_call_impl', '_forward_hooks', '_forward_hooks_always_called', '_forward_hooks_with_kwargs', '_forward_pre_hooks', '_forward_pre_hooks_with_kwargs', '_get_backward_hooks', '_get_backward_pre_hooks', '_get_checkpoint_from_hub', '_get_current_device', '_get_name', '_init_weights', '_is_full_backward_hook', '_load_from_state_dict', '_load_state_dict_post_hooks', '_load_state_dict_pre_hooks', '_maybe_warn_non_full_backward_hook', '_modules', '_named_members', '_non_persistent_buffers_set', '_parameters', '_register_load_state_dict_pre_hook', '_register_state_dict_hook', '_replicate_for_data_parallel', '_save_to_state_dict', '_slow_forward', '_split_kwargs', '_state_dict_hooks', '_state_dict_pre_hooks', '_version', '_wrapped_call_impl', 'add_and_load_reward_modeling_adapter', 'add_module', 'apply', 'base_model_prefix', 'bfloat16', 'buffers', 'call_super_init', 'children', 'compile', 'compute_reward_score', 'config', 'cpu', 'cuda', 'current_device', 'double', 'dump_patches', 'enable_input_require_grads', 'eval', 'extra_repr', 'float', 'forward', 'from_pretrained', 'generate', 'get_buffer', 'get_extra_state', 'get_parameter', 'get_submodule', 'gradient_checkpointing_disable', 'gradient_checkpointing_enable', 'half', 'ipu', 'is_loaded_in_4bit', 'is_loaded_in_8bit', 'is_peft_model', 'is_sequential_parallel', 'load_state_dict', 'modules', 'mtia', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'parameters', 'policy_adapter_name', 'post_init', 'prepare_inputs_for_generation', 'pretrained_model', 'push_to_hub', 'register_backward_hook', 'register_buffer', 'register_forward_hook', 'register_forward_pre_hook', 'register_full_backward_hook', 'register_full_backward_pre_hook', 'register_load_state_dict_post_hook', 'register_load_state_dict_pre_hook', 'register_module', 'register_parameter', 'register_state_dict_post_hook', 'register_state_dict_pre_hook', 'requires_grad_', 'rm_adapter_name', 'save_pretrained', 'set_extra_state', 'set_submodule', 'share_memory', 'smart_apply', 'state_dict', 'supported_args', 'supported_modules', 'supported_pretrained_model_architectures', 'supported_rm_modules', 'supports_rm_adapter', 'to', 'to_empty', 'train', 'training', 'transformers_parent_class', 'type', 'v_head', 'xpu', 'zero_grad']\n"
     ]
    }
   ],
   "source": [
    "if ALGORITHM == \"PPO\":\n",
    "    #TODO: use create reference model function here instead...\n",
    "\n",
    "    ref_model = create_reference_model(model)\n",
    "    # load the value model with same peft setup as the policy model\n",
    "    value_model = AutoModelForCausalLMWithValueHead.from_pretrained(MODEL, peft_config=lora_config)\n",
    "    value_model.base_model_prefix = \"pretrained_model\"\n",
    "\n",
    "    \n",
    "    print(type(value_model))\n",
    "    print(hasattr(value_model, \"pretrained_model\"))\n",
    "    print(dir(value_model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ALGORITHM == \"PPO\":\n",
    "\n",
    "    \n",
    "\n",
    "    training_args = PPOConfig(\n",
    "        output_dir=RL_TRAINING_FILES, \n",
    "        logging_steps=10, \n",
    "        gradient_checkpointing=True,\n",
    "        learning_rate=1e-5,\n",
    "        num_train_epochs=3,\n",
    "        weight_decay=0.01,\n",
    "        warmup_steps=100,\n",
    "        logging_dir=\"logs\",\n",
    "        save_steps=1000,\n",
    "        save_total_limit=2,\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=100,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        )\n",
    "\n",
    "    # Initialize GRPO trainer\n",
    "    trainer = CustomPPOTrainer(\n",
    "        model=model,\n",
    "        reward_func=reward_function,\n",
    "        train_dataset=final_splits_PPO['train'],\n",
    "        eval_dataset=final_splits_PPO['validation'],\n",
    "        args=training_args,\n",
    "        ref_model=ref_model,\n",
    "        value_model=value_model,\n",
    "        # **grpo_config\n",
    "        peft_config=lora_config,\n",
    "        processing_class=tokenizer,\n",
    "        data_collator=data_collator\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideas to replace reward model with actual reward function:\n",
    "\n",
    "1. Get stub reward model but make sure it is not used anywhere\n",
    "2. overwrite get_reward function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: add metrics to evaluate training like reward, KL divergence (how much does finetuned model differ from original one), entropy of the policy (exploration versus exploitation), sampling outputs, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===training policy===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjacques-furst123\u001b[0m (\u001b[33mjacques-furst123-none\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jacques.furst/development/RAG/flintfiller-precondition-rl/rl_training_new/wandb/run-20250613_154737-39hnq4ei</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jacques-furst123-none/huggingface/runs/39hnq4ei' target=\"_blank\">ppo_config__42__1749822456</a></strong> to <a href='https://wandb.ai/jacques-furst123-none/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jacques-furst123-none/huggingface' target=\"_blank\">https://wandb.ai/jacques-furst123-none/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jacques-furst123-none/huggingface/runs/39hnq4ei' target=\"_blank\">https://wandb.ai/jacques-furst123-none/huggingface/runs/39hnq4ei</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Queries: tensor([[    1, 29473,   781,  ..., 12309,  1101, 19642],\n",
      "        [    1, 29473,   781,  ..., 12309,  1101, 19642],\n",
      "        [    1, 29473,   781,  ..., 12309,  1101, 19642],\n",
      "        ...,\n",
      "        [    1, 29473,   781,  ..., 12309,  1101, 19642],\n",
      "        [    1, 29473,   781,  ..., 12309,  1101, 19642],\n",
      "        [    1, 29473,   781,  ..., 12309,  1101, 19642]])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "getattr(): attribute name must be string",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/development/RAG/flintfiller-precondition-rl/rl_training_new/ppo_trainer_custom.py:500\u001b[0m, in \u001b[0;36mCustomPPOTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    496\u001b[0m unwrapped_value_model \u001b[38;5;241m=\u001b[39m accelerator\u001b[38;5;241m.\u001b[39munwrap_model(model)\u001b[38;5;241m.\u001b[39mvalue_model\n\u001b[1;32m    498\u001b[0m \u001b[38;5;66;03m#TODO: call get_reward differently\u001b[39;00m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;66;03m#TODO: keep first call since it gets reward from value model somehow\u001b[39;00m\n\u001b[0;32m--> 500\u001b[0m full_value, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mget_reward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m    \u001b[49m\u001b[43munwrapped_value_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_response\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext_length\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    503\u001b[0m value \u001b[38;5;241m=\u001b[39m full_value[:, context_length \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m : \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    505\u001b[0m \u001b[38;5;66;03m###############################\u001b[39;00m\n\u001b[1;32m    506\u001b[0m \u001b[38;5;66;03m#TODO: replace this properly\u001b[39;00m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;66;03m# Needed\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;66;03m# , postprocessed_query_response, processing_class.pad_token_id, context_length\u001b[39;00m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;66;03m#TODO: find out how they get query here\u001b[39;00m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;66;03m#TODO: somehow need to manage to just get the original texts for queries and responses here, my class should handle rest...\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/RL/lib/python3.10/site-packages/trl/trainer/utils.py:1213\u001b[0m, in \u001b[0;36mget_reward\u001b[0;34m(model, query_responses, pad_token_id, context_length)\u001b[0m\n\u001b[1;32m   1211\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m query_responses \u001b[38;5;241m!=\u001b[39m pad_token_id\n\u001b[1;32m   1212\u001b[0m position_ids \u001b[38;5;241m=\u001b[39m attention_mask\u001b[38;5;241m.\u001b[39mcumsum(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m-\u001b[39m attention_mask\u001b[38;5;241m.\u001b[39mlong()  \u001b[38;5;66;03m# exclusive cumsum\u001b[39;00m\n\u001b[0;32m-> 1213\u001b[0m lm_backbone \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model_prefix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1214\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmasked_fill(query_responses, \u001b[38;5;241m~\u001b[39mattention_mask, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m   1215\u001b[0m output \u001b[38;5;241m=\u001b[39m lm_backbone(\n\u001b[1;32m   1216\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1217\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1221\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,  \u001b[38;5;66;03m# otherwise mistral-based RM would error out\u001b[39;00m\n\u001b[1;32m   1222\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: getattr(): attribute name must be string"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: training and validation loss are near zero, need to debug"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
