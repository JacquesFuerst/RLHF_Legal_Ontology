{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,1,2,3\n",
      "2.7.1+cu126\n",
      "12.6\n",
      "90501\n",
      "True\n",
      "There are 4 GPU(s) available.\n",
      "CUDA is available. Using GPU: NVIDIA L40S\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForCausalLM\n",
    "from trl import PPOConfig, create_reference_model, AutoModelForCausalLMWithValueHead\n",
    "from trl import GRPOTrainer, GRPOConfig\n",
    "from datasets import DatasetDict\n",
    "\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from utils import CustomRewardFunction, LabelPreservingCollator, CustomRewardFunctionPPOTrainer\n",
    "from ppo_trainer_custom import CustomPPOTrainer\n",
    "import pandas as pd\n",
    "from accelerate import Accelerator\n",
    "\n",
    "from types import MethodType\n",
    "# import sys\n",
    "# import wandb\n",
    "\n",
    "# # Add the parent directory to the Python path\n",
    "# sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# load the relevant devices available on the server\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = os.getenv(\"AVAILABLE_DEVICES\")\n",
    "\n",
    "# Enable expandable CUDA segments\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "print(os.environ[\"CUDA_VISIBLE_DEVICES\"])\n",
    "\n",
    "# import torch\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(torch.backends.cudnn.version())\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "# load cuda\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print(\"CUDA is available. Using GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA is not available. Using CPU.\")\n",
    "\n",
    "\n",
    "accelerator = Accelerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = os.getenv(\"GENERATION_MODEL_NAME\")\n",
    "ALGORITHM = os.getenv(\"RL_ALGORITHM\")\n",
    "REWARD_MODEL = os.getenv(\"REWARD_MODEL_NAME\")\n",
    "REWARD_MODEL_EXTRACTION_LORA = os.getenv(\"REWARD_MODEL_EXTRACTION_LORA\")\n",
    "REWARD_MODEL_DETECTION_LORA = os.getenv(\"REWARD_MODEL_DETECTION_LORA\")\n",
    "RL_TOKENIZATION = \"best_window\"\n",
    "MAX_LENGTH = int(os.getenv(\"RL_MAX_LENGTH\"))\n",
    "STRIDE = int(os.getenv(\"RL_STRIDE\"))\n",
    "PROMPT_DATASET = os.getenv(\"PROMPT_DATASET_CSV\")\n",
    "DETECTION_DIFFERENCE = int(os.getenv(\"DETECTION_DIFFERENCE\"))\n",
    "WEIGHT_EXTRACTION = float(os.getenv(\"WEIGHT_EXTRACTION\"))\n",
    "WEIGHT_DETECTION = float(os.getenv(\"WEIGHT_DETECTION\"))\n",
    "RL_TRAINING_FILES = os.getenv(\"RL_TRAINING_FILES\") + \"_\" + ALGORITHM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load prompt dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_df = pd.read_csv(PROMPT_DATASET, sep=\";\")\n",
    "dataset = Dataset.from_pandas(prompt_df)\n",
    "\n",
    "#TODO: test whether everything is well-separated\n",
    "train_test_split = dataset.train_test_split(test_size=0.3, seed=42)\n",
    "eval_test_split = train_test_split[\"test\"].train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "\n",
    "final_splits = DatasetDict({\n",
    "    'train': train_test_split['train'],\n",
    "    'validation': eval_test_split['train'],\n",
    "    'test': eval_test_split['test']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['prompt', 'precondition_texts', 'precondition_positions'], dtype='object')\n",
      "26\n",
      "['prompt', 'precondition_texts', 'precondition_positions']\n"
     ]
    }
   ],
   "source": [
    "print(prompt_df.columns)\n",
    "print(len(prompt_df))\n",
    "print(dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: do train test eval split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset columns\n",
    "\n",
    "1. prompt\n",
    "2. precondition_text_dict --> key: id, value: text \n",
    "3. precondition_position_dict --> key: id, value: position\n",
    "\n",
    "Think about whether any other components are needed...\n",
    "Need to iterate through all preconditions to get reward, or return several rewards per response and finetune model on each one --> iteration should be good enough, just need to define reward function properly for this..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.31it/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at nlpaueb/legal-bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load base model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL,  \n",
    "                                             device_map={\"\": accelerator.process_index},  # For GPU/TPU acceleration\n",
    "                                             torch_dtype=\"auto\")   # Optimize precision)\n",
    "\n",
    "device = model.device\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load reward model feedback extraction\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(REWARD_MODEL, num_labels=1)\n",
    "reward_tokenizer = AutoTokenizer.from_pretrained(REWARD_MODEL)\n",
    "\n",
    "extraction_model = PeftModel.from_pretrained(base_model, REWARD_MODEL_EXTRACTION_LORA).to(device)\n",
    "# extraction_model = extraction_model.merge_and_unload()\n",
    "\n",
    "detection_model = PeftModel.from_pretrained(base_model, REWARD_MODEL_DETECTION_LORA).to(device)\n",
    "# detection_model = detection_model.merge_and_unload()\n",
    "\n",
    "\n",
    "# Create the custom reward function\n",
    "reward_function = CustomRewardFunction(extraction_model, detection_model, reward_tokenizer, MAX_LENGTH, STRIDE, RL_TOKENIZATION, device, weight_extraction=WEIGHT_EXTRACTION, weight_detection=WEIGHT_DETECTION, detection_difference=DETECTION_DIFFERENCE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoRA config and wnadb init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type='CAUSAL_LM',  \n",
    ")\n",
    "\n",
    "# wandb.init(project=\"RL-preconditions\", name=\"grpo-run-test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRPO Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ALGORITHM == \"GRPO\":\n",
    "\n",
    "    training_args = GRPOConfig(\n",
    "        output_dir=RL_TRAINING_FILES, \n",
    "        logging_steps=1, \n",
    "        gradient_checkpointing=True,\n",
    "        learning_rate=1e-5,\n",
    "        num_train_epochs=10,\n",
    "        weight_decay=0.01,\n",
    "        warmup_steps=5, # TODO:check if this makes any sense at all\n",
    "        logging_dir=\"logs\",\n",
    "        # save_steps=1,\n",
    "        # save_total_limit=2,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        # eval_steps=1,\n",
    "        # batch_size=2,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        gradient_accumulation_steps=3, #TODO: think about whether this is truly necessary\n",
    "        report_to=\"wandb\",\n",
    "        )\n",
    "\n",
    "    # Initialize GRPO trainer\n",
    "    trainer = GRPOTrainer(\n",
    "        model=model,\n",
    "        reward_funcs=reward_function,\n",
    "        train_dataset=final_splits['train'],\n",
    "        eval_dataset=final_splits['validation'],\n",
    "        args=training_args,\n",
    "        # **grpo_config\n",
    "        peft_config=lora_config\n",
    "    )\n",
    "\n",
    "#TODO: maybe get a learning rate scheduler for this...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code from Huggingface TRL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes:\n",
    "\n",
    "1. Do not use SLURM since I am in a single-node multi GPU setting and SLURM would work with scheduled training on a multi node cluster... --> use accelerate instead\n",
    "\n",
    "2. install transformers accelerate deepspeed trl\n",
    "\n",
    "3. DAPO paper for some hyperparameter settings, DeepSeekMath paper for hyperparametersettings is good as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: initialize proper weights here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/26 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 26/26 [00:00<00:00, 807.16 examples/s]\n",
      "Map: 100%|██████████| 26/26 [00:00<00:00, 5591.83 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask', 'additional_entries'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#TODO: Prepare dataset here\n",
    "# Need to tokenize to use for PPO\n",
    "\n",
    "def tokenize_and_keep_original(example):\n",
    "    # Tokenize the \"text\" column\n",
    "    tokenized = tokenizer(example[\"prompt\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "    # Keep the original text\n",
    "    # tokenized[\"original_text\"] = example[\"prompt\"]\n",
    "    return tokenized\n",
    "\n",
    "# Apply the function to the dataset\n",
    "tokenized_dataset = dataset.map(tokenize_and_keep_original, batched=True)\n",
    "\n",
    "# Create label column for this to be handled properly in PPO Trainer\n",
    "def create_label(example):\n",
    "    return {\"additional_entries\": (example[\"prompt\"], example[\"precondition_texts\"], example[\"precondition_positions\"])}\n",
    "\n",
    "tokenized_dataset = tokenized_dataset.map(create_label)\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([\"prompt\", \"precondition_texts\", \"precondition_positions\"])\n",
    "print(tokenized_dataset[0].keys())\n",
    "# print(tokenized_dataset[0][\"prompt\"])\n",
    "\n",
    "#TODO: do train test split on this\n",
    "\n",
    "#TODO: test whether everything is well-separated\n",
    "train_test_split_PPO = tokenized_dataset.train_test_split(test_size=0.3, seed=42)\n",
    "eval_test_split_PPO = train_test_split_PPO[\"test\"].train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "\n",
    "final_splits_PPO = DatasetDict({\n",
    "    'train': train_test_split_PPO['train'],\n",
    "    'validation': eval_test_split_PPO['train'],\n",
    "    'test': eval_test_split_PPO['test']\n",
    "})\n",
    "\n",
    "# use own data collator that does not pad label column\n",
    "data_collator = LabelPreservingCollator(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value model and ref_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.29it/s]\n",
      "WARNING:root:A <class 'peft.peft_model.PeftModelForCausalLM'> model is loaded from 'mistralai/Mistral-7B-Instruct-v0.3', and no v_head weight is found. This IS expected if you are not resuming PPO training.\n"
     ]
    }
   ],
   "source": [
    "if ALGORITHM == \"PPO\":\n",
    "    #TODO: use create reference model function here instead...\n",
    "\n",
    "    ref_model = create_reference_model(model)\n",
    "    # ref_model.to(model.device)\n",
    "    # load the value model with same peft setup as the policy model\n",
    "    value_model = AutoModelForCausalLMWithValueHead.from_pretrained(MODEL, \n",
    "                                                                    peft_config=lora_config, \n",
    "                                                                    device_map={\"\": accelerator.process_index},  # For GPU/TPU acceleration\n",
    "                                                                    torch_dtype=\"auto\")\n",
    "    value_model.base_model_prefix = \"pretrained_model\"\n",
    "\n",
    "    def score(self, hidden_states):\n",
    "        return self.v_head(hidden_states).squeeze(-1)\n",
    "\n",
    "    value_model.score = MethodType(score, value_model)\n",
    "\n",
    "    #TODO: use accelerator.process_index here maybe\n",
    "\n",
    "    reward_function_PPO = CustomRewardFunctionPPOTrainer(extraction_model, \n",
    "                                                         detection_model, \n",
    "                                                         reward_tokenizer, \n",
    "                                                         MAX_LENGTH, \n",
    "                                                         STRIDE, \n",
    "                                                         RL_TOKENIZATION, \n",
    "                                                         device, \n",
    "                                                         weight_extraction=WEIGHT_EXTRACTION, \n",
    "                                                         weight_detection=WEIGHT_DETECTION, \n",
    "                                                         detection_difference=DETECTION_DIFFERENCE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ALGORITHM == \"PPO\":\n",
    "\n",
    "    \n",
    "\n",
    "    training_args = PPOConfig(\n",
    "        output_dir=RL_TRAINING_FILES, \n",
    "        logging_steps=10, \n",
    "        gradient_checkpointing=True,\n",
    "        learning_rate=1e-5,\n",
    "        num_train_epochs=3,\n",
    "        weight_decay=0.01,\n",
    "        warmup_steps=100,\n",
    "        logging_dir=\"logs\",\n",
    "        save_steps=1000,\n",
    "        save_total_limit=2,\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=100,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        )\n",
    "\n",
    "    # Initialize GRPO trainer\n",
    "    trainer = CustomPPOTrainer(\n",
    "        model=model,\n",
    "        reward_func=reward_function_PPO,\n",
    "        train_dataset=final_splits_PPO['train'],\n",
    "        eval_dataset=final_splits_PPO['validation'],\n",
    "        args=training_args,\n",
    "        ref_model=ref_model,\n",
    "        value_model=value_model,\n",
    "        # **grpo_config\n",
    "        peft_config=lora_config,\n",
    "        processing_class=tokenizer,\n",
    "        data_collator=data_collator\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideas to replace reward model with actual reward function:\n",
    "\n",
    "1. Get stub reward model but make sure it is not used anywhere\n",
    "2. overwrite get_reward function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: add metrics to evaluate training like reward, KL divergence (how much does finetuned model differ from original one), entropy of the policy (exploration versus exploitation), sampling outputs, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===training policy===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjacques-furst123\u001b[0m (\u001b[33mjacques-furst123-none\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jacques.furst/development/RAG/flintfiller-precondition-rl/rl_training_new/wandb/run-20250613_171909-d9bcmzyl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jacques-furst123-none/huggingface/runs/d9bcmzyl' target=\"_blank\">ppo_config__42__1749827949</a></strong> to <a href='https://wandb.ai/jacques-furst123-none/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jacques-furst123-none/huggingface' target=\"_blank\">https://wandb.ai/jacques-furst123-none/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jacques-furst123-none/huggingface/runs/d9bcmzyl' target=\"_blank\">https://wandb.ai/jacques-furst123-none/huggingface/runs/d9bcmzyl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Queries: tensor([[    1, 29473,   781,  ..., 12309,  1101, 19642],\n",
      "        [    1, 29473,   781,  ..., 12309,  1101, 19642],\n",
      "        [    1, 29473,   781,  ..., 12309,  1101, 19642],\n",
      "        ...,\n",
      "        [    1, 29473,   781,  ..., 12309,  1101, 19642],\n",
      "        [    1, 29473,   781,  ..., 12309,  1101, 19642],\n",
      "        [    1, 29473,   781,  ..., 12309,  1101, 19642]], device='cuda:0')\n",
      "Postprocessed query and response batch: tensor([[    1, 29473,   781,  ...,  5313,  1037,  3071],\n",
      "        [    1, 29473,   781,  ...,  1055, 17958,  3778],\n",
      "        [    1, 29473,   781,  ...,  8516, 29515,  1181],\n",
      "        ...,\n",
      "        [    1, 29473,   781,  ...,  2197, 11787,  2934],\n",
      "        [    1, 29473,   781,  ..., 12708,   781,   781],\n",
      "        [    1, 29473,   781,  ..., 12726, 29491, 21490]], device='cuda:0')\n",
      "Query as passed into reward model: ['<s> \\n\\n\\n                --- Definitie ---\\n\\n                Preconditie: Een preconditie beschrijft de omstandigheden waaronder de handeling wettelijk kan worden uitgevoerd.\\n                Act: Een act kan worden uitgevoerd door een agent binnen het normatieve systeem dat wordt gedefinieerd door het juridische document.\\n                Fact: Fact frames beschrijven zaken waarvan de aanwezigheid of waarheidswaarde de toestand van het normatieve syst', '<s> \\n\\n\\n                --- Definitie ---\\n\\n                Preconditie: Een preconditie beschrijft de omstandigheden waaronder de handeling wettelijk kan worden uitgevoerd.\\n                Act: Een act kan worden uitgevoerd door een agent binnen het normatieve systeem dat wordt gedefinieerd door het juridische document.\\n                Fact: Fact frames beschrijven zaken waarvan de aanwezigheid of waarheidswaarde de toestand van het normatieve syst', '<s> \\n\\n\\n                --- Definitie ---\\n\\n                Preconditie: Een preconditie beschrijft de omstandigheden waaronder de handeling wettelijk kan worden uitgevoerd.\\n                Act: Een act kan worden uitgevoerd door een agent binnen het normatieve systeem dat wordt gedefinieerd door het juridische document.\\n                Fact: Fact frames beschrijven zaken waarvan de aanwezigheid of waarheidswaarde de toestand van het normatieve syst', '<s> \\n\\n\\n                --- Definitie ---\\n\\n                Preconditie: Een preconditie beschrijft de omstandigheden waaronder de handeling wettelijk kan worden uitgevoerd.\\n                Act: Een act kan worden uitgevoerd door een agent binnen het normatieve systeem dat wordt gedefinieerd door het juridische document.\\n                Fact: Fact frames beschrijven zaken waarvan de aanwezigheid of waarheidswaarde de toestand van het normatieve syst', '<s> \\n\\n\\n                --- Definitie ---\\n\\n                Preconditie: Een preconditie beschrijft de omstandigheden waaronder de handeling wettelijk kan worden uitgevoerd.\\n                Act: Een act kan worden uitgevoerd door een agent binnen het normatieve systeem dat wordt gedefinieerd door het juridische document.\\n                Fact: Fact frames beschrijven zaken waarvan de aanwezigheid of waarheidswaarde de toestand van het normatieve syst', '<s> \\n\\n\\n                --- Definitie ---\\n\\n                Preconditie: Een preconditie beschrijft de omstandigheden waaronder de handeling wettelijk kan worden uitgevoerd.\\n                Act: Een act kan worden uitgevoerd door een agent binnen het normatieve systeem dat wordt gedefinieerd door het juridische document.\\n                Fact: Fact frames beschrijven zaken waarvan de aanwezigheid of waarheidswaarde de toestand van het normatieve syst', '<s> \\n\\n\\n                --- Definitie ---\\n\\n                Preconditie: Een preconditie beschrijft de omstandigheden waaronder de handeling wettelijk kan worden uitgevoerd.\\n                Act: Een act kan worden uitgevoerd door een agent binnen het normatieve systeem dat wordt gedefinieerd door het juridische document.\\n                Fact: Fact frames beschrijven zaken waarvan de aanwezigheid of waarheidswaarde de toestand van het normatieve syst', '<s> \\n\\n\\n                --- Definitie ---\\n\\n                Preconditie: Een preconditie beschrijft de omstandigheden waaronder de handeling wettelijk kan worden uitgevoerd.\\n                Act: Een act kan worden uitgevoerd door een agent binnen het normatieve systeem dat wordt gedefinieerd door het juridische document.\\n                Fact: Fact frames beschrijven zaken waarvan de aanwezigheid of waarheidswaarde de toestand van het normatieve syst']\n",
      "Response as passed into reward model: ['<s> \\n\\n\\n                --- Definitie ---\\n\\n                Preconditie: Een preconditie beschrijft de omstandigheden waaronder de handeling wettelijk kan worden uitgevoerd.\\n                Act: Een act kan worden uitgevoerd door een agent binnen het normatieve systeem dat wordt gedefinieerd door het juridische document.\\n                Fact: Fact frames beschrijven zaken waarvan de aanwezigheid of waarheidswaarde de toestand van het normatieve systeem beïnvloedt.\\n                Wet: Een wet is een norm waarbij de macht van het parlement de grondslag is, en die dient als keuzebepaling voor de samenlev', '<s> \\n\\n\\n                --- Definitie ---\\n\\n                Preconditie: Een preconditie beschrijft de omstandigheden waaronder de handeling wettelijk kan worden uitgevoerd.\\n                Act: Een act kan worden uitgevoerd door een agent binnen het normatieve systeem dat wordt gedefinieerd door het juridische document.\\n                Fact: Fact frames beschrijven zaken waarvan de aanwezigheid of waarheidswaarde de toestand van het normatieve systeem beïnvloedt. Facts kunnen zowel in het verleden als in het heden bestaan.\\n                Consequentie: De consequentie is de uitkomst van een act binnen het', '<s> \\n\\n\\n                --- Definitie ---\\n\\n                Preconditie: Een preconditie beschrijft de omstandigheden waaronder de handeling wettelijk kan worden uitgevoerd.\\n                Act: Een act kan worden uitgevoerd door een agent binnen het normatieve systeem dat wordt gedefinieerd door het juridische document.\\n                Fact: Fact frames beschrijven zaken waarvan de aanwezigheid of waarheidswaarde de toestand van het normatieve systeem kan invloeden.\\n                Reactie: Een reactie is de wettelijke receptie van het aannemen van een act door een agent binnen het normatieve systeem.\\n                Resultaat: E', '<s> \\n\\n\\n                --- Definitie ---\\n\\n                Preconditie: Een preconditie beschrijft de omstandigheden waaronder de handeling wettelijk kan worden uitgevoerd.\\n                Act: Een act kan worden uitgevoerd door een agent binnen het normatieve systeem dat wordt gedefinieerd door het juridische document.\\n                Fact: Fact frames beschrijven zaken waarvan de aanwezigheid of waarheidswaarde de toestand van het normatieve systeem beïnvloedt.\\n                Recht: Een recht is de verplichting tot een bepaalde handeling en/of de toelating tot een bepaalde handeling.\\n                Ver', '<s> \\n\\n\\n                --- Definitie ---\\n\\n                Preconditie: Een preconditie beschrijft de omstandigheden waaronder de handeling wettelijk kan worden uitgevoerd.\\n                Act: Een act kan worden uitgevoerd door een agent binnen het normatieve systeem dat wordt gedefinieerd door het juridische document.\\n                Fact: Fact frames beschrijven zaken waarvan de aanwezigheid of waarheidswaarde de toestand van het normatieve systeem kan veranderen.\\n                Rechtsfeit: Rechtsfeiten worden geregistreerd door de rechtbank of het notarisambt bij een rechtsact. Hierdoor worden de feiten een onderdeel van', '<s> \\n\\n\\n                --- Definitie ---\\n\\n                Preconditie: Een preconditie beschrijft de omstandigheden waaronder de handeling wettelijk kan worden uitgevoerd.\\n                Act: Een act kan worden uitgevoerd door een agent binnen het normatieve systeem dat wordt gedefinieerd door het juridische document.\\n                Fact: Fact frames beschrijven zaken waarvan de aanwezigheid of waarheidswaarde de toestand van het normatieve systeem beïnvloeden.\\n                Effect: Het effect van een handeling door de agent is de toestand die na de uitvoering van de handeling valt.\\n                Consequence: Een consequence wordt veroorza', '<s> \\n\\n\\n                --- Definitie ---\\n\\n                Preconditie: Een preconditie beschrijft de omstandigheden waaronder de handeling wettelijk kan worden uitgevoerd.\\n                Act: Een act kan worden uitgevoerd door een agent binnen het normatieve systeem dat wordt gedefinieerd door het juridische document.\\n                Fact: Fact frames beschrijven zaken waarvan de aanwezigheid of waarheidswaarde de toestand van het normatieve systeem beïnvloedt.\\n                Consequentie: Een consequentie beschrijft de gevolgen van de uitvoering van de act door de agent.\\n\\n                --- Toelichting ---\\n\\n', '<s> \\n\\n\\n                --- Definitie ---\\n\\n                Preconditie: Een preconditie beschrijft de omstandigheden waaronder de handeling wettelijk kan worden uitgevoerd.\\n                Act: Een act kan worden uitgevoerd door een agent binnen het normatieve systeem dat wordt gedefinieerd door het juridische document.\\n                Fact: Fact frames beschrijven zaken waarvan de aanwezigheid of waarheidswaarde de toestand van het normatieve systeem beïnvloedt.\\n\\n                --- Informatie structureren ---\\n\\n                De informatie in een juridisch document kan worden opgesteld en opgedeeld in verschillende manieren. Hier']\n"
     ]
    },
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<unknown>, line 4)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[1;32m~/miniconda3/envs/RL/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3579\u001b[0m in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0m  Cell \u001b[1;32mIn[14], line 2\u001b[0m\n    trainer.train()\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/development/RAG/flintfiller-precondition-rl/rl_training_new/ppo_trainer_custom.py:517\u001b[0m in \u001b[1;35mtrain\u001b[0m\n    _, score, _ = self.reward_func(\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/development/RAG/flintfiller-precondition-rl/rl_training_new/utils.py:602\u001b[0m in \u001b[1;35m__call__\u001b[0m\n    precondition_texts_dict = ast.literal_eval(precondition_texts)\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/miniconda3/envs/RL/lib/python3.10/ast.py:64\u001b[0m in \u001b[1;35mliteral_eval\u001b[0m\n    node_or_string = parse(node_or_string.lstrip(\" \\t\"), mode='eval')\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m~/miniconda3/envs/RL/lib/python3.10/ast.py:50\u001b[0;36m in \u001b[0;35mparse\u001b[0;36m\n\u001b[0;31m    return compile(source, filename, mode, flags,\u001b[0;36m\n",
      "\u001b[0;36m  File \u001b[0;32m<unknown>:4\u001b[0;36m\u001b[0m\n\u001b[0;31m    --- Definitie ---\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: training and validation loss are near zero, need to debug"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
