{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7313a04",
   "metadata": {},
   "source": [
    "# Notebook to split datasets into proper files adn find imbalances in data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e286de1",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63aad9b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacques.furst/miniconda3/envs/Evaluation/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from utils import parse_ratings\n",
    "\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e800d9f",
   "metadata": {},
   "source": [
    "## Prompt dataset for RL training and final evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16b4c7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "RL_DATA_PATH = os.getenv(\"RL_DATA_PATH\")\n",
    "PROMPT_DATASET = os.getenv(\"PROMPT_DATASET_CSV\")\n",
    "\n",
    "# load prompt dataset\n",
    "df = pd.read_csv(PROMPT_DATASET, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ba2aca",
   "metadata": {},
   "source": [
    "## Random split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77386f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['prompt', 'precondition_texts', 'precondition_positions'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2512fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 18\n",
      "Validation: 4\n",
      "Test: 4\n",
      "The lomg amswer counts for train, val, test are: (np.int64(2), np.int64(1), np.int64(1))\n"
     ]
    }
   ],
   "source": [
    "# separate into df with short answers and long answers\n",
    "short_df = df[df['precondition_texts'].apply(lambda x: len(ast.literal_eval(x)) <= 10)]\n",
    "long_df = df[df['precondition_texts'].apply(lambda x: len(ast.literal_eval(x)) > 10)]\n",
    "\n",
    "\n",
    "train_df_short, temp_df_short = train_test_split(short_df, test_size=0.25, random_state=42)\n",
    "train_df_long, temp_df_long = train_test_split(long_df, test_size=0.5, random_state=42)\n",
    "\n",
    "val_df_short, test_df_short = train_test_split(temp_df_short, test_size=0.5, random_state=42)\n",
    "val_df_long, test_df_long = train_test_split(temp_df_long, test_size=0.5, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "train_df = pd.concat([train_df_short, train_df_long], ignore_index=True)\n",
    "val_df = pd.concat([val_df_short, val_df_long], ignore_index=True)\n",
    "test_df = pd.concat([test_df_short, test_df_long], ignore_index=True)\n",
    "\n",
    "#TODO: change this back if using separate split\n",
    "\n",
    "# train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42)\n",
    "# val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "# Check the sizes\n",
    "print(f\"Train: {len(train_df)}\")\n",
    "print(f\"Validation: {len(val_df)}\")\n",
    "print(f\"Test: {len(test_df)}\")\n",
    "\n",
    "count_train = train_df['precondition_texts'].apply(lambda x: isinstance(ast.literal_eval(x), dict) and len(ast.literal_eval(x)) >= 10).sum()\n",
    "count_val = val_df['precondition_texts'].apply(lambda x: isinstance(ast.literal_eval(x), dict) and len(ast.literal_eval(x)) >= 10).sum()\n",
    "count_test = test_df['precondition_texts'].apply(lambda x: isinstance(ast.literal_eval(x), dict) and len(ast.literal_eval(x)) >= 10).sum()\n",
    "\n",
    "\n",
    "print(f\"The lomg amswer counts for train, val, test are: {count_train, count_val, count_test}\")\n",
    "\n",
    "# Save to CSV files without the index column\n",
    "train_df.to_csv(RL_DATA_PATH + \"/train_random.csv\", index=False, sep=';')\n",
    "val_df.to_csv(RL_DATA_PATH + \"/validation_random.csv\", index=False, sep=';')\n",
    "test_df.to_csv(RL_DATA_PATH + \"/test_random.csv\", index=False, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b476df08",
   "metadata": {},
   "source": [
    "## Dedicated separate split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02b57531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 17\n",
      "Validation: 4\n",
      "Test: 5\n"
     ]
    }
   ],
   "source": [
    "# Use facts rijksbegroting as valid + test since if is made of 9 facts, hence 9 different prompts out of 25 --> about 30%\n",
    "phrase1 = \"--- Aantal Subfacts ---\"\n",
    "phrase2 = \"Comptabiliteitswet 2016\"\n",
    "\n",
    "mask = df['prompt'].str.contains(phrase1, case=False, na=False) & df['prompt'].str.contains(phrase2, case=False, na=False)\n",
    "\n",
    "train_df_det = df[~mask]\n",
    "val_test_df_det = df[mask]\n",
    "\n",
    "# Second split: validation + test\n",
    "val_df_det, test_df_det = train_test_split(val_test_df_det, test_size=0.5, random_state=42)\n",
    "\n",
    "# Check the sizes\n",
    "print(f\"Train: {len(train_df_det)}\")\n",
    "print(f\"Validation: {len(val_df_det)}\")\n",
    "print(f\"Test: {len(test_df_det)}\")\n",
    "\n",
    "# Save to CSV files without the index column\n",
    "train_df_det.to_csv(RL_DATA_PATH + \"/train_determined.csv\", index=False, sep=';')\n",
    "val_df_det.to_csv(RL_DATA_PATH + \"/validation_determined.csv\", index=False, sep=';')\n",
    "test_df_det.to_csv(RL_DATA_PATH + \"/test_determined.csv\", index=False, sep=';')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633e0b34",
   "metadata": {},
   "source": [
    "## Feedback dataset for Reward model finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "680c61c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reward data path\n",
    "REWARD_DATA_PATH = os.getenv(\"REWARD_DATA_PATH\")\n",
    "\n",
    "# Reward model data\n",
    "FILE_1 = os.getenv(\"FILE_1\")\n",
    "FILE_5 = os.getenv(\"FILE_5\")\n",
    "FILE_7 = os.getenv(\"FILE_7\")\n",
    "FILE_9 = os.getenv(\"FILE_9\")\n",
    "FILE_10_1 = os.getenv(\"FILE_10_1\")\n",
    "FILE_10_2 = os.getenv(\"FILE_10_2\")\n",
    "FILE_SYNTH = os.getenv(\"FILE_SYNTH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "649ced42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataframes\n",
    "df_1 = pd.read_csv(FILE_1, sep=\";\")\n",
    "df_5 = pd.read_csv(FILE_5, sep=\";\")\n",
    "df_7 = pd.read_csv(FILE_7, sep=\";\")\n",
    "df_9 = pd.read_csv(FILE_9, sep=\";\")\n",
    "df_10_1 = pd.read_csv(FILE_10_1, sep=\";\")\n",
    "df_10_2 = pd.read_csv(FILE_10_2, sep=\";\")\n",
    "df_synth = pd.read_csv(FILE_SYNTH, sep=\";\")\n",
    "\n",
    "df_human = pd.concat([df_1, df_5, df_7, df_9, df_10_1, df_10_2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc34c2c",
   "metadata": {},
   "source": [
    "#### Re-structure df synthetic to fit in training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f227d446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic feedback shape: (563, 14)\n",
      "Index(['file', 'frame_ID', 'frame_type', 'frame_text', 'precondition_id',\n",
      "       'precondition_text', 'precondition_position', 'response_text',\n",
      "       'prompt_config_examples', 'prompt_config_chain_of_thought',\n",
      "       'feedback_extraction', 'feedback_detection', 'additional_feedback',\n",
      "       'synthetic_feedback'],\n",
      "      dtype='object')\n",
      "Synthetic feedback shape: (564, 14)\n"
     ]
    }
   ],
   "source": [
    "print(\"Synthetic feedback shape:\", df_synth.shape)\n",
    "\n",
    "\n",
    "# Save the current headers since forgot to store headers in csv file\n",
    "old_headers = df_synth.columns.tolist()\n",
    "\n",
    "# Step 2: Insert the headers as the first row\n",
    "df_synth.loc[-1] = old_headers # Add headers as a new row\n",
    "df_synth.index = df_synth.index + 1 # Shift index\n",
    "df_synth = df_synth.sort_index() # Sort index to place the new row at the top\n",
    "\n",
    "\n",
    "# Step 3: Assign new headers\n",
    "df_synth.columns = ['file', \n",
    "                            'frame_ID', \n",
    "                            'frame_type', \n",
    "                            'frame_text', \n",
    "                            'precondition_id', \n",
    "                            'precondition_text', \n",
    "                            'precondition_position', \n",
    "                            'response_text', \n",
    "                            'prompt_config_examples', \n",
    "                            'prompt_config_chain_of_thought', \n",
    "                            'feedback_extraction', \n",
    "                            'feedback_detection', \n",
    "                            'additional_feedback',\n",
    "                            'synthetic_feedback',\n",
    "                ]\n",
    "\n",
    "print(df_synth.columns)\n",
    "\n",
    "\n",
    "df_synth['prompt_config_examples'] = (df_synth['prompt_config_examples']                                              \n",
    "                                                .astype(str)\n",
    "                                                .str.strip()\n",
    "                                                .str.lower()\n",
    "                                                .map({'true': True, 'false': False})\n",
    ")\n",
    "\n",
    "df_synth['prompt_config_chain_of_thought'] = (df_synth['prompt_config_chain_of_thought']\n",
    "                                                .astype(str)\n",
    "                                                .str.strip()\n",
    "                                                .str.lower()\n",
    "                                                .map({'true': True, 'false': False})\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print(\"Synthetic feedback shape:\", df_synth.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab1e5c5",
   "metadata": {},
   "source": [
    "## Random split (human data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fafa4c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 650\n",
      "Validation: 139\n",
      "Test: 140\n"
     ]
    }
   ],
   "source": [
    "train_df_human, temp_df_human = train_test_split(df_human, test_size=0.3, random_state=42)\n",
    "val_df_human, test_df_human = train_test_split(temp_df_human, test_size=0.5, random_state=42)\n",
    "\n",
    "# Check the sizes\n",
    "print(f\"Train: {len(train_df_human)}\")\n",
    "print(f\"Validation: {len(val_df_human)}\")\n",
    "print(f\"Test: {len(test_df_human)}\")\n",
    "\n",
    "# Save to CSV files without the index column\n",
    "train_df_human.to_csv(REWARD_DATA_PATH + \"/train_human_random.csv\", index=False, sep=';')\n",
    "val_df_human.to_csv(REWARD_DATA_PATH + \"/validation_human_random.csv\", index=False, sep=';')\n",
    "test_df_human.to_csv(REWARD_DATA_PATH + \"/test_human_random.csv\", index=False, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2852c48",
   "metadata": {},
   "source": [
    "## Defined split (human data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b496136",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "file\n",
       "Interpretatie_Vw_over_besluiten_op_aanvragen_voor_een_verblijfsvergunning_regulier_bepaalde_tijd.json    432\n",
       "Participatiewet_most_recent_public.json                                                                  255\n",
       "rijksbegrotingscyclus.json                                                                               242\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use participatiewet since it makes up about one third (0.27) of the data and is hence good for test + eval\n",
    "\n",
    "df_human['file'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cdd450b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 674\n",
      "Validation: 127\n",
      "Test: 128\n"
     ]
    }
   ],
   "source": [
    "train_df_human_det = df_human[df_human['file'] != 'Participatiewet_most_recent_public.json']\n",
    "\n",
    "temp_df_human_det = df_human[df_human['file'] == 'Participatiewet_most_recent_public.json']\n",
    "\n",
    "# Second split: validation + test\n",
    "val_df_human_det, test_df_human_det = train_test_split(temp_df_human_det, test_size=0.5, random_state=42)\n",
    "\n",
    "# Check the sizes\n",
    "print(f\"Train: {len(train_df_human_det)}\")\n",
    "print(f\"Validation: {len(val_df_human_det)}\")\n",
    "print(f\"Test: {len(test_df_human_det)}\")\n",
    "\n",
    "# Save to CSV files without the index column\n",
    "train_df_human_det.to_csv(REWARD_DATA_PATH + \"/train_human_determined.csv\", index=False, sep=';')\n",
    "val_df_human_det.to_csv(REWARD_DATA_PATH + \"/validation_human_determined.csv\", index=False, sep=';')\n",
    "test_df_human_det.to_csv(REWARD_DATA_PATH + \"/test_human_determined.csv\", index=False, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2844be9",
   "metadata": {},
   "source": [
    "## Random split (synthetic data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58678439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 423\n",
      "Validation: 70\n",
      "Test: 71\n"
     ]
    }
   ],
   "source": [
    "train_df_synth, temp_df_synth = train_test_split(df_synth, test_size=0.25, random_state=42)\n",
    "val_df_synth, test_df_synth = train_test_split(temp_df_synth, test_size=0.5, random_state=42)\n",
    "\n",
    "# Check the sizes\n",
    "print(f\"Train: {len(train_df_synth)}\")\n",
    "print(f\"Validation: {len(val_df_synth)}\")\n",
    "print(f\"Test: {len(test_df_synth)}\")\n",
    "\n",
    "# Save to CSV files without the index column\n",
    "train_df_synth.to_csv(REWARD_DATA_PATH + \"/train_synth_random.csv\", index=False, sep=';')\n",
    "val_df_synth.to_csv(REWARD_DATA_PATH + \"/validation_synth_random.csv\", index=False, sep=';')\n",
    "test_df_synth.to_csv(REWARD_DATA_PATH + \"/test_synth_random.csv\", index=False, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6728c093",
   "metadata": {},
   "source": [
    "## Determined split (synthetic data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88e61c6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "file\n",
       "Participatiewet_most_recent_public.json                                                                  208\n",
       "Interpretatie_Vw_over_besluiten_op_aanvragen_voor_een_verblijfsvergunning_regulier_bepaalde_tijd.json    188\n",
       "rijksbegrotingscyclus.json                                                                               168\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_synth['file'].value_counts()\n",
    "\n",
    "# TAking rijksbegrotingscyclus since it make up roughly 30% of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd0bb9a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 396\n",
      "Validation: 84\n",
      "Test: 84\n"
     ]
    }
   ],
   "source": [
    "train_df_synth_det = df_synth[df_synth['file'] != 'rijksbegrotingscyclus.json']\n",
    "\n",
    "temp_df_synth_det = df_synth[df_synth['file'] == 'rijksbegrotingscyclus.json']\n",
    "\n",
    "# Second split: validation + test\n",
    "val_df_synth_det, test_df_synth_det = train_test_split(temp_df_synth_det, test_size=0.5, random_state=42)\n",
    "\n",
    "# Check the sizes\n",
    "print(f\"Train: {len(train_df_synth_det)}\")\n",
    "print(f\"Validation: {len(val_df_synth_det)}\")\n",
    "print(f\"Test: {len(test_df_synth_det)}\")\n",
    "\n",
    "# Save to CSV files without the index column\n",
    "train_df_synth_det.to_csv(REWARD_DATA_PATH + \"/train_synth_determined.csv\", index=False, sep=';')\n",
    "val_df_synth_det.to_csv(REWARD_DATA_PATH + \"/validation_synth_determined.csv\", index=False, sep=';')\n",
    "test_df_synth_det.to_csv(REWARD_DATA_PATH + \"/test_synth_determined.csv\", index=False, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bac78c",
   "metadata": {},
   "source": [
    "# Find data imbalances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a97f7c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human data counts: {'feedback_extraction': feedback_extraction\n",
      "0    499\n",
      "3    231\n",
      "2    104\n",
      "1     95\n",
      "Name: count, dtype: int64, 'feedback_detection': feedback_detection\n",
      "4    630\n",
      "6    280\n",
      "5     19\n",
      "Name: count, dtype: int64}\n",
      "Synthetic data counts: {'feedback_extraction': feedback_extraction\n",
      "0    377\n",
      "2     94\n",
      "1     50\n",
      "3     43\n",
      "Name: count, dtype: int64, 'feedback_detection': feedback_detection\n",
      "4    404\n",
      "6    144\n",
      "5     16\n",
      "Name: count, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "columns_of_interest = ['feedback_extraction','feedback_detection']\n",
    "\n",
    "\n",
    "for col in columns_of_interest:\n",
    "    df_human[col] = df_human[col].apply(parse_ratings)\n",
    "    df_synth[col] = df_synth[col].apply(parse_ratings)\n",
    "\n",
    "\n",
    "# Function to get unique value counts for each column\n",
    "def unique_value_counts(df, columns_of_interest):\n",
    "    return {col: df[col].value_counts() for col in columns_of_interest}\n",
    "\n",
    "# Get value counts for each DataFrame\n",
    "df_human_counts = unique_value_counts(df_human, columns_of_interest)\n",
    "df_synth_counts = unique_value_counts(df_synth, columns_of_interest)\n",
    "\n",
    "print(f\"Human data counts: {df_human_counts}\")\n",
    "print(f\"Synthetic data counts: {df_synth_counts}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Evaluation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
