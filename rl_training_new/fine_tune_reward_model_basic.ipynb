{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tune reward model from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODOs:\n",
    "\n",
    "#TODO: double-check that labels are not somehow misaligned...\n",
    "\n",
    "#TODO: check if you need to plot \n",
    "\n",
    "1. LoRA learns the position of the low rank adaptation matrix that is needed to finetune a model of a much higher rank\n",
    "\n",
    "#TODO: double check model performance, generate output, maybe adjust training metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports, setup, and global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacques.furst/miniconda3/envs/RL/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 GPU(s) available.\n",
      "CUDA is available. Using GPU: NVIDIA L40S\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Add the parent directory to the Python path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname(os.getcwd()), '..')))\n",
    "\n",
    "from transformers import TrainingArguments, EarlyStoppingCallback\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from datasets import Dataset, DatasetDict, load_from_disk\n",
    "\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "\n",
    "from utils import parse_ratings, tokenize_fn_with_best_window, tokenize_fn_basic_batched, CustomRewardTrainer, find_best_window, convert_label_to_int\n",
    "\n",
    "\n",
    "# from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# load the relevant devices available on the server\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = os.getenv(\"AVAILABLE_DEVICES\")\n",
    "\n",
    "# Enable expandable CUDA segments\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# load cuda\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print(\"CUDA is available. Using GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA is not available. Using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training variables\n",
    "FEEDBACK_TO_TRAIN_ON = os.getenv(\"FEEDBACK_TO_TRAIN_ON\")\n",
    "FEEDBACK_TO_REMOVE = os.getenv(\"FEEDBACK_TO_REMOVE\")\n",
    "MODEL = os.getenv(\"REWARD_MODEL\")\n",
    "DATASET = os.getenv(\"REWARD_DATASET\")\n",
    "TOKENIZE_FN = os.getenv(\"TOKENIZE_FN\")\n",
    "MAX_LENGTH = os.getenv(\"MAX_LENGTH\")\n",
    "STRIDE = os.getenv(\"STRIDE\")\n",
    "LORA_CHECKPOINTS_FOLDER = os.getenv(\"LORA_CHECKPOINTS_FOLDER\")\n",
    "FINAL_LORA_ADAPTERS = os.getenv(\"FINAL_LORA_ADAPTERS_FOLDER\") + f\"_{FEEDBACK_TO_TRAIN_ON}_{TOKENIZE_FN}_{DATASET}\"\n",
    "TOKENIZED_DATA_TRAIN = os.getenv(\"TOKENIZED_DATA\") + f\"_{FEEDBACK_TO_TRAIN_ON}_{TOKENIZE_FN}_{DATASET}_{MODEL}_train\"\n",
    "TOKENIZED_DATA_EVAL = os.getenv(\"TOKENIZED_DATA\") + f\"_{FEEDBACK_TO_TRAIN_ON}_{TOKENIZE_FN}_{DATASET}_{MODEL}_eval\"\n",
    "TOKENIZED_DATA_TEST = os.getenv(\"TOKENIZED_DATA\") + f\"_{FEEDBACK_TO_TRAIN_ON}_{TOKENIZE_FN}_{DATASET}_{MODEL}_test\"\n",
    "DATASET_STRUCTURE = os.getenv(\"DATASET_STRUCTURE\")\n",
    "\n",
    "REWARD_DATA_PATH = os.getenv(\"REWARD_DATA_PATH\")\n",
    "\n",
    "if DATASET_STRUCTURE == \"determined\":\n",
    "    REWARD_MODEL_TRAIN_DATA_HUMAN = REWARD_DATA_PATH + \"/train_human_determined.csv\"\n",
    "    REWARD_MODEL_EVAL_DATA_HUMAN = REWARD_DATA_PATH + \"/validation_human_determined.csv\"\n",
    "    REWARD_MODEL_TEST_DATA_HUMAN = REWARD_DATA_PATH + \"/test_human_determined.csv\"\n",
    "\n",
    "    REWARD_MODEL_TRAIN_DATA_SYNTH = REWARD_DATA_PATH + \"/train_synth_determined.csv\"\n",
    "    REWARD_MODEL_EVAL_DATA_SYNTH = REWARD_DATA_PATH + \"/validation_synth_determined.csv\"\n",
    "    REWARD_MODEL_TEST_DATA_SYNTH = REWARD_DATA_PATH + \"/test_synth_determined.csv\"\n",
    "\n",
    "elif DATASET_STRUCTURE == \"random\":\n",
    "    REWARD_MODEL_TRAIN_DATA_HUMAN = REWARD_DATA_PATH + \"/train_human_random.csv\"\n",
    "    REWARD_MODEL_EVAL_DATA_HUMAN = REWARD_DATA_PATH + \"/validation_human_random.csv\"\n",
    "    REWARD_MODEL_TEST_DATA_HUMAN = REWARD_DATA_PATH + \"/test_human_random.csv\"\n",
    "\n",
    "    REWARD_MODEL_TRAIN_DATA_SYNTH = REWARD_DATA_PATH + \"/train_synth_random.csv\"\n",
    "    REWARD_MODEL_EVAL_DATA_SYNTH = REWARD_DATA_PATH + \"/validation_synth_random.csv\"\n",
    "    REWARD_MODEL_TEST_DATA_SYNTH = REWARD_DATA_PATH + \"/test_synth_random.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset loading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load dataframes\n",
    "# df_1 = pd.read_csv(FILE_1, sep=\";\")\n",
    "# df_5 = pd.read_csv(FILE_5, sep=\";\")\n",
    "# df_7 = pd.read_csv(FILE_7, sep=\";\")\n",
    "# df_9 = pd.read_csv(FILE_9, sep=\";\")\n",
    "# df_10_1 = pd.read_csv(FILE_10_1, sep=\";\")\n",
    "# df_10_2 = pd.read_csv(FILE_10_2, sep=\";\")\n",
    "# df_synth = pd.read_csv(FILE_SYNTH, sep=\";\")\n",
    "\n",
    "# df_human = pd.concat([df_1, df_5, df_7, df_9, df_10_1, df_10_2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Re-structure df synthetic to fit in training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Synthetic feedback shape:\", df_synth.shape)\n",
    "\n",
    "\n",
    "# # Save the current headers since forgot to store headers in csv file\n",
    "# old_headers = df_synth.columns.tolist()\n",
    "\n",
    "# # print(\"Old headers:\", old_headers)\n",
    "\n",
    "# # Step 2: Insert the headers as the first row\n",
    "# df_synth.loc[-1] = old_headers # Add headers as a new row\n",
    "# df_synth.index = df_synth.index + 1 # Shift index\n",
    "# df_synth = df_synth.sort_index() # Sort index to place the new row at the top\n",
    "\n",
    "\n",
    "# # Step 3: Assign new headers (optional)\n",
    "# df_synth.columns = ['file', \n",
    "#                     'frame_ID', \n",
    "#                     'frame_type', \n",
    "#                     'frame_text', \n",
    "#                     'precondition_id', \n",
    "#                     'precondition_text', \n",
    "#                     'precondition_position', \n",
    "#                     'response_text', \n",
    "#                     'prompt_config_examples', \n",
    "#                     'prompt_config_chain_of_thought', \n",
    "#                     'feedback_extraction', \n",
    "#                     'feedback_detection', \n",
    "#                     'additional_feedback',\n",
    "#                     'synthetic_feedback',\n",
    "#                 ]\n",
    "\n",
    "# print(df_synth.columns)\n",
    "\n",
    "\n",
    "# df_synth['prompt_config_examples'] = (df_synth['prompt_config_examples']                                              \n",
    "#                                                 .astype(str)\n",
    "#                                                 .str.strip()\n",
    "#                                                 .str.lower()\n",
    "#                                                 .map({'true': True, 'false': False})\n",
    "# )\n",
    "\n",
    "# df_synth['prompt_config_chain_of_thought'] = (df_synth['prompt_config_chain_of_thought']\n",
    "#                                                 .astype(str)\n",
    "#                                                 .str.strip()\n",
    "#                                                 .str.lower()\n",
    "#                                                 .map({'true': True, 'false': False})\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "# print(\"Synthetic feedback shape:\", df_synth.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['file', 'frame_ID', 'frame_type', 'frame_text', 'precondition_id',\n",
       "       'precondition_text', 'precondition_position', 'response_text',\n",
       "       'prompt_config_examples', 'prompt_config_chain_of_thought',\n",
       "       'feedback_extraction', 'feedback_detection', 'additional_feedback'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if DATASET == \"human\":\n",
    "    df_train = pd.read_csv(REWARD_MODEL_TRAIN_DATA_HUMAN, sep=\";\")\n",
    "    df_eval = pd.read_csv(REWARD_MODEL_EVAL_DATA_HUMAN, sep=\";\")\n",
    "    df_test = pd.read_csv(REWARD_MODEL_TEST_DATA_HUMAN, sep=\";\")\n",
    "elif DATASET == \"synthetic\":\n",
    "    df_train = pd.read_csv(REWARD_MODEL_TRAIN_DATA_SYNTH, sep=\";\")\n",
    "    df_eval = pd.read_csv(REWARD_MODEL_EVAL_DATA_SYNTH, sep=\";\")\n",
    "    df_test = pd.read_csv(REWARD_MODEL_TEST_DATA_SYNTH, sep=\";\")\n",
    "    \n",
    "    \n",
    "df_train.shape\n",
    "df_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. a) Parse ratings to numeric values for MSE Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed feedback for extraction: 0    2\n",
      "1    2\n",
      "2    2\n",
      "3    2\n",
      "4    3\n",
      "Name: feedback_extraction, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df_train[FEEDBACK_TO_TRAIN_ON] = [parse_ratings(feedback) for feedback in df_train[FEEDBACK_TO_TRAIN_ON]]\n",
    "df_eval[FEEDBACK_TO_TRAIN_ON] = [parse_ratings(feedback) for feedback in df_eval[FEEDBACK_TO_TRAIN_ON]]\n",
    "df_test[FEEDBACK_TO_TRAIN_ON] = [parse_ratings(feedback) for feedback in df_test[FEEDBACK_TO_TRAIN_ON]]\n",
    "print(\"Parsed feedback for extraction:\", df_train[FEEDBACK_TO_TRAIN_ON][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. b) look at biases in feedback to train on for weights in RL loop --> feedback_detection is very biased through way it was collected, so gets less weight overall..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feedback_extraction\n",
       "0    354\n",
       "3    153\n",
       "1     84\n",
       "2     83\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[FEEDBACK_TO_TRAIN_ON].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. c) keep only relevant feedback column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['file', 'frame_ID', 'frame_type', 'frame_text', 'precondition_id', 'precondition_text', 'precondition_position', 'response_text', 'prompt_config_examples', 'prompt_config_chain_of_thought', 'feedback_extraction', 'feedback_detection', 'additional_feedback'],\n",
      "    num_rows: 674\n",
      "})\n",
      "feedback_extraction\n"
     ]
    }
   ],
   "source": [
    "dataset_train = Dataset.from_pandas(df_train)\n",
    "dataset_eval = Dataset.from_pandas(df_eval)\n",
    "dataset_test = Dataset.from_pandas(df_test)\n",
    "\n",
    "print(dataset_train)\n",
    "print(FEEDBACK_TO_TRAIN_ON) \n",
    "\n",
    "datasets = [dataset_train, dataset_eval, dataset_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2', '2', '2', '2', '3', '2', '2', '2', '2', '3', '0', '0', '2', '2', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '3', '3', '0', '0', '2', '0', '0', '0', '0', '0', '0', '0', '0', '0', '3', '3', '3', '3', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '2', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '0', '3', '3', '3', '3', '3', '3', '2', '3', '3', '3', '3', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '2', '3', '1', '1', '3', '0', '2', '2', '1', '3', '0', '0', '2', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '2', '0', '1', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '2', '3', '1', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '2', '2', '2', '2', '1', '3', '0', '0', '2', '2', '2', '0', '2', '2', '2', '3', '2', '2', '1', '0', '3', '2', '2', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '1', '1', '1', '1', '1', '1', '0', '0', '0', '3', '0', '0', '0', '1', '0', '0', '0', '0', '1', '0', '1', '0', '0', '2', '2', '1', '1', '1', '3', '1', '1', '1', '1', '1', '0', '0', '0', '0', '0', '0', '0', '1', '1', '1', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '1', '1', '1', '0', '0', '0', '0', '3', '0', '0', '2', '2', '3', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '2', '2', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '2', '3', '2', '2', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '3', '0', '3', '0', '3', '1', '3', '3', '0', '0', '3', '3', '2', '2', '3', '2', '2', '2', '2', '3', '0', '0', '0', '2', '0', '0', '0', '0', '0', '1', '1', '0', '0', '0', '0', '1', '2', '2', '1', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '3', '3', '3', '3', '0', '0', '1', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '1', '0', '3', '3', '3', '3', '2', '3', '3', '3', '3', '3', '3', '0', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '2', '2', '2', '2', '2', '3', '3', '2', '2', '2', '2', '3', '3', '2', '2', '2', '3', '3', '0', '3', '0', '0', '0', '0', '1', '0', '1', '1', '1', '1', '1', '1', '3', '3', '3', '1', '1', '1', '3', '3', '3', '3', '3', '3', '0', '0', '0', '0', '0', '0', '0', '0', '3', '3', '3', '0', '0', '0', '0', '1', '0', '0', '0', '0', '1', '0', '0', '1', '1', '0', '3', '3', '3', '3', '2', '3', '1', '2', '3', '1', '3', '3', '3', '3', '0', '0', '2', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '3', '3', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '2', '3', '2', '2', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '3', '3', '3', '3', '0', '3', '3', '0', '3', '3', '3', '0', '3', '3', '3', '3', '3', '3', '3', '2', '3', '3', '3', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '1', '1', '0', '3', '0', '1', '0', '1', '0', '0', '1', '0', '2', '2', '2', '2', '3', '3', '3', '1', '1', '1', '3', '2', '2', '2', '2', '2', '0', '0', '0', '0', '0', '0', '0', '3', '3', '2', '2', '0', '0', '0', '0', '0', '0', '1', '1', '0', '1', '1', '1', '1', '1', '1', '2', '1', '1', '1', '3', '0', '3', '0', '3', '1', '3', '3', '0', '0']\n"
     ]
    }
   ],
   "source": [
    "datasets= [dataset.remove_columns([FEEDBACK_TO_REMOVE]) for dataset in datasets]\n",
    "datasets = [dataset.rename_column(FEEDBACK_TO_TRAIN_ON, \"label\") for dataset in datasets]\n",
    "\n",
    "print(datasets[0][\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load model with LoRA layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ModernBertForSequenceClassification were not initialized from the model checkpoint at answerdotai/ModernBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModernBertForSequenceClassification(\n",
      "  (model): ModernBertModel(\n",
      "    (embeddings): ModernBertEmbeddings(\n",
      "      (tok_embeddings): Embedding(50368, 768, padding_idx=50283)\n",
      "      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (drop): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): ModernBertEncoderLayer(\n",
      "        (attn_norm): Identity()\n",
      "        (attn): ModernBertAttention(\n",
      "          (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
      "          (rotary_emb): ModernBertRotaryEmbedding()\n",
      "          (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
      "          (out_drop): Identity()\n",
      "        )\n",
      "        (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): ModernBertMLP(\n",
      "          (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
      "          (act): GELUActivation()\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "          (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
      "        )\n",
      "      )\n",
      "      (1-21): 21 x ModernBertEncoderLayer(\n",
      "        (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): ModernBertAttention(\n",
      "          (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
      "          (rotary_emb): ModernBertRotaryEmbedding()\n",
      "          (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
      "          (out_drop): Identity()\n",
      "        )\n",
      "        (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): ModernBertMLP(\n",
      "          (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
      "          (act): GELUActivation()\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "          (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (head): ModernBertPredictionHead(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=False)\n",
      "    (act): GELUActivation()\n",
      "    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (drop): Dropout(p=0.0, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Load the model and the tokenizer\n",
    "model_id = MODEL \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_id, num_labels=1) # num_labels = 1 since we want to prodict a single scalar (the rating)\n",
    "\n",
    "# Comment: Automodel for sequence classification with num_labels=1 already has a regression head\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8192\n",
      "8192\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.model_max_length)\n",
    "print(model.config.max_position_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,149,697 || all params: 150,755,330 || trainable%: 0.7626\n"
     ]
    }
   ],
   "source": [
    "# Define LoRA config\n",
    "\n",
    "\n",
    "if MODEL == \"answerdotai/ModernBERT-base\":\n",
    "\n",
    "    lora_config = LoraConfig(\n",
    "    r=8,           # Rank of the LoRA matrices (smaller = less memory)\n",
    "    lora_alpha=16, # Scaling factor (higher = stronger adaptation)\n",
    "    target_modules=[\"Wqkv\", \"Wo\"], # Apply LoRA to attention layers\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_CLS\"  # classify each answer \n",
    "    )\n",
    "else:\n",
    "    lora_config = LoraConfig(\n",
    "    r=8,           # Rank of the LoRA matrices (smaller = less memory)\n",
    "    lora_alpha=16, # Scaling factor (higher = stronger adaptation)\n",
    "    target_modules=[\"query\", \"key\", \"value\"], # Apply LoRA to attention layers\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_CLS\"  # classify each answer \n",
    "    )\n",
    "    \n",
    "\n",
    "# Freeze base model\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "\n",
    "# Convert the model to a PEFT (LoRA) model\n",
    "model = get_peft_model(model, lora_config)\n",
    "# model.gradient_checkpointing_enable()\n",
    "model.print_trainable_parameters()  # Check trainable params (~0.1% of full model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[50281, 1276, 310, 253, 5347, 273, 6181, 32, 50282, 50283, 50283], [50281, 1276, 310, 253, 6253, 5347, 275, 253, 1533, 32, 50282]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test tokenizer\n",
    "sample_data = [\"What is the capital of France?\", \"What is the largest capital in the world?\"]\n",
    "tokenizer(sample_data, padding=True, truncation=True, max_length=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Encode dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['file', 'frame_ID', 'frame_type', 'frame_text', 'precondition_id', 'precondition_text', 'precondition_position', 'response_text', 'prompt_config_examples', 'prompt_config_chain_of_thought', 'label', 'additional_feedback']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 674/674 [00:00<00:00, 1818.20 examples/s]\n",
      "Map: 100%|██████████| 127/127 [00:00<00:00, 7866.10 examples/s]\n",
      "Map: 100%|██████████| 128/128 [00:00<00:00, 9701.32 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2, 2, 2, 3]\n",
      "['1. Subfact: Onze Minister\\n                2. Positie: Artikel 1, sectie 1 IN Vreemdelingenwet geldig vanaf 2024\\n                3. Subfact: Onze Minister\\n                4. Positie: Artikel 8, sectie 1 IN Vreemdelingenwet geldig vanaf 2024\\n                5. Subfact: Onze Minister\\n                6. Positie: Artikel 14, sectie 1 IN Vreemdelingenwet geldig vanaf 2024\\n                7. Subfact: Onze Minister\\n                8. Positie: Artikel 16, sectie 1 IN Vreemdelingenwet geldig vanaf 2024\\n                9. Subfact: Onze Minister\\n                10. Positie: Artikel 17, sectie 1 IN Vreemdelingenwet geldig vanaf 2024\\n                11. Subfact: Onze Minister\\n                12. Positie: Artikel 17a, sectie 1 IN Vreemdelingenwet geldig vanaf 2024\\n                13. Subfact: Onze Minister\\n                14. Positie: Artikel 26, sectie 1 IN Vreemdelingenwet geldig vanaf 2024\\n                15. Subfact: Onze Minister\\n                16. Positie: Artikel 27, sectie 1 IN Vreemdelingenwet geldig vanaf 2024\\n                17. Subfact: Onze Minister\\n                18. Positie: Artikel 66a, sectie 6 IN Vreemdelingenwet geldig vanaf 2024\\n                19. Subfact: Onze Minister\\n                20. Positie: Artikel 67, sectie 3 IN Vreemdelingenwet geldig vanaf 2024\\n', '1. Subfact: Onze Minister\\n                2. Positie: Artikel 1, sectie 1 IN Vreemdelingenwet geldig vanaf 2024\\n                3. Subfact: Onze Minister\\n                4. Positie: Artikel 8, sectie 1 IN Vreemdelingenwet geldig vanaf 2024\\n                5. Subfact: Onze Minister\\n                6. Positie: Artikel 14, sectie 1 IN Vreemdelingenwet geldig vanaf 2024\\n                7. Subfact: Onze Minister\\n                8. Positie: Artikel 16, sectie 1 IN Vreemdelingenwet geldig vanaf 2024\\n                9. Subfact: Onze Minister\\n                10. Positie: Artikel 17, sectie 1 IN Vreemdelingenwet geldig vanaf 2024\\n                11. Subfact: Onze Minister\\n                12. Positie: Artikel 17a, sectie 1 IN Vreemdelingenwet geldig vanaf 2024\\n                13. Subfact: Onze Minister\\n                14. Positie: Artikel 26, sectie 1 IN Vreemdelingenwet geldig vanaf 2024\\n                15. Subfact: Onze Minister\\n                16. Positie: Artikel 27, sectie 1 IN Vreemdelingenwet geldig vanaf 2024\\n                17. Subfact: Onze Minister\\n                18. Positie: Artikel 66a, sectie 6 IN Vreemdelingenwet geldig vanaf 2024\\n                19. Subfact: Onze Minister\\n                20. Positie: Artikel 67, sectie 3 IN Vreemdelingenwet geldig vanaf 2024\\n', '1. Subfact: Onze Minister\\n                2. Positie: Artikel 1, sectie 1 IN Vreemdelingenwet geldig vanaf 2024\\n                3. Subfact: Onze Minister\\n                4. Positie: Artikel 8, sectie 1 IN Vreemdelingenwet geldig vanaf 2024\\n                5. Subfact: Onze Minister\\n                6. Positie: Artikel 14, sectie 1 IN Vreemdelingenwet geldig vanaf 2024\\n                7. Subfact: Onze Minister\\n                8. Positie: Artikel 16, sectie 1 IN Vreemdelingenwet geldig vanaf 2024\\n                9. Subfact: Onze Minister\\n                10. Positie: Artikel 17, sectie 1 IN Vreemdelingenwet geldig vanaf 2024\\n                11. Subfact: Onze Minister\\n                12. Positie: Artikel 17a, sectie 1 IN Vreemdelingenwet geldig vanaf 2024\\n                13. Subfact: Onze Minister\\n                14. Positie: Artikel 26, sectie 1 IN Vreemdelingenwet geldig vanaf 2024\\n                15. Subfact: Onze Minister\\n                16. Positie: Artikel 27, sectie 1 IN Vreemdelingenwet geldig vanaf 2024\\n                17. Subfact: Onze Minister\\n                18. Positie: Artikel 66a, sectie 6 IN Vreemdelingenwet geldig vanaf 2024\\n                19. Subfact: Onze Minister\\n                20. Positie: Artikel 67, sectie 3 IN Vreemdelingenwet geldig vanaf 2024\\n', '1. Subfact: Onze Minister\\n\\n                2. Positie: Artikel 1, sectie 1 IN Algemene wet bestuursrecht\\n\\n                3. Positie: Artikel 1, sectie 1 IN Vreemdelingenwet geldig vanaf 2024\\n\\n                4. Positie: Artikel 14, 1 IN Vreemdelingenwet geldig vanaf 2024\\n\\n                5. Positie: Artikel 1, sub 2° IN Vreemdelingenwet geldig vanaf 2024\\n\\n                6. Positie: Artikel 1, sub 4° IN Vreemdelingenwet geldig vanaf 2024\\n\\n                7. Positie: Artikel 1, sub 6° IN Vreemdelingenwet geldig vanaf 2024\\n\\n                8. Positie: Artikel 1, sub 2° IN Vreemdelingenwet geldig vanaf 2024, Afdeling 3. De verblijfsvergunning regulier\\n\\n                9. Positie: Artikel 9, 1 IN Vreemdelingenwet geldig vanaf 2024\\n\\n                10. Positie: Artikel 14, 1a IN Vreemdelingenwet geldig vanaf 2024\\n\\n                11. Positie: Artikel 14, 1b IN Vreemdelingenwet geldig vanaf 2024\\n\\n                12. Positie: Artikel 14, 1c IN Vreemdelingenwet geldig vanaf 2024\\n\\n                13. Positie: Artikel 14, 1d IN Vreemdelingenwet geldig vanaf 2024\\n\\n                14. Positie: Artikel 14, 1e IN Vreemdel', '1. Subfact: Onze Minister\\n\\n                2. Positie: Artikel 1, sectie 1 IN Algemene wet bestuursrecht\\n\\n                3. Positie: Artikel 1, sectie 1 IN Vreemdelingenwet geldig vanaf 2024\\n\\n                4. Positie: Artikel 14, 1 IN Vreemdelingenwet geldig vanaf 2024\\n\\n                5. Positie: Artikel 1, sub 2° IN Vreemdelingenwet geldig vanaf 2024\\n\\n                6. Positie: Artikel 1, sub 4° IN Vreemdelingenwet geldig vanaf 2024\\n\\n                7. Positie: Artikel 1, sub 6° IN Vreemdelingenwet geldig vanaf 2024\\n\\n                8. Positie: Artikel 1, sub 2° IN Vreemdelingenwet geldig vanaf 2024, Afdeling 3. De verblijfsvergunning regulier\\n\\n                9. Positie: Artikel 9, 1 IN Vreemdelingenwet geldig vanaf 2024\\n\\n                10. Positie: Artikel 14, 1a IN Vreemdelingenwet geldig vanaf 2024\\n\\n                11. Positie: Artikel 14, 1b IN Vreemdelingenwet geldig vanaf 2024\\n\\n                12. Positie: Artikel 14, 1c IN Vreemdelingenwet geldig vanaf 2024\\n\\n                13. Positie: Artikel 14, 1d IN Vreemdelingenwet geldig vanaf 2024\\n\\n                14. Positie: Artikel 14, 1e IN Vreemdel']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(datasets[0].column_names)\n",
    "# mao string labels to integers\n",
    "datasets = [dataset.map(convert_label_to_int) for dataset in datasets]\n",
    "\n",
    "print(datasets[0][\"label\"][:5])  # Check labels\n",
    "print(datasets[0][\"response_text\"][:5])  # Check labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comment\n",
    "\n",
    "1. Needed for feedback extraction: precondition_text, response_text, label(rating feedback extraction)\n",
    "2. Needed for feedback detection: precondition_text, precondition_position, response_text, label (rating feedback detection)\n",
    "3. For the precondition position to be found well, it is a crucial for the model to find the precondition text (at least to a recognizable degree) as well, otherwise the precondition is not found at all..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Titel: De Weg Door Het Leven\n",
      "\n",
      "Het leven is een reis vol onverwachte wendingen, een pad dat zich zelden rechtlijnig ontvouwt. Vanaf het moment dat we onze eerste ademhaling nemen, worden we ondergedompeld in een wereld die we nog moeten leren begrijpen. Als kind lijkt alles eenvoudig: lachen, spelen, ontdekken. Maar naarmate we ouder worden, beginnen de lagen van complexiteit zich op te stapelen. We leren dat mensen niet altijd zeggen wat ze bedoelen, dat keuzes consequenties hebben, en dat geluk soms vluchtiger is dan we zouden willen.\n",
      "\n",
      "In de vroege ochtenden, wanneer de zon net boven de horizon verschijnt en de wereld nog stil is, denken velen na over hun plaats in het grotere geheel. Sommigen vragen zich af of ze de juiste keuzes hebben gemaakt, of ze trouw zijn gebleven aan zichzelf. Anderen proberen simpelweg de dag door te komen, met hoop op iets beters. In die momenten van stilte komt vaak het besef dat, hoewel we allemaal verschillende paden bewandelen, we één waarheid delen: dat het leven, ondanks al onze inspanningen en verlangens, nooit gemakkelijk is. Of, zoals mijn grootmoeder het ooit zei terwijl ze haar handen vouwde na een lange dag werken op het land: “Je moet weten, kind, het leven is nooit gemakkelijk, maar het is wel de moeite waard.”\n",
      "\n",
      "We worden gevormd door onze ervaringen, door de mensen die we ontmoeten en de obstakels die we overwinnen. Elke fout, elk succes, elke traan en elke glimlach draagt bij aan wie we zijn. En toch, ondanks al die ervaringen, blijven we zoek\n"
     ]
    }
   ],
   "source": [
    "# Code to test bestw indow function\n",
    "\n",
    "test_text = \"\"\"\n",
    "        Titel: De Weg Door Het Leven\n",
    "\n",
    "Het leven is een reis vol onverwachte wendingen, een pad dat zich zelden rechtlijnig ontvouwt. Vanaf het moment dat we onze eerste ademhaling nemen, worden we ondergedompeld in een wereld die we nog moeten leren begrijpen. Als kind lijkt alles eenvoudig: lachen, spelen, ontdekken. Maar naarmate we ouder worden, beginnen de lagen van complexiteit zich op te stapelen. We leren dat mensen niet altijd zeggen wat ze bedoelen, dat keuzes consequenties hebben, en dat geluk soms vluchtiger is dan we zouden willen.\n",
    "\n",
    "In de vroege ochtenden, wanneer de zon net boven de horizon verschijnt en de wereld nog stil is, denken velen na over hun plaats in het grotere geheel. Sommigen vragen zich af of ze de juiste keuzes hebben gemaakt, of ze trouw zijn gebleven aan zichzelf. Anderen proberen simpelweg de dag door te komen, met hoop op iets beters. In die momenten van stilte komt vaak het besef dat, hoewel we allemaal verschillende paden bewandelen, we één waarheid delen: dat het leven, ondanks al onze inspanningen en verlangens, nooit gemakkelijk is. Of, zoals mijn grootmoeder het ooit zei terwijl ze haar handen vouwde na een lange dag werken op het land: “Je moet weten, kind, het leven is nooit gemakkelijk, maar het is wel de moeite waard.”\n",
    "\n",
    "We worden gevormd door onze ervaringen, door de mensen die we ontmoeten en de obstakels die we overwinnen. Elke fout, elk succes, elke traan en elke glimlach draagt bij aan wie we zijn. En toch, ondanks al die ervaringen, blijven we zoeken. Naar betekenis. Naar verbinding. Naar rust.\n",
    "\n",
    "Soms lijkt het alsof de wereld te snel draait. Technologie verandert ons leven in een razend tempo, verwachtingen worden hoger, en de druk om te presteren neemt toe. In die chaos vergeten we soms stil te staan. Te ademen. Te voelen. Maar juist in die momenten van rust vinden we vaak de antwoorden die we zo hard nodig hebben.\n",
    "\n",
    "De liefde, bijvoorbeeld, is een van de krachtigste krachten die ons voortdrijft. Liefde voor een partner, een kind, een vriend, of zelfs voor een passie. Het is die liefde die ons helpt vol te houden wanneer alles tegenzit. Die ons eraan herinnert waarom we begonnen zijn, waarom we blijven proberen.\n",
    "\n",
    "En dan is er verlies. Een onvermijdelijk onderdeel van het leven. We verliezen mensen, kansen, dromen. Maar in dat verlies schuilt ook groei. We leren loslaten, opnieuw beginnen, sterker worden. Het is pijnlijk, ja, maar ook noodzakelijk.\n",
    "\n",
    "Wanneer we terugkijken op ons leven, zijn het zelden de materiële zaken die we herinneren. Het zijn de momenten. De gesprekken bij kaarslicht. De wandelingen in de regen. De onverwachte lachbuien. De stilte van een gedeeld verdriet. Die momenten vormen de essentie van ons bestaan.\n",
    "\n",
    "Dus ja, het leven is vol uitdagingen. Het is rommelig, verwarrend, soms oneerlijk. Maar het is ook prachtig, rijk aan betekenis, en gevuld met kansen om te groeien, te leren en lief te hebben. En misschien is dat wel de grootste les van allemaal: dat we, ondanks alles, blijven kiezen voor hoop. Voor verbinding. Voor het leven zelf.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "test_ground_truth = \"Het leven is nooit gemakkelijk.\"\n",
    "\n",
    "print(find_best_window(test_text, test_ground_truth, device, tokenizer))\n",
    "\n",
    "# Works as expectd, I am impressed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 674/674 [00:02<00:00, 233.69 examples/s]\n",
      "Map: 100%|██████████| 127/127 [00:00<00:00, 163.04 examples/s]\n",
      "Map: 100%|██████████| 128/128 [00:00<00:00, 155.53 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 674/674 [00:00<00:00, 50989.52 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 127/127 [00:00<00:00, 14139.85 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 128/128 [00:00<00:00, 19254.42 examples/s]\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(TOKENIZED_DATA_TRAIN):\n",
    "    if TOKENIZE_FN == \"best_window\":\n",
    "        datasets = [dataset.map(tokenize_fn_with_best_window, \n",
    "                                fn_kwargs={\"feedback_train\": FEEDBACK_TO_TRAIN_ON, \n",
    "                                            \"tokenizer\": tokenizer, \n",
    "                                            \"max_length\": int(MAX_LENGTH), \n",
    "                                            \"stride\": int(STRIDE),\n",
    "                                            \"device\": device\n",
    "                                            },\n",
    "                                batched=False) for dataset in datasets]\n",
    "    else:\n",
    "        datasets = [dataset.map(tokenize_fn_basic_batched, \n",
    "                                fn_kwargs={\"feedback_train\": FEEDBACK_TO_TRAIN_ON, \n",
    "                                            \"tokenizer\": tokenizer \n",
    "                                            },\n",
    "                                batched=True) for dataset in datasets]\n",
    "    \n",
    "    \n",
    "    datasets[0].save_to_disk(TOKENIZED_DATA_TRAIN)\n",
    "    datasets[1].save_to_disk(TOKENIZED_DATA_EVAL)\n",
    "    datasets[2].save_to_disk(TOKENIZED_DATA_TEST)\n",
    "else:\n",
    "    datasets[0] = load_from_disk(TOKENIZED_DATA_TRAIN)\n",
    "    datasets[1] = load_from_disk(TOKENIZED_DATA_TEST)\n",
    "    datasets[2] = load_from_disk(TOKENIZED_DATA_EVAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'Interpretatie_Vw_over_besluiten_op_aanvragen_voor_een_verblijfsvergunning_regulier_bepaalde_tijd.json': 432, 'rijksbegrotingscyclus.json': 242})\n"
     ]
    }
   ],
   "source": [
    "print(Counter(datasets[0]['file']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train reward model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-18 15:19:41,005] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacques.furst/miniconda3/envs/RL/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/home/jacques.furst/miniconda3/envs/RL/compiler_compat/ld: cannot find -lcufile: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-18 15:19:41,386] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=LORA_CHECKPOINTS_FOLDER,\n",
    "    eval_strategy='steps',\n",
    "    save_strategy='steps',\n",
    "    save_steps=10,\n",
    "    eval_steps=10,\n",
    "    save_total_limit=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    learning_rate=3e-4,\n",
    "    num_train_epochs=20,\n",
    "    logging_steps=10,\n",
    "    label_names=[\"labels\"],\n",
    "    # report_to=\"none\",\n",
    "    logging_dir=\"./logs\",\n",
    "    fp16=True,  # Use mixed precision training\n",
    "    metric_for_best_model=\"eval_loss\", # or \"eval_loss\"\n",
    "    greater_is_better=False, # False if using loss\n",
    "    # gradient_accumulation_steps=4, # \n",
    "    # weight_decay=0.01\n",
    ")\n",
    "\n",
    "# Initialize custom trainer\n",
    "trainer = CustomRewardTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=datasets[0],\n",
    "    eval_dataset=datasets[1],\n",
    "    # compute_metrics=trainer.compute_metrics,  # Use the custom metrics function\n",
    "    processing_class=tokenizer,\n",
    "    loss_type=\"huber\",  # \"mse\" or \"huber\"\n",
    "    weight_strategy=\"linear\",  # \"linear\", \"inverse\", or None\n",
    "    # callbacks=[EarlyStoppingCallback(early_stopping_patience=50)] # use early stopping since we are sing high amount of epochs\n",
    "    # data_collator=RewardDataCollator()\n",
    ")\n",
    "\n",
    "print(trainer.args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjacques-furst123\u001b[0m (\u001b[33mjacques-furst123-none\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jacques.furst/development/RAG/flintfiller-precondition-rl/rl_training_new/wandb/run-20250618_151945-qanmrzvp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jacques-furst123-none/huggingface/runs/qanmrzvp' target=\"_blank\">/home/jacques.furst/development/RAG/flintfiller-precondition-rl/reward_training_files/lora_checkpoints</a></strong> to <a href='https://wandb.ai/jacques-furst123-none/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jacques-furst123-none/huggingface' target=\"_blank\">https://wandb.ai/jacques-furst123-none/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jacques-furst123-none/huggingface/runs/qanmrzvp' target=\"_blank\">https://wandb.ai/jacques-furst123-none/huggingface/runs/qanmrzvp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacques.furst/miniconda3/envs/RL/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:236: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "Caught OutOfMemoryError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/home/jacques.furst/miniconda3/envs/RL/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 97, in _worker\n    output = module(*input, **kwargs)\n  File \"/home/jacques.furst/miniconda3/envs/RL/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/jacques.furst/miniconda3/envs/RL/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/jacques.furst/miniconda3/envs/RL/lib/python3.10/site-packages/peft/peft_model.py\", line 1559, in forward\n    return self.base_model(\n  File \"/home/jacques.furst/miniconda3/envs/RL/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/jacques.furst/miniconda3/envs/RL/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/jacques.furst/miniconda3/envs/RL/lib/python3.10/site-packages/peft/tuners/tuners_utils.py\", line 193, in forward\n    return self.model.forward(*args, **kwargs)\n  File \"/home/jacques.furst/miniconda3/envs/RL/lib/python3.10/site-packages/transformers/models/modernbert/modeling_modernbert.py\", line 1166, in forward\n    outputs = self.model(\n  File \"/home/jacques.furst/miniconda3/envs/RL/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/jacques.furst/miniconda3/envs/RL/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/jacques.furst/miniconda3/envs/RL/lib/python3.10/site-packages/transformers/models/modernbert/modeling_modernbert.py\", line 881, in forward\n    layer_outputs = encoder_layer(\n  File \"/home/jacques.furst/miniconda3/envs/RL/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/jacques.furst/miniconda3/envs/RL/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/jacques.furst/miniconda3/envs/RL/lib/python3.10/site-packages/transformers/models/modernbert/modeling_modernbert.py\", line 548, in forward\n    self.compiled_mlp(hidden_states)\n  File \"/home/jacques.furst/miniconda3/envs/RL/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 655, in _fn\n    return fn(*args, **kwargs)\n  File \"/home/jacques.furst/miniconda3/envs/RL/lib/python3.10/site-packages/transformers/models/modernbert/modeling_modernbert.py\", line 523, in compiled_mlp\n    @torch.compile(dynamic=True)\n  File \"/home/jacques.furst/miniconda3/envs/RL/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 838, in _fn\n    return fn(*args, **kwargs)\n  File \"/home/jacques.furst/miniconda3/envs/RL/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py\", line 1209, in forward\n    return compiled_fn(full_args)\n  File \"/home/jacques.furst/miniconda3/envs/RL/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py\", line 315, in runtime_wrapper\n    all_outs = call_func_at_runtime_with_args(\n  File \"/home/jacques.furst/miniconda3/envs/RL/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py\", line 126, in call_func_at_runtime_with_args\n    out = normalize_as_list(f(args))\n  File \"/home/jacques.furst/miniconda3/envs/RL/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py\", line 100, in g\n    return f(*args)\n  File \"/home/jacques.furst/miniconda3/envs/RL/lib/python3.10/site-packages/torch/autograd/function.py\", line 575, in apply\n    return super().apply(*args, **kwargs)  # type: ignore[misc]\n  File \"/home/jacques.furst/miniconda3/envs/RL/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py\", line 1937, in forward\n    fw_outs = call_func_at_runtime_with_args(\n  File \"/home/jacques.furst/miniconda3/envs/RL/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py\", line 126, in call_func_at_runtime_with_args\n    out = normalize_as_list(f(args))\n  File \"/home/jacques.furst/miniconda3/envs/RL/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py\", line 495, in wrapper\n    return compiled_fn(runtime_args)\n  File \"/home/jacques.furst/miniconda3/envs/RL/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py\", line 689, in inner_fn\n    outs = compiled_fn(args)\n  File \"/home/jacques.furst/miniconda3/envs/RL/lib/python3.10/site-packages/torch/_inductor/output_code.py\", line 460, in __call__\n    return self.current_callable(inputs)\n  File \"/home/jacques.furst/miniconda3/envs/RL/lib/python3.10/site-packages/torch/_inductor/utils.py\", line 2404, in run\n    return model(new_inputs)\n  File \"/tmp/torchinductor_jacques.furst/we/cwe323lhnvrbiewv5s226esrfjevpk74mapz5wkqvyjb6sgwx4cw.py\", line 249, in call\n    buf11 = empty_strided_cuda((s0, s1, 1152), (1152*s1, 1152, 1), torch.float32)\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 576.00 MiB. GPU 0 has a total capacity of 44.40 GiB of which 114.31 MiB is free. Including non-PyTorch memory, this process has 44.28 GiB memory in use. Of the allocated memory 43.59 GiB is allocated by PyTorch, and 19.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# if not os.path.exists(FINAL_LORA_ADAPTERS):\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# train model\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# # store final model parameters\u001b[39;00m\n\u001b[1;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39msave_pretrained(FINAL_LORA_ADAPTERS)\n",
      "File \u001b[0;32m~/miniconda3/envs/RL/lib/python3.10/site-packages/transformers/trainer.py:2240\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2238\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2239\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2241\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2245\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/RL/lib/python3.10/site-packages/transformers/trainer.py:2555\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2548\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2549\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2550\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2551\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2552\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2553\u001b[0m )\n\u001b[1;32m   2554\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2555\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2557\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2558\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2559\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2560\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2561\u001b[0m ):\n\u001b[1;32m   2562\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2563\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/miniconda3/envs/RL/lib/python3.10/site-packages/transformers/trainer.py:3745\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3742\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3744\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3745\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3747\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3749\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3750\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3751\u001b[0m ):\n",
      "File \u001b[0;32m~/development/RAG/flintfiller-precondition-rl/rl_training_new/utils.py:268\u001b[0m, in \u001b[0;36mCustomRewardTrainer.compute_loss\u001b[0;34m(self, model, inputs, num_items_in_batch, return_outputs)\u001b[0m\n\u001b[1;32m    265\u001b[0m weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sample_weights(labels) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight_strategy \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m--> 268\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    269\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\u001b[38;5;241m.\u001b[39msqueeze()  \u001b[38;5;66;03m# Shape: (batch_size) --> logits are the predicted rewards in this case\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;66;03m# Custom loss calculation\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/RL/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/RL/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/RL/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:194\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule(\u001b[38;5;241m*\u001b[39minputs[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodule_kwargs[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    193\u001b[0m replicas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplicate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids[: \u001b[38;5;28mlen\u001b[39m(inputs)])\n\u001b[0;32m--> 194\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather(outputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/RL/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:213\u001b[0m, in \u001b[0;36mDataParallel.parallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mparallel_apply\u001b[39m(\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28mself\u001b[39m, replicas: Sequence[T], inputs: Sequence[Any], kwargs: Any\n\u001b[1;32m    212\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Any]:\n\u001b[0;32m--> 213\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/RL/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:127\u001b[0m, in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m    125\u001b[0m     output \u001b[38;5;241m=\u001b[39m results[i]\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ExceptionWrapper):\n\u001b[0;32m--> 127\u001b[0m         \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mappend(output)\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/miniconda3/envs/RL/lib/python3.10/site-packages/torch/_utils.py:750\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    746\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    747\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments or otherwise can't\u001b[39;00m\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;66;03m# be constructed, don't try to instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    749\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 750\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: Caught OutOfMemoryError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/home/jacques.furst/miniconda3/envs/RL/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 97, in _worker\n    output = module(*input, **kwargs)\n  File \"/home/jacques.furst/miniconda3/envs/RL/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/jacques.furst/miniconda3/envs/RL/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/jacques.furst/miniconda3/envs/RL/lib/python3.10/site-packages/peft/peft_model.py\", line 1559, in forward\n    return self.base_model(\n  File \"/home/jacques.furst/miniconda3/envs/RL/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/jacques.furst/miniconda3/envs/RL/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/jacques.furst/miniconda3/envs/RL/lib/python3.10/site-packages/peft/tuners/tuners_utils.py\", line 193, in forward\n    return self.model.forward(*args, **kwargs)\n  File \"/home/jacques.furst/miniconda3/envs/RL/lib/python3.10/site-packages/transformers/models/modernbert/modeling_modernbert.py\", line 1166, in forward\n    outputs = self.model(\n  File \"/home/jacques.furst/miniconda3/envs/RL/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/jacques.furst/miniconda3/envs/RL/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/jacques.furst/miniconda3/envs/RL/lib/python3.10/site-packages/transformers/models/modernbert/modeling_modernbert.py\", line 881, in forward\n    layer_outputs = encoder_layer(\n  File \"/home/jacques.furst/miniconda3/envs/RL/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/jacques.furst/miniconda3/envs/RL/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/jacques.furst/miniconda3/envs/RL/lib/python3.10/site-packages/transformers/models/modernbert/modeling_modernbert.py\", line 548, in forward\n    self.compiled_mlp(hidden_states)\n  File \"/home/jacques.furst/miniconda3/envs/RL/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 655, in _fn\n    return fn(*args, **kwargs)\n  File \"/home/jacques.furst/miniconda3/envs/RL/lib/python3.10/site-packages/transformers/models/modernbert/modeling_modernbert.py\", line 523, in compiled_mlp\n    @torch.compile(dynamic=True)\n  File \"/home/jacques.furst/miniconda3/envs/RL/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 838, in _fn\n    return fn(*args, **kwargs)\n  File \"/home/jacques.furst/miniconda3/envs/RL/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py\", line 1209, in forward\n    return compiled_fn(full_args)\n  File \"/home/jacques.furst/miniconda3/envs/RL/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py\", line 315, in runtime_wrapper\n    all_outs = call_func_at_runtime_with_args(\n  File \"/home/jacques.furst/miniconda3/envs/RL/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py\", line 126, in call_func_at_runtime_with_args\n    out = normalize_as_list(f(args))\n  File \"/home/jacques.furst/miniconda3/envs/RL/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py\", line 100, in g\n    return f(*args)\n  File \"/home/jacques.furst/miniconda3/envs/RL/lib/python3.10/site-packages/torch/autograd/function.py\", line 575, in apply\n    return super().apply(*args, **kwargs)  # type: ignore[misc]\n  File \"/home/jacques.furst/miniconda3/envs/RL/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py\", line 1937, in forward\n    fw_outs = call_func_at_runtime_with_args(\n  File \"/home/jacques.furst/miniconda3/envs/RL/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py\", line 126, in call_func_at_runtime_with_args\n    out = normalize_as_list(f(args))\n  File \"/home/jacques.furst/miniconda3/envs/RL/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py\", line 495, in wrapper\n    return compiled_fn(runtime_args)\n  File \"/home/jacques.furst/miniconda3/envs/RL/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py\", line 689, in inner_fn\n    outs = compiled_fn(args)\n  File \"/home/jacques.furst/miniconda3/envs/RL/lib/python3.10/site-packages/torch/_inductor/output_code.py\", line 460, in __call__\n    return self.current_callable(inputs)\n  File \"/home/jacques.furst/miniconda3/envs/RL/lib/python3.10/site-packages/torch/_inductor/utils.py\", line 2404, in run\n    return model(new_inputs)\n  File \"/tmp/torchinductor_jacques.furst/we/cwe323lhnvrbiewv5s226esrfjevpk74mapz5wkqvyjb6sgwx4cw.py\", line 249, in call\n    buf11 = empty_strided_cuda((s0, s1, 1152), (1152*s1, 1152, 1), torch.float32)\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 576.00 MiB. GPU 0 has a total capacity of 44.40 GiB of which 114.31 MiB is free. Including non-PyTorch memory, this process has 44.28 GiB memory in use. Of the allocated memory 43.59 GiB is allocated by PyTorch, and 19.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    }
   ],
   "source": [
    "# if not os.path.exists(FINAL_LORA_ADAPTERS):\n",
    "# train model\n",
    "trainer.train()\n",
    "# # store final model parameters\n",
    "model.save_pretrained(FINAL_LORA_ADAPTERS)\n",
    "\n",
    "# #TODO: not storing this properly I suppose, need to change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reload saved LoRA adapter for inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at nlpaueb/legal-bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "base_model_test = AutoModelForSequenceClassification.from_pretrained(MODEL, num_labels=1)\n",
    "new_model = PeftModel.from_pretrained(base_model_test, FINAL_LORA_ADAPTERS)\n",
    "# new_model = new_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer with new model\n",
    "trainer = CustomRewardTrainer(\n",
    "    model=new_model,\n",
    "    args=training_args,\n",
    "    train_dataset=datasets[0],\n",
    "    eval_dataset=datasets[1],\n",
    "    # compute_metrics=trainer.compute_metrics,  # Use the custom metrics function\n",
    "    processing_class=tokenizer,\n",
    "    loss_type=\"huber\",  # \"mse\" or \"huber\"\n",
    "    weight_strategy=\"linear\",  # \"linear\", \"inverse\", or None\n",
    "    # callbacks=[EarlyStoppingCallback(early_stopping_patience=50)] # use early stopping since we are sing high amount of epochs\n",
    "    # data_collator=RewardDataCollator()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results: {'eval_loss': 0.5358006358146667, 'eval_model_preparation_time': 0.008, 'eval_runtime': 0.2558, 'eval_samples_per_second': 547.295, 'eval_steps_per_second': 35.183}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_results = trainer.evaluate(eval_dataset=datasets[2])\n",
    "print(\"Test Results:\", test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1: Predicted Rating: 2.779296875, True Rating: 1\n",
      "Sample 2: Predicted Rating: -0.07684326171875, True Rating: 0\n",
      "Sample 3: Predicted Rating: 2.345703125, True Rating: 3\n",
      "Sample 4: Predicted Rating: -0.010162353515625, True Rating: 0\n",
      "Sample 5: Predicted Rating: 2.9140625, True Rating: 3\n",
      "Sample 6: Predicted Rating: 0.10626220703125, True Rating: 0\n",
      "Sample 7: Predicted Rating: 2.4453125, True Rating: 1\n",
      "Sample 8: Predicted Rating: 0.07196044921875, True Rating: 0\n",
      "Sample 9: Predicted Rating: 0.056854248046875, True Rating: 0\n",
      "Sample 10: Predicted Rating: -0.040679931640625, True Rating: 0\n",
      "Sample 11: Predicted Rating: 0.053863525390625, True Rating: 0\n",
      "Sample 12: Predicted Rating: 0.17333984375, True Rating: 0\n",
      "Sample 13: Predicted Rating: -0.03515625, True Rating: 0\n",
      "Sample 14: Predicted Rating: 1.3251953125, True Rating: 1\n",
      "Sample 15: Predicted Rating: 1.2685546875, True Rating: 0\n",
      "Sample 16: Predicted Rating: 0.18017578125, True Rating: 0\n",
      "Sample 17: Predicted Rating: 2.4453125, True Rating: 3\n",
      "Sample 18: Predicted Rating: 2.87109375, True Rating: 3\n",
      "Sample 19: Predicted Rating: 0.0848388671875, True Rating: 1\n",
      "Sample 20: Predicted Rating: 2.955078125, True Rating: 3\n"
     ]
    }
   ],
   "source": [
    "# evaluate model manually on some test cases\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "#TODO: change tokenization function here!\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(20):\n",
    "        sample = datasets[2][i]\n",
    "        inputs = tokenizer(sample['precondition_text'] + \" \" + sample['response_text'], return_tensors='pt', truncation=True, padding=\"max_length\").to(device)\n",
    "        outputs = model(**inputs)\n",
    "        prediction = outputs.logits.item()\n",
    "        print(f\"Sample {i+1}: Predicted Rating: {prediction}, True Rating: {sample['label']}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
