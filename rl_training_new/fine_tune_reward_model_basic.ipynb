{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tune reward model from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODOs:\n",
    "\n",
    "#TODO: double-check that labels are not somehow misaligned...\n",
    "\n",
    "#TODO: check if you need to plot \n",
    "\n",
    "1. LoRA learns the position of the low rank adaptation matrix that is needed to finetune a model of a much higher rank\n",
    "\n",
    "#TODO: double check model performance, generate output, maybe adjust training metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports, setup, and global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacques.furst/miniconda3/envs/RL/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 GPU(s) available.\n",
      "CUDA is available. Using GPU: NVIDIA L40S\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Add the parent directory to the Python path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname(os.getcwd()), '..')))\n",
    "\n",
    "from transformers import TrainingArguments, EarlyStoppingCallback\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from datasets import Dataset, DatasetDict, load_from_disk\n",
    "\n",
    "from peft import LoraConfig, get_peft_model, PeftModel, PeftConfig\n",
    "\n",
    "from utils import parse_ratings, tokenize_fn_with_best_window, tokenize_fn_basic_batched, CustomRewardTrainer, find_best_window, convert_label_to_int\n",
    "\n",
    "\n",
    "# from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# load the relevant devices available on the server\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = os.getenv(\"AVAILABLE_DEVICES\")\n",
    "\n",
    "# Enable expandable CUDA segments\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# load cuda\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print(\"CUDA is available. Using GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA is not available. Using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training variables\n",
    "FEEDBACK_TO_TRAIN_ON = os.getenv(\"FEEDBACK_TO_TRAIN_ON\")\n",
    "FEEDBACK_TO_REMOVE = os.getenv(\"FEEDBACK_TO_REMOVE\")\n",
    "MODEL = os.getenv(\"REWARD_MODEL\")\n",
    "DATASET = os.getenv(\"REWARD_DATASET\")\n",
    "TOKENIZE_FN = os.getenv(\"TOKENIZE_FN\")\n",
    "MAX_LENGTH = os.getenv(\"MAX_LENGTH\")\n",
    "STRIDE = os.getenv(\"STRIDE\")\n",
    "LORA_CHECKPOINTS_FOLDER = os.getenv(\"LORA_CHECKPOINTS_FOLDER\")\n",
    "\n",
    "#TODO: change this to not store model since contains /!!!\n",
    "FINAL_LORA_ADAPTERS = os.getenv(\"FINAL_LORA_ADAPTERS_FOLDER\") + f\"_{FEEDBACK_TO_TRAIN_ON}_{TOKENIZE_FN}_{DATASET}\"\n",
    "TOKENIZED_DATA_TRAIN = os.getenv(\"TOKENIZED_DATA\") + f\"_{FEEDBACK_TO_TRAIN_ON}_{TOKENIZE_FN}_{DATASET}_train\"\n",
    "TOKENIZED_DATA_EVAL = os.getenv(\"TOKENIZED_DATA\") + f\"_{FEEDBACK_TO_TRAIN_ON}_{TOKENIZE_FN}_{DATASET}_eval\"\n",
    "TOKENIZED_DATA_TEST = os.getenv(\"TOKENIZED_DATA\") + f\"_{FEEDBACK_TO_TRAIN_ON}_{TOKENIZE_FN}_{DATASET}_test\"\n",
    "DATASET_STRUCTURE = os.getenv(\"DATASET_STRUCTURE\")\n",
    "\n",
    "REWARD_DATA_PATH = os.getenv(\"REWARD_DATA_PATH\")\n",
    "\n",
    "if DATASET_STRUCTURE == \"determined\":\n",
    "    REWARD_MODEL_TRAIN_DATA_HUMAN = REWARD_DATA_PATH + \"/train_human_determined.csv\"\n",
    "    REWARD_MODEL_EVAL_DATA_HUMAN = REWARD_DATA_PATH + \"/validation_human_determined.csv\"\n",
    "    REWARD_MODEL_TEST_DATA_HUMAN = REWARD_DATA_PATH + \"/test_human_determined.csv\"\n",
    "\n",
    "    REWARD_MODEL_TRAIN_DATA_SYNTH = REWARD_DATA_PATH + \"/train_synth_determined.csv\"\n",
    "    REWARD_MODEL_EVAL_DATA_SYNTH = REWARD_DATA_PATH + \"/validation_synth_determined.csv\"\n",
    "    REWARD_MODEL_TEST_DATA_SYNTH = REWARD_DATA_PATH + \"/test_synth_determined.csv\"\n",
    "\n",
    "elif DATASET_STRUCTURE == \"random\":\n",
    "    REWARD_MODEL_TRAIN_DATA_HUMAN = REWARD_DATA_PATH + \"/train_human_random.csv\"\n",
    "    REWARD_MODEL_EVAL_DATA_HUMAN = REWARD_DATA_PATH + \"/validation_human_random.csv\"\n",
    "    REWARD_MODEL_TEST_DATA_HUMAN = REWARD_DATA_PATH + \"/test_human_random.csv\"\n",
    "\n",
    "    REWARD_MODEL_TRAIN_DATA_SYNTH = REWARD_DATA_PATH + \"/train_synth_random.csv\"\n",
    "    REWARD_MODEL_EVAL_DATA_SYNTH = REWARD_DATA_PATH + \"/validation_synth_random.csv\"\n",
    "    REWARD_MODEL_TEST_DATA_SYNTH = REWARD_DATA_PATH + \"/test_synth_random.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset loading and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Re-structure df synthetic to fit in training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['file', 'frame_ID', 'frame_type', 'frame_text', 'precondition_id',\n",
       "       'precondition_text', 'precondition_position', 'response_text',\n",
       "       'prompt_config_examples', 'prompt_config_chain_of_thought',\n",
       "       'feedback_extraction', 'feedback_detection', 'additional_feedback'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if DATASET == \"human\":\n",
    "    df_train = pd.read_csv(REWARD_MODEL_TRAIN_DATA_HUMAN, sep=\";\")\n",
    "    df_eval = pd.read_csv(REWARD_MODEL_EVAL_DATA_HUMAN, sep=\";\")\n",
    "    df_test = pd.read_csv(REWARD_MODEL_TEST_DATA_HUMAN, sep=\";\")\n",
    "elif DATASET == \"synthetic\":\n",
    "    df_train = pd.read_csv(REWARD_MODEL_TRAIN_DATA_SYNTH, sep=\";\")\n",
    "    df_eval = pd.read_csv(REWARD_MODEL_EVAL_DATA_SYNTH, sep=\";\")\n",
    "    df_test = pd.read_csv(REWARD_MODEL_TEST_DATA_SYNTH, sep=\";\")\n",
    "    \n",
    "    \n",
    "df_train.shape\n",
    "df_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. a) Parse ratings to numeric values for MSE Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed feedback for extraction: 0    3\n",
      "1    0\n",
      "2    2\n",
      "3    0\n",
      "4    3\n",
      "Name: feedback_extraction, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df_train[FEEDBACK_TO_TRAIN_ON] = [parse_ratings(feedback) for feedback in df_train[FEEDBACK_TO_TRAIN_ON]]\n",
    "df_eval[FEEDBACK_TO_TRAIN_ON] = [parse_ratings(feedback) for feedback in df_eval[FEEDBACK_TO_TRAIN_ON]]\n",
    "df_test[FEEDBACK_TO_TRAIN_ON] = [parse_ratings(feedback) for feedback in df_test[FEEDBACK_TO_TRAIN_ON]]\n",
    "print(\"Parsed feedback for extraction:\", df_train[FEEDBACK_TO_TRAIN_ON][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. b) look at biases in feedback to train on for weights in RL loop --> feedback_detection is very biased through way it was collected, so gets less weight overall..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feedback_extraction\n",
       "0    347\n",
       "3    168\n",
       "2     72\n",
       "1     63\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[FEEDBACK_TO_TRAIN_ON].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. c) keep only relevant feedback column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['file', 'frame_ID', 'frame_type', 'frame_text', 'precondition_id', 'precondition_text', 'precondition_position', 'response_text', 'prompt_config_examples', 'prompt_config_chain_of_thought', 'feedback_extraction', 'feedback_detection', 'additional_feedback'],\n",
      "    num_rows: 650\n",
      "})\n",
      "feedback_extraction\n"
     ]
    }
   ],
   "source": [
    "dataset_train = Dataset.from_pandas(df_train)\n",
    "dataset_eval = Dataset.from_pandas(df_eval)\n",
    "dataset_test = Dataset.from_pandas(df_test)\n",
    "\n",
    "print(dataset_train)\n",
    "print(FEEDBACK_TO_TRAIN_ON) \n",
    "\n",
    "datasets = [dataset_train, dataset_eval, dataset_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3', '0', '2', '0', '3', '0', '0', '2', '2', '0', '0', '0', '0', '0', '3', '0', '0', '2', '3', '3', '3', '0', '3', '0', '0', '1', '0', '0', '0', '1', '0', '0', '0', '1', '0', '0', '0', '2', '0', '0', '3', '3', '3', '0', '3', '0', '0', '0', '0', '3', '0', '0', '0', '3', '2', '0', '3', '0', '1', '3', '3', '0', '2', '0', '3', '0', '0', '1', '3', '2', '0', '3', '0', '0', '0', '0', '3', '0', '0', '0', '3', '1', '0', '0', '0', '3', '0', '2', '3', '0', '0', '0', '2', '0', '1', '0', '0', '3', '3', '0', '0', '2', '3', '0', '1', '2', '0', '3', '3', '0', '0', '1', '0', '3', '3', '0', '0', '3', '0', '2', '2', '3', '0', '0', '3', '0', '0', '0', '2', '0', '3', '3', '0', '2', '2', '0', '0', '3', '0', '0', '1', '0', '3', '0', '1', '3', '0', '0', '0', '0', '3', '2', '0', '0', '2', '1', '0', '3', '3', '0', '3', '2', '0', '0', '0', '0', '3', '3', '1', '0', '0', '3', '0', '0', '3', '2', '0', '0', '0', '0', '2', '3', '2', '0', '0', '0', '3', '3', '0', '0', '1', '3', '2', '3', '2', '3', '0', '0', '0', '1', '3', '3', '2', '0', '1', '0', '0', '0', '0', '0', '0', '3', '1', '1', '0', '2', '3', '0', '1', '3', '3', '3', '1', '3', '0', '3', '0', '2', '0', '0', '0', '1', '3', '0', '3', '1', '2', '0', '2', '0', '0', '3', '0', '1', '3', '3', '0', '0', '3', '1', '3', '3', '0', '0', '0', '3', '0', '0', '2', '0', '0', '3', '0', '3', '0', '1', '2', '0', '1', '1', '3', '0', '0', '2', '1', '1', '3', '0', '3', '0', '0', '3', '0', '0', '3', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '3', '0', '3', '0', '0', '0', '0', '0', '0', '0', '0', '3', '0', '3', '0', '3', '3', '0', '0', '0', '0', '0', '3', '2', '3', '0', '0', '3', '0', '0', '0', '0', '3', '0', '3', '0', '0', '2', '0', '0', '3', '3', '0', '0', '0', '3', '0', '0', '2', '0', '0', '0', '3', '0', '0', '3', '0', '2', '3', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '3', '0', '1', '3', '0', '1', '3', '2', '0', '0', '0', '0', '2', '0', '0', '0', '0', '3', '0', '1', '1', '0', '0', '1', '0', '0', '2', '3', '1', '2', '0', '1', '0', '3', '3', '3', '1', '1', '0', '3', '3', '1', '1', '3', '0', '3', '0', '1', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '3', '0', '0', '0', '2', '0', '2', '0', '2', '3', '2', '0', '3', '0', '1', '3', '1', '0', '2', '0', '0', '3', '3', '0', '0', '2', '0', '3', '1', '0', '0', '3', '3', '2', '3', '0', '2', '0', '0', '0', '0', '0', '0', '3', '0', '3', '0', '2', '0', '0', '0', '0', '0', '0', '3', '2', '2', '0', '3', '1', '3', '3', '2', '0', '0', '2', '3', '0', '0', '3', '3', '0', '3', '3', '0', '0', '3', '2', '0', '0', '0', '0', '3', '0', '0', '3', '1', '0', '0', '2', '3', '1', '0', '3', '3', '3', '1', '0', '3', '3', '2', '3', '0', '3', '3', '0', '0', '3', '2', '0', '0', '0', '3', '0', '0', '0', '3', '3', '3', '0', '0', '0', '2', '3', '0', '3', '0', '2', '2', '0', '3', '3', '2', '0', '3', '3', '2', '3', '0', '0', '2', '0', '3', '3', '1', '0', '3', '1', '0', '3', '0', '0', '0', '0', '0', '1', '0', '2', '0', '1', '0', '3', '0', '0', '3', '3', '2', '2', '0', '1', '3', '0', '0', '0', '0', '0', '0', '2', '0', '3', '0', '0', '0', '1', '3', '0', '3', '0', '0', '3', '0', '0', '1', '0', '1', '1', '0', '0', '0', '2', '1', '0', '3', '0', '0', '0', '3', '0', '0', '0', '2', '2', '3', '3', '0', '0', '0', '2', '1', '3', '0', '0']\n"
     ]
    }
   ],
   "source": [
    "datasets= [dataset.remove_columns([FEEDBACK_TO_REMOVE]) for dataset in datasets]\n",
    "datasets = [dataset.rename_column(FEEDBACK_TO_TRAIN_ON, \"label\") for dataset in datasets]\n",
    "\n",
    "print(datasets[0][\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load model with LoRA layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at nlpaueb/legal-bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSdpaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Load the model and the tokenizer\n",
    "model_id = MODEL \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_id, num_labels=1) # num_labels = 1 since we want to prodict a single scalar (the rating)\n",
    "\n",
    "# Comment: Automodel for sequence classification with num_labels=1 already has a regression head\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n",
      "512\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.model_max_length)\n",
    "print(model.config.max_position_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 443,137 || all params: 109,926,146 || trainable%: 0.4031\n"
     ]
    }
   ],
   "source": [
    "# Define LoRA config\n",
    "\n",
    "\n",
    "if MODEL == \"answerdotai/ModernBERT-base\":\n",
    "\n",
    "    lora_config = LoraConfig(\n",
    "    r=8,           # Rank of the LoRA matrices (smaller = less memory)\n",
    "    lora_alpha=16, # Scaling factor (higher = stronger adaptation)\n",
    "    target_modules=[\"Wqkv\", \"Wo\"], # Apply LoRA to attention layers\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_CLS\"  # classify each answer \n",
    "    )\n",
    "else:\n",
    "    lora_config = LoraConfig(\n",
    "    r=8,           # Rank of the LoRA matrices (smaller = less memory)\n",
    "    lora_alpha=16, # Scaling factor (higher = stronger adaptation)\n",
    "    target_modules=[\"query\", \"key\", \"value\"], # Apply LoRA to attention layers\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_CLS\"  # classify each answer \n",
    "    )\n",
    "    \n",
    "\n",
    "# Freeze base model\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "\n",
    "# Convert the model to a PEFT (LoRA) model\n",
    "peft_model = get_peft_model(model, lora_config)\n",
    "# model.gradient_checkpointing_enable()\n",
    "peft_model.print_trainable_parameters()  # Check trainable params (~0.1% of full model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 1067, 223, 207, 580, 210, 1335, 124, 102, 0, 0, 0], [101, 1067, 223, 207, 5601, 190, 580, 213, 207, 1727, 124, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test tokenizer\n",
    "sample_data = [\"What is the capital of France?\", \"What is the largest capital in the world?\"]\n",
    "tokenizer(sample_data, padding=True, truncation=True, max_length=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Encode dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['file', 'frame_ID', 'frame_type', 'frame_text', 'precondition_id', 'precondition_text', 'precondition_position', 'response_text', 'prompt_config_examples', 'prompt_config_chain_of_thought', 'label', 'additional_feedback']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 650/650 [00:00<00:00, 9045.15 examples/s]\n",
      "Map: 100%|██████████| 139/139 [00:00<00:00, 11114.87 examples/s]\n",
      "Map: 100%|██████████| 140/140 [00:00<00:00, 11245.43 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 0, 2, 0, 3]\n",
      "['Inhoud: <inhoud van subfact>\\n\\n\\n                 1. Subfact: Budgettaire totaalbeeld\\n                  Positie: Artikel 4.2.4.a IN Rijksbegrotingswet 2016\\n                  Inhoud: het budgettaire totaalbeeld voor het betrokken begrotingsjaar en de vier daaropvolgende jaren van de rijksbegroting en de niet tot de rijksbegroting behorende budgetdisciplinesectoren\\n\\n                 2. Subfact: Budgettaire beschouwingen\\n                  Positie: Artikel 4.2.4.b IN Rijksbegrotingswet 2016\\n                  Inhoud: de budgettaire beschouwingen over het voorgenomen beleid voor de collectieve sector\\n\\n                 3. Subfact: Overzicht van de uitgaven en ontvangsten\\n                  Positie: Artikel 4.2.4.c IN Rijksbegrotingswet 2016\\n                  Inhoud: een overzicht van de uitgaven en de ontvangsten in de begrotingen voor het begrotingsjaar en de vier daarop aansluitende jaren.', '1. Subfact: Begroting bevat begrotingsstaat\\n\\n                Positie: Artikel 2.1, eerste lid, IN Comptabiliteitswet 2016\\n\\n            2. Subfact: De departementale begroting bevat begrotingsstaat\\n\\n               Positie: Artikel 2.1, eerste lid, IN Comptabiliteitswet 2016\\n\\n            3. Subfact: De niet-departementale begroting bevat begrotingsstaat\\n\\n              Positie: Artikel 2.5, IN Comptabiliteitswet 2016\\n\\n            4. Subfact: De begroting van een begrotingsfonds bevat begrotingsstaat\\n\\n              Positie: Artikel 2.11, eerste lid, IN Comptabiliteitswet 2016', '1. Vreemdeling: ieder die de Nederlandse nationaliteit niet bezit en niet op grond van een wettelijke bepaling als Nederlander moet worden behandeld \\n \\n                Positie: Artikel 1, Algemene wet bestuursrecht\\n \\n                2. Vreemdeling: ieder die de Nederlandse nationaliteit niet bezit en niet op grond van een wettelijke bepaling als Nederlander moet worden behandeld; ieder die rechtmatig verblijf heeft op grond van artikel 8 \\n \\n                Positie: Artikel 8, Vreemdelingenwet\\n \\n                3. Vreemdeling: ieder die de Nederlandse nationaliteit niet bezit en niet op grond van een wettelijke bepaling als Nederlander moet worden behandeld; ieder die rechtmatig verblijf heeft op grond van artikel 8, onder a tot en met d, f tot en met h en j tot en met m, en aan de vreemdeling die rechtmatig verblijf heeft op grond van artikel 8, onder e, en gemeenschapsonderdaan is als bedoeld in artikel 1, sub 2°, 4° en 6° \\n \\n                Positie: Artikel 9, Vreemdelingenwet', '1. De aanvraag is ontvangen \\n                Positie: Artikel 27, 1, 1 IN Vreemdelingenwet geldig vanaf 2024\\n \\n                2. De vreemdeling aangetoond dat hij aan alle voorwaarden voldoet \\n                Positie: Niet in de Vreemdelingenwet geldig vanaf 2024, maar wel in het algemene wet bestuursrecht\\n                Artikel 1:3, 3 IN Algemene wet bestuursrecht\\n \\n                3. De verblijfsvergunning wordt verleend met ingang van de dag waarop de aanvraag is ontvangen \\n                Positie: Artikel 27, 1, 1 IN Vreemdelingenwet geldig vanaf 2024\\n \\n                4. De verblijfsvergunning wordt verleend niet eerder dan met ingang van de dag waarop de aanvraag is ontvangen \\n                Positie: Artikel 27, 1, 1 IN Vreemdelingenwet geldig vanaf 2024', ' Inhoud: <inhoud>\\n\\n\\n\\n                Subfact: De minister van Binnenlandse Zaken en Koninkrijksrelaties is verantwoordelijk voor het beheer van de begrotingen van de Staten-Generaal, de Raad van State, de Algemene Rekenkamer, de Nationale ombudsman, de Kanselarij der Nederlandse Orden, het Kabinet van de Gouverneur van Aruba, het Kabinet van de Gouverneur van Curaçao, het Kabinet van de Gouverneur van Sint Maarten en de Kiesraad.\\n\\n                Positie: Artikel 4.4, sectie 2 IN Rijksbegrotingswet 2016\\n\\n                Inhoud: Onze Minister van Binnenlandse Zaken en Koninkrijksrelaties is verantwoordelijk voor het beheer van de begrotingen van de Staten-Generaal, de Raad van State, de Algemene Rekenkamer, de Nationale ombudsman, de Kanselarij der Nederlandse Orden, het Kabinet van de Gouverneur van Aruba, het Kabinet van de Gouverneur van Curaçao, het Kabinet van de Gouverneur van Sint Maarten en de Kiesraad.\\n\\n                Subfact: De minister van Binnenlandse Zaken en Koninkrijksrelaties is verantwoordelijk voor het beheer van de begroting van koninkrijksrelaties.\\n\\n                Positie: Artikel 4.3, sectie 2 IN Rijksbegrotingswet 2016\\n\\n                Inhoud: Onze Minister van Binnenlandse Zaken en Koninkrijksrelaties is verantwoordelijk voor het beheer van de begroting van koninkrijksrelaties.\\n\\n                Subfact: De minister van Binnenlandse Zaken en Koninkrijksrelaties is verantwoordelijk voor het beheer van de begrotingen van de Staten-Generaal.\\n\\n                Positie: Artikel 4.4, sectie 2 IN Rijksbegrotingswet 2016\\n\\n                Inhoud: De minister van Binnenlandse Zaken en Koninkrijksrelaties is verantwoordelijk voor het beheer van de begrotingen van de Staten-Generaal.\\n\\n                Subfact: De minister van Binnenlandse Zaken en Koninkrijksrelaties is verantwoordelijk voor het beheer van de begroting van de Raad van State.\\n\\n                Positie: Artikel 4.4, sectie 2 IN Rijksbegrotingswet 2016\\n\\n                Inhoud: De minister van Binnenlandse Zaken en Koninkrijksrelaties is verantwoordelijk voor het beheer van de begroting van de Raad van State.\\n\\n                Subfact: De minister van Binnenlandse Zaken en Koninkrijksrelaties is verantwoordelijk voor het beheer van de begroting van de Algemene Rekenkamer.\\n\\n                Positie: Artikel 4.4, sectie 2 IN Rijksbegrotingswet 2016\\n\\n                Inhoud: De minister van Binnenlandse Zaken en Koninkrijksrelaties is verantwoordelijk voor het beheer van de begroting van de Algemene Rekenkamer.\\n\\n                Subfact: De minister van Binnenlandse Zaken en Koninkrijksrelaties is verantwoordelijk voor het beheer van de begroting van de Nationale ombudsman.\\n\\n                Positie: Artikel 4.4, sectie 2 IN Rijksbegrotingswet 2016\\n\\n                Inhoud: De minister van Binnenlandse Zaken en Koninkrijksrelaties is verantwoordelijk voor het beheer van de begroting van de Nationale ombudsman.\\n\\n                Subfact: De minister van Binnenlandse Zaken en Koninkrijksrelaties is verantwoordelijk voor het beheer van de Kanselarij der Nederlandse Orden.\\n\\n                Positie: Artikel 4.4, sectie 2 IN Rijksbegrotingswet 2016\\n\\n                Inhoud: De minister van Binnenlandse Zaken en Koninkrijksrelaties is verantwoordelijk voor het beheer van de Kanselarij der Nederlandse Orden.\\n\\n                Subfact: De minister van Binnenlandse Zaken en Koninkrijksrelaties is verantwoordelijk voor het beheer van het Kabinet van de Gouverneur van Aruba.\\n\\n                Positie: Artikel 4.4, sectie 2 IN Rijksbegrotingswet 2016\\n\\n                Inhoud: De minister van Binnenlandse Zaken en Koninkrijksrelaties is verantwoordelijk voor het beheer van het Kabinet van de Gouverneur van Aruba.\\n\\n                Subfact: De minister van Binnenlandse Zaken en Koninkrijksrelaties is verantwoordelijk voor het beheer van het Kabinet van de Gouverneur van Curaçao.\\n\\n                Positie: Artikel 4.4, sectie 2 IN Rijksbegrotingswet 2016\\n\\n                Inhoud: De minister van Binnenlandse Zaken en Koninkrijksrelaties is verant']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(datasets[0].column_names)\n",
    "# mao string labels to integers\n",
    "datasets = [dataset.map(convert_label_to_int) for dataset in datasets]\n",
    "\n",
    "print(datasets[0][\"label\"][:5])  # Check labels\n",
    "print(datasets[0][\"response_text\"][:5])  # Check labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comment\n",
    "\n",
    "1. Needed for feedback extraction: precondition_text, response_text, label(rating feedback extraction)\n",
    "2. Needed for feedback detection: precondition_text, precondition_position, response_text, label (rating feedback detection)\n",
    "3. For the precondition position to be found well, it is a crucial for the model to find the precondition text (at least to a recognizable degree) as well, otherwise the precondition is not found at all..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1127 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "de juiste keuzes hebben gemaakt, of ze trouw zijn gebleven aan zichzelf. anderen proberen simpelweg de dag door te komen, met hoop op iets beters. in die momenten van stilte komt vaak het besef dat, hoewel we allemaal verschillende paden bewandelen, we een waarheid delen : dat het leven, ondanks al onze inspanningen en verlangens, nooit gemakkelijk is. of, zoals mijn grootmoeder het ooit zei terwijl ze haar handen vouwde na een lange dag werken op het land : [UNK] je moet weten, kind, het leven is nooit gemakkelijk, maar het is wel de moeite waard. [UNK] we worden gevormd door onze ervaringen, door de mensen die we ontmoeten en de obstakels die we overwinnen. elke fout, elk succes, elke traan en elke glimlach draagt bij aan wie we zijn. en toch, ondanks al die ervaringen, blijven we zoeken. naar betekenis. naar verbinding. naar rust. soms lijkt het alsof de wereld te snel draait. technologie verandert ons leven in een razend tempo, verwachtingen worden hoger, en de druk om te presteren neemt toe. in die chaos vergeten we soms stil te staan. te ademen. te voelen. maar juist in die momenten van rust vinden we vaak de antwoorden die we zo hard nodig hebben. de liefde, bijvoorbeeld, is een van de krachtigste krachten die ons voortdrijft. liefde voor een partner, een kind, een vriend, of zelfs voor een passie. het is die liefde die ons helpt vol te houden wanneer alles tegen\n"
     ]
    }
   ],
   "source": [
    "# Code to test bestw indow function\n",
    "\n",
    "test_text = \"\"\"\n",
    "        Titel: De Weg Door Het Leven\n",
    "\n",
    "Het leven is een reis vol onverwachte wendingen, een pad dat zich zelden rechtlijnig ontvouwt. Vanaf het moment dat we onze eerste ademhaling nemen, worden we ondergedompeld in een wereld die we nog moeten leren begrijpen. Als kind lijkt alles eenvoudig: lachen, spelen, ontdekken. Maar naarmate we ouder worden, beginnen de lagen van complexiteit zich op te stapelen. We leren dat mensen niet altijd zeggen wat ze bedoelen, dat keuzes consequenties hebben, en dat geluk soms vluchtiger is dan we zouden willen.\n",
    "\n",
    "In de vroege ochtenden, wanneer de zon net boven de horizon verschijnt en de wereld nog stil is, denken velen na over hun plaats in het grotere geheel. Sommigen vragen zich af of ze de juiste keuzes hebben gemaakt, of ze trouw zijn gebleven aan zichzelf. Anderen proberen simpelweg de dag door te komen, met hoop op iets beters. In die momenten van stilte komt vaak het besef dat, hoewel we allemaal verschillende paden bewandelen, we één waarheid delen: dat het leven, ondanks al onze inspanningen en verlangens, nooit gemakkelijk is. Of, zoals mijn grootmoeder het ooit zei terwijl ze haar handen vouwde na een lange dag werken op het land: “Je moet weten, kind, het leven is nooit gemakkelijk, maar het is wel de moeite waard.”\n",
    "\n",
    "We worden gevormd door onze ervaringen, door de mensen die we ontmoeten en de obstakels die we overwinnen. Elke fout, elk succes, elke traan en elke glimlach draagt bij aan wie we zijn. En toch, ondanks al die ervaringen, blijven we zoeken. Naar betekenis. Naar verbinding. Naar rust.\n",
    "\n",
    "Soms lijkt het alsof de wereld te snel draait. Technologie verandert ons leven in een razend tempo, verwachtingen worden hoger, en de druk om te presteren neemt toe. In die chaos vergeten we soms stil te staan. Te ademen. Te voelen. Maar juist in die momenten van rust vinden we vaak de antwoorden die we zo hard nodig hebben.\n",
    "\n",
    "De liefde, bijvoorbeeld, is een van de krachtigste krachten die ons voortdrijft. Liefde voor een partner, een kind, een vriend, of zelfs voor een passie. Het is die liefde die ons helpt vol te houden wanneer alles tegenzit. Die ons eraan herinnert waarom we begonnen zijn, waarom we blijven proberen.\n",
    "\n",
    "En dan is er verlies. Een onvermijdelijk onderdeel van het leven. We verliezen mensen, kansen, dromen. Maar in dat verlies schuilt ook groei. We leren loslaten, opnieuw beginnen, sterker worden. Het is pijnlijk, ja, maar ook noodzakelijk.\n",
    "\n",
    "Wanneer we terugkijken op ons leven, zijn het zelden de materiële zaken die we herinneren. Het zijn de momenten. De gesprekken bij kaarslicht. De wandelingen in de regen. De onverwachte lachbuien. De stilte van een gedeeld verdriet. Die momenten vormen de essentie van ons bestaan.\n",
    "\n",
    "Dus ja, het leven is vol uitdagingen. Het is rommelig, verwarrend, soms oneerlijk. Maar het is ook prachtig, rijk aan betekenis, en gevuld met kansen om te groeien, te leren en lief te hebben. En misschien is dat wel de grootste les van allemaal: dat we, ondanks alles, blijven kiezen voor hoop. Voor verbinding. Voor het leven zelf.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "test_ground_truth = \"Het leven is nooit gemakkelijk.\"\n",
    "\n",
    "print(find_best_window(test_text, test_ground_truth, device, tokenizer))\n",
    "\n",
    "# Works as expectd, I am impressed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(TOKENIZED_DATA_TRAIN):\n",
    "    if TOKENIZE_FN == \"best_window\":\n",
    "        datasets = [dataset.map(tokenize_fn_with_best_window, \n",
    "                                fn_kwargs={\"feedback_train\": FEEDBACK_TO_TRAIN_ON, \n",
    "                                            \"tokenizer\": tokenizer, \n",
    "                                            \"max_length\": int(MAX_LENGTH), \n",
    "                                            \"stride\": int(STRIDE),\n",
    "                                            \"device\": device\n",
    "                                            },\n",
    "                                batched=False) for dataset in datasets]\n",
    "    else:\n",
    "        datasets = [dataset.map(tokenize_fn_basic_batched, \n",
    "                                fn_kwargs={\"feedback_train\": FEEDBACK_TO_TRAIN_ON, \n",
    "                                            \"tokenizer\": tokenizer \n",
    "                                            },\n",
    "                                batched=True) for dataset in datasets]\n",
    "    \n",
    "    \n",
    "    datasets[0].save_to_disk(TOKENIZED_DATA_TRAIN)\n",
    "    datasets[1].save_to_disk(TOKENIZED_DATA_EVAL)\n",
    "    datasets[2].save_to_disk(TOKENIZED_DATA_TEST)\n",
    "else:\n",
    "    datasets[0] = load_from_disk(TOKENIZED_DATA_TRAIN)\n",
    "    datasets[1] = load_from_disk(TOKENIZED_DATA_TEST)\n",
    "    datasets[2] = load_from_disk(TOKENIZED_DATA_EVAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'Interpretatie_Vw_over_besluiten_op_aanvragen_voor_een_verblijfsvergunning_regulier_bepaalde_tijd.json': 298, 'Participatiewet_most_recent_public.json': 181, 'rijksbegrotingscyclus.json': 171})\n"
     ]
    }
   ],
   "source": [
    "print(Counter(datasets[0]['file']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train reward model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-19 16:29:16,380] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacques.furst/miniconda3/envs/RL/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/home/jacques.furst/miniconda3/envs/RL/compiler_compat/ld: cannot find -lcufile: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-19 16:29:17,666] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Training arguments\n",
    "#TODO: switch to cross entropy loss...\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=LORA_CHECKPOINTS_FOLDER,\n",
    "    eval_strategy='steps',\n",
    "    save_strategy='steps',\n",
    "    save_steps=10,\n",
    "    eval_steps=10,\n",
    "    save_total_limit=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    learning_rate=3e-4,\n",
    "    num_train_epochs=5,\n",
    "    logging_steps=10,\n",
    "    label_names=[\"labels\"],\n",
    "    # report_to=\"none\",\n",
    "    logging_dir=\"./logs\",\n",
    "    # fp16=True,  # Use mixed precision training\n",
    "    metric_for_best_model=\"eval_loss\", # or \"eval_loss\"\n",
    "    greater_is_better=False, # False if using loss\n",
    "    # gradient_accumulation_steps=4, # \n",
    "    # torch_compile=False\n",
    "    # weight_decay=0.01\n",
    "    warmup_steps=82, \n",
    ")\n",
    "\n",
    "# Initialize custom trainer\n",
    "trainer = CustomRewardTrainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=datasets[0],\n",
    "    eval_dataset=datasets[1],\n",
    "    # compute_metrics=trainer.compute_metrics,  # Use the custom metrics function\n",
    "    processing_class=tokenizer,\n",
    "    loss_type=\"huber\",  # \"mse\" or \"huber\"\n",
    "    weight_strategy=\"linear\",  # \"linear\", \"inverse\", or None\n",
    ")\n",
    "\n",
    "print(trainer.args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjacques-furst123\u001b[0m (\u001b[33mjacques-furst123-none\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jacques.furst/development/RAG/flintfiller-precondition-rl/rl_training_new/wandb/run-20250619_141723-7s7uiyt7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jacques-furst123-none/huggingface/runs/7s7uiyt7' target=\"_blank\">/home/jacques.furst/development/RAG/flintfiller-precondition-rl/reward_training_files/lora_checkpoints</a></strong> to <a href='https://wandb.ai/jacques-furst123-none/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jacques-furst123-none/huggingface' target=\"_blank\">https://wandb.ai/jacques-furst123-none/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jacques-furst123-none/huggingface/runs/7s7uiyt7' target=\"_blank\">https://wandb.ai/jacques-furst123-none/huggingface/runs/7s7uiyt7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='815' max='815' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [815/815 02:05, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>7.420500</td>\n",
       "      <td>7.271349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>7.458600</td>\n",
       "      <td>6.872603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>7.135300</td>\n",
       "      <td>6.236314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>6.417000</td>\n",
       "      <td>5.264727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>4.889100</td>\n",
       "      <td>3.455577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.521200</td>\n",
       "      <td>1.363854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.064100</td>\n",
       "      <td>0.725643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.826100</td>\n",
       "      <td>0.659922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.825400</td>\n",
       "      <td>0.667690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.548600</td>\n",
       "      <td>0.728845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.696400</td>\n",
       "      <td>0.583818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.535800</td>\n",
       "      <td>0.582741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.798800</td>\n",
       "      <td>0.584526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.794600</td>\n",
       "      <td>0.676706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.745300</td>\n",
       "      <td>0.632412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.648900</td>\n",
       "      <td>0.567844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.551200</td>\n",
       "      <td>0.538916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.679900</td>\n",
       "      <td>0.537397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.578400</td>\n",
       "      <td>0.568525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.742200</td>\n",
       "      <td>0.495203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.623100</td>\n",
       "      <td>0.581421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.608800</td>\n",
       "      <td>0.533139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.633300</td>\n",
       "      <td>0.555237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.638500</td>\n",
       "      <td>0.623641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.468700</td>\n",
       "      <td>0.434943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.621200</td>\n",
       "      <td>0.475234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.610500</td>\n",
       "      <td>0.403907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.584300</td>\n",
       "      <td>0.486024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.710500</td>\n",
       "      <td>0.473218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.580600</td>\n",
       "      <td>0.638788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.517500</td>\n",
       "      <td>0.438711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.659400</td>\n",
       "      <td>0.626662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.597800</td>\n",
       "      <td>0.444126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.593978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.461700</td>\n",
       "      <td>0.607476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.745100</td>\n",
       "      <td>0.576243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.615200</td>\n",
       "      <td>0.529457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.349300</td>\n",
       "      <td>0.467786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.510300</td>\n",
       "      <td>0.440764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.693800</td>\n",
       "      <td>0.378469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.407500</td>\n",
       "      <td>0.494620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.386300</td>\n",
       "      <td>0.384126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.441100</td>\n",
       "      <td>0.361592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.335700</td>\n",
       "      <td>0.301383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.491700</td>\n",
       "      <td>0.347048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.459100</td>\n",
       "      <td>0.281431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.449100</td>\n",
       "      <td>0.277672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.479200</td>\n",
       "      <td>0.366384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.413500</td>\n",
       "      <td>0.353475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.326600</td>\n",
       "      <td>0.313787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.496200</td>\n",
       "      <td>0.317907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.417700</td>\n",
       "      <td>0.581288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>0.613600</td>\n",
       "      <td>0.345794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.270300</td>\n",
       "      <td>0.307025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.345700</td>\n",
       "      <td>0.289412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.350300</td>\n",
       "      <td>0.354420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>0.507300</td>\n",
       "      <td>0.302015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.361700</td>\n",
       "      <td>0.311665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>0.403700</td>\n",
       "      <td>0.281238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.345400</td>\n",
       "      <td>0.434638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>0.322700</td>\n",
       "      <td>0.494488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.424300</td>\n",
       "      <td>0.292000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>0.278900</td>\n",
       "      <td>0.319386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.554600</td>\n",
       "      <td>0.281142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.392000</td>\n",
       "      <td>0.300886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.462300</td>\n",
       "      <td>0.292203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>0.367200</td>\n",
       "      <td>0.299473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.291500</td>\n",
       "      <td>0.290280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>0.215700</td>\n",
       "      <td>0.295830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.318500</td>\n",
       "      <td>0.291647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>0.524400</td>\n",
       "      <td>0.292296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.163600</td>\n",
       "      <td>0.301846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>0.296800</td>\n",
       "      <td>0.308884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>0.270600</td>\n",
       "      <td>0.306169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.393600</td>\n",
       "      <td>0.303525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>0.249600</td>\n",
       "      <td>0.300816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>0.453500</td>\n",
       "      <td>0.307583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>0.603600</td>\n",
       "      <td>0.323048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>0.258400</td>\n",
       "      <td>0.313894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.581000</td>\n",
       "      <td>0.307607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>0.262100</td>\n",
       "      <td>0.300972</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# if not os.path.exists(FINAL_LORA_ADAPTERS):\n",
    "# train model\n",
    "trainer.train()\n",
    "# # store final model parameters\n",
    "peft_model.save_pretrained(FINAL_LORA_ADAPTERS)\n",
    "\n",
    "# #TODO: not storing this properly I suppose, need to change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reload saved LoRA adapter for inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at nlpaueb/legal-bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "base_model_test = AutoModelForSequenceClassification.from_pretrained(MODEL, num_labels=1)\n",
    "\n",
    "\n",
    "# config = PeftConfig.from_pretrained(FINAL_LORA_ADAPTERS)\n",
    "# base_model_test = AutoModelForSequenceClassification.from_pretrained(config.base_model_name_or_path, num_labels=1)\n",
    "\n",
    "new_model = PeftModel.from_pretrained(base_model_test, FINAL_LORA_ADAPTERS)\n",
    "# new_model = new_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jacques.furst/development/RAG/flintfiller-precondition-rl/reward_training_files/final_lora_adapters_feedback_extraction_basic_human\n"
     ]
    }
   ],
   "source": [
    "print(FINAL_LORA_ADAPTERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer with new model\n",
    "trainer = CustomRewardTrainer(\n",
    "    model=new_model,\n",
    "    args=training_args,\n",
    "    train_dataset=datasets[0],\n",
    "    eval_dataset=datasets[1],\n",
    "    # compute_metrics=trainer.compute_metrics,  # Use the custom metrics function\n",
    "    processing_class=tokenizer,\n",
    "    loss_type=\"huber\",  # \"mse\" or \"huber\"\n",
    "    weight_strategy=\"linear\",  # \"linear\", \"inverse\", or None\n",
    "    # callbacks=[EarlyStoppingCallback(early_stopping_patience=50)] # use early stopping since we are sing high amount of epochs\n",
    "    # data_collator=RewardDataCollator()\n",
    "    # torch_compile=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjacques-furst123\u001b[0m (\u001b[33mjacques-furst123-none\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jacques.furst/development/RAG/flintfiller-precondition-rl/rl_training_new/wandb/run-20250619_162931-3j1gsdbi</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jacques-furst123-none/huggingface/runs/3j1gsdbi' target=\"_blank\">/home/jacques.furst/development/RAG/flintfiller-precondition-rl/reward_training_files/lora_checkpoints</a></strong> to <a href='https://wandb.ai/jacques-furst123-none/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jacques-furst123-none/huggingface' target=\"_blank\">https://wandb.ai/jacques-furst123-none/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jacques-furst123-none/huggingface/runs/3j1gsdbi' target=\"_blank\">https://wandb.ai/jacques-furst123-none/huggingface/runs/3j1gsdbi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results: {'eval_loss': 0.6914925575256348, 'eval_runtime': 1.9345, 'eval_samples_per_second': 71.853, 'eval_steps_per_second': 9.305}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_results = trainer.evaluate(eval_dataset=datasets[2])\n",
    "print(\"Test Results:\", test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1: Predicted Rating: 2.4259257316589355, True Rating: 1\n",
      "Sample 2: Predicted Rating: 0.02743436023592949, True Rating: 0\n",
      "Sample 3: Predicted Rating: 2.2848269939422607, True Rating: 3\n",
      "Sample 4: Predicted Rating: 1.1239687204360962, True Rating: 3\n",
      "Sample 5: Predicted Rating: -0.14834876358509064, True Rating: 0\n",
      "Sample 6: Predicted Rating: 0.062238018959760666, True Rating: 0\n",
      "Sample 7: Predicted Rating: 0.023523341864347458, True Rating: 0\n",
      "Sample 8: Predicted Rating: 2.4145941734313965, True Rating: 2\n",
      "Sample 9: Predicted Rating: 2.04484224319458, True Rating: 1\n",
      "Sample 10: Predicted Rating: 0.048886608332395554, True Rating: 2\n",
      "Sample 11: Predicted Rating: 2.058927536010742, True Rating: 3\n",
      "Sample 12: Predicted Rating: 3.241154193878174, True Rating: 0\n",
      "Sample 13: Predicted Rating: 3.055595874786377, True Rating: 3\n",
      "Sample 14: Predicted Rating: 0.48818302154541016, True Rating: 1\n",
      "Sample 15: Predicted Rating: -0.15813829004764557, True Rating: 0\n",
      "Sample 16: Predicted Rating: 0.04465421661734581, True Rating: 0\n",
      "Sample 17: Predicted Rating: 0.41628366708755493, True Rating: 0\n",
      "Sample 18: Predicted Rating: -0.01194592285901308, True Rating: 0\n",
      "Sample 19: Predicted Rating: -0.09112488478422165, True Rating: 0\n",
      "Sample 20: Predicted Rating: 0.13556934893131256, True Rating: 0\n"
     ]
    }
   ],
   "source": [
    "# evaluate model manually on some test cases\n",
    "new_model.to(device)\n",
    "new_model.eval()\n",
    "\n",
    "#TODO: change tokenization function here!\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(20):\n",
    "        sample = datasets[2][i]\n",
    "        inputs = tokenizer(sample['precondition_text'] + \" \" + sample['response_text'], return_tensors='pt', truncation=True, padding=\"max_length\").to(device)\n",
    "        outputs = new_model(**inputs)\n",
    "        prediction = outputs.logits.item()\n",
    "        print(f\"Sample {i+1}: Predicted Rating: {prediction}, True Rating: {sample['label']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test reward model on prompt structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1: Predicted Rating: 0.28896862268447876\n"
     ]
    }
   ],
   "source": [
    "response_text = \"\"\"\n",
    "                Inhoud: <inhoud>\n",
    "                 <details>\n",
    "                 <summary>parsering</summary>\n",
    "                 <pre>\n",
    "                 <code>\n",
    "        subfact\n",
    "                 </code>\n",
    "                 </pre>\n",
    "                 </details>\n",
    " \n",
    "                 Resultaat:\n",
    "\n",
    "\n",
    "                Subfact: vreemdeling \n",
    " \n",
    "                Positie: Artikel 8 IN Verordening vreemdelingenattributen\n",
    " \n",
    "                Inhoud: de vreemdeling heeft in Nederland uitsluitend rechtmatig verblijf:\n",
    "                <details>\n",
    "                <summary>parsering</summary>\n",
    "                <pre>\n",
    "                <code>\n",
    "        de vreemdeling\n",
    "                </code>\n",
    "                </pre>\n",
    "                </details>\n",
    " \n",
    "                Subfact: vreemdeling \n",
    " \n",
    "                Positie: Artikel 8 IN Verordening vreemdelingenattributen\n",
    " \n",
    "                Inhoud: het verblijf van een vreemdeling in Nederland op grond van deze wet anders dan op de \n",
    "                gronden bedoeld in de artikelen 29 en 34\n",
    "                <details>\n",
    "                <summary>parsering</summary>\n",
    "                <pre>\n",
    "                <code>\n",
    "        het verblijf van een vreemdeling\n",
    "                </code>\n",
    "                </pre>\n",
    "                </details>\n",
    " \n",
    "                Subfact: vreemdeling \n",
    " \n",
    "                Positie: Artikel 8, onder a IN Verordening vreemdelingenattributen\n",
    " \n",
    "                Inhoud: op grond van een verblijfsvergunning voor bepaalde tijd als bedoeld in artikel 14;\n",
    "                <details>\n",
    "                <summary>parsering</summary>\n",
    "                <pre>\n",
    "                <code>\n",
    "        op grond van een verblijfsvergunning voor bepaalde tijd\n",
    "                </code>\n",
    "                </pre>\n",
    "                </details>\n",
    "                \"\"\"\n",
    "\n",
    "precon_text = \"NOT ieder die op grond van een wettelijke bepaling als Nederlander moet worden behandeld\"\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputs = tokenizer(precon_text + \" \" + response_text, return_tensors='pt', truncation=True, padding=\"max_length\").to(device)\n",
    "    outputs = new_model(**inputs)\n",
    "    prediction = outputs.logits.item()\n",
    "    print(f\"Sample {1}: Predicted Rating: {prediction}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
