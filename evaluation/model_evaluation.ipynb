{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e99baab",
   "metadata": {},
   "source": [
    "# Evaluate finetuned Mistral Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d0599d",
   "metadata": {},
   "source": [
    "## Imports and environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7235bb20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "CUDA is available. Using GPU: NVIDIA L40S\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "import ast\n",
    "import evaluate\n",
    "\n",
    "from deepeval.metrics import GEval\n",
    "from deepeval.test_case import LLMTestCaseParams, LLMTestCase\n",
    "\n",
    "from distutils.util import strtobool\n",
    "\n",
    "from deepeval.models.azure_openai import AzureOpenAIModel\n",
    "\n",
    "from peft import PeftModel\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, AutoModelForCausalLM\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Add the parent directory to the Python path\n",
    "# __file__\n",
    "\n",
    "\n",
    "# Adjust this path to point to the directory containing rl_training_new\n",
    "module_path = os.path.abspath(os.path.join('..')) # or another relative path\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from rl_training_new.utils import find_best_window\n",
    "\n",
    "\n",
    "from bert_score import score\n",
    "\n",
    "# load the relevant devices available on the server\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = os.getenv(\"AVAILABLE_DEVICES\")\n",
    "\n",
    "# Enable expandable CUDA segments\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# load cuda\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print(\"CUDA is available. Using GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA is not available. Using CPU.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d1260ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_bool(s):\n",
    "    return bool(strtobool(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "329c4a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = os.getenv(\"GENERATION_MODEL_NAME\")\n",
    "ALGORITHM = os.getenv(\"EVAL_MODEL_ALGORITHM\")\n",
    "RL_TRAINED_ADAPTERS = os.getenv(\"EVAL_MODEL_FOLDER\")\n",
    "EVAL_ANSWERS_CSV = os.getenv(\"EVAL_ANSWERS_CSV\")\n",
    "GENERATE_RESPONSES = str_to_bool(os.getenv(\"GENERATE_RESPONSES\"))\n",
    "OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db917af8",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2ca7f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:01<00:00,  2.09it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): MistralForCausalLM(\n",
       "      (model): MistralModel(\n",
       "        (embed_tokens): Embedding(32768, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x MistralDecoderLayer(\n",
       "            (self_attn): MistralAttention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (mlp): MistralMLP(\n",
       "              (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "              (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "              (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "            (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "        (rotary_emb): MistralRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=32768, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(MODEL)\n",
    "new_model = PeftModel.from_pretrained(base_model, RL_TRAINED_ADAPTERS)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "base_model.eval()\n",
    "new_model.eval()\n",
    "\n",
    "base_model.to(device)\n",
    "new_model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7ace9b",
   "metadata": {},
   "source": [
    "## Get test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a729e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "RL_DATA_PATH = os.getenv(\"RL_DATA_PATH\")\n",
    "df = pd.read_csv(RL_DATA_PATH + \"/test_random.csv\", sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afccf5c",
   "metadata": {},
   "source": [
    "## Generate Model responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab9ff172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate response\n",
    "def generate_response(prompt, tokenizer, model, max_length=1024):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    input_length = inputs['input_ids'].shape[1]\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_length=input_length + max_length, do_sample=True, top_k=50)\n",
    "        generated_ids = outputs[0][input_length:]\n",
    "    return tokenizer.decode(generated_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fe4e660",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              prompt  \\\n",
      "0  \\n\\n\\n                --- Definitie ---\\n\\n   ...   \n",
      "1  \\n\\n\\n                --- Definitie ---\\n\\n   ...   \n",
      "2  \\n\\n\\n                --- Definitie ---\\n\\n   ...   \n",
      "3  \\n\\n\\n                --- Definitie ---\\n\\n   ...   \n",
      "\n",
      "                                  response_new_model  \\\n",
      "0     In deze tekst zijn er voor de volgende fact...   \n",
      "1       Voorbeeld: Preconditie: De vreemdeling ve...   \n",
      "2                     1. Een preconditie voor de ...   \n",
      "3  1. De belastbare moet zijn of woonachtig in Ne...   \n",
      "\n",
      "                                 response_base_model  \n",
      "0  Inhoud: <overzicht van de inhoud van de subfac...  \n",
      "1  1. De vreemdeling beschikt over een geldige ma...  \n",
      "2  1. Verantwoordelijk is een Minister, met betre...  \n",
      "3  ------------------------------------------\\n\\n...  \n"
     ]
    }
   ],
   "source": [
    "if GENERATE_RESPONSES:\n",
    "    # Apply both models\n",
    "    df['response_base_model'] = df['prompt'].apply(lambda x: generate_response(x, tokenizer, base_model))\n",
    "    df['response_new_model'] = df['prompt'].apply(lambda x: generate_response(x, tokenizer, new_model))\n",
    "\n",
    "    # Show result\n",
    "    print(df[['prompt', 'response_new_model', 'response_base_model']])\n",
    "\n",
    "    # store response df in csv\n",
    "    df.to_csv(EVAL_ANSWERS_CSV, index=False, sep=';')\n",
    "else:\n",
    "    pd.read_csv(EVAL_ANSWERS_CSV, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0658429a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inhoud: <overzicht van de inhoud van de subfact> \n",
      "\n",
      " ---------------------------------------------------------\n",
      "\n",
      "                Subfact: budgettaire totaalbeeld\n",
      " ----------------------------------------------------------\n",
      "  Positie: Artikel 4.4, eerste lid, onderdeel d IN Hoofdstuk 4. Begrotingsbeheer en financieel beheer: verantwoordelijkheden\n",
      "\n",
      "  Inhoud: bevat in elk geval: a. het budgettaire totaalbeeld voor het betrokken begrotingsjaar en de vier daaropvolgende jaren van de rijksbegroting en de niet tot de rijksbegroting behorende budgetdisciplinesectoren\n",
      "\n",
      "  ---------------------------------------------------------\n",
      "\n",
      "                Subfact: budgettaire beschouwingen\n",
      " ----------------------------------------------------------\n",
      "  Positie: Artikel 4.4, eerste lid, onderdeel d IN Hoofdstuk 4. Begrotingsbeheer en financieel beheer: verantwoordelijkheden\n",
      "\n",
      "  Inhoud: bevat in elk geval: b. de budgettaire beschouwingen over het voorgenomen beleid voor de collectieve sector\n",
      "\n",
      "  ---------------------------------------------------------\n",
      "\n",
      "                Subfact: overzicht van de uitgaven en ontvangsten\n",
      " ----------------------------------------------------------\n",
      "  Positie: Artikel 4.4, eerste lid, onderdeel d IN Hoofdstuk 4. Begrotingsbeheer en financieel beheer: verantwoordelijkheden\n",
      "\n",
      "  Inhoud: bevat in elk gavldt een overzicht van de uitgaven en de ontvangsten in de begrotingen voor het begrotingsjaar en de vier daarop aansluitende jaren.\n",
      "   In deze tekst zijn er voor de volgende fact drie subfacts te vinden:\n",
      "\n",
      "                1. Subfact: Budgettaire beschouwingen\n",
      "                    Positie: Artikel 4.2.4.a, sectie 2.24 van de Rijksbegrotingsvoorschriften 2024\n",
      "\n",
      "                2. Subfact: Budgettaire totaalbeeld\n",
      "                    Positie: Artikel 4.2.4.a, sectie 2.24 van de Rijksbegrotingsvoorschriften 2024\n",
      "\n",
      "                3. Subfact: Overzicht van de uitgaven en ontvangsten\n",
      "                    Positie: Artikel 4.2.4.c, sectie 2.24 van de Rijksbegrotingsvoorschriften 2024\n",
      "\n",
      "                Voer elke subfact in de miljodenotabeschrijving in volgorde op:\n",
      "\n",
      "                - Budgettaire beschouwingen\n",
      "                - Budgettaire totaalbeeld\n",
      "                - Overzicht van de uitgaven en ontvangsten\n"
     ]
    }
   ],
   "source": [
    "print(df['response_base_model'][0])\n",
    "print(df['response_new_model'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0cb0c503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['een overzicht van de uitgaven en de ontvangsten in de begrotingen voor het begrotingsjaar en de vier daarop aansluitende jaren.\\nArtikel 2.23 sectie 4c IN Comptabiliteitswet 2016\\n\\nde budgettaire beschouwingen over het voorgenomen beleid voor de collectieve sector\\nArtikel 2.23 sectie 4b IN Comptabiliteitswet 2016\\n\\nhet budgettaire totaalbeeld voor het betrokken begrotingsjaar en de vier daaropvolgende jaren van de rijksbegroting en de niet tot de rijksbegroting behorende budgetdisciplinesectoren\\nArtikel 2.23 sectie 4a IN Comptabiliteitswet 2016\\n\\n', 'de verblijfsvergunning wordt verleend met ingang van de dag waarop de vreemdeling heeft aangetoond dat hij aan alle voorwaarden voldoet\\nArtikel 26, sectie 1 IN Vreemdelingenwet 2024\\n\\nde verblijfsvergunning wordt verleend met ingang van een dag eerder dan de dag waarop de aanvraag is ontvangen\\nArtikel 26, sectie 1 IN Vreemdelingenwet 2024\\n\\n', 'Begroting bevat begrotingsstaat\\nArtikel 2.2 IN Comptabiliteitswet 2016\\n\\n', 'college stelt het recht op bijstand vast op schriftelijke aanvraag\\nArtikel 43, sectie 1 IN Participatiewet\\n\\nNOT bijstand wordt door de echtgenoten gezamenlijk aangevraagd dan wel door een van hen met schriftelijke toestemming van de ander\\nArtikel 43, sectie 2 IN Participatiewet\\n\\nNOT belanghebbende is een in Nederland woonachtige Nederlander\\nArtikel 11, sectie 1 IN Participatiewet\\n\\nNOT belanghebbende is een hier te lande woonachtige vreemdeling die rechtmatig in Nederland verblijf houdt in de zin van artikel 8, onderdelen a tot en met e en l, van de Vreemdelingenwet 2000\\nArtikel 11, sectie 2 IN Participatiewet\\n\\n\\nbelanghebbende aan wie rechtens zijn vrijheid is ontnomen\\nArtikel 13, sectie 1 a IN Participatiewet\\n\\nbelanghebbende die zich onttrekt aan de tenuitvoerlegging van een vrijheidsstraf of vrijheidsbenemende maatregel\\nArtikel 13, sectie 1 b IN Participatiewet\\n\\nbelanghebbende die zijn militaire of vervangende dienstplicht vervult\\nArtikel 13, sectie 1 c IN Participatiewet\\n\\nbelanghebbende die wegens werkstaking of uitsluiting niet deelneemt aan de arbeid, voorzover diens gebrek aan middelen daarvan het gevolg is\\nArtikel 13, sectie 1 d IN Participatiewet\\n\\nbelanghebbende die per kalenderjaar langer dan vier weken verblijf houdt buiten Nederland\\nArtikel 13, sectie 1 e IN Participatiewet\\n\\nbelanghebbende die bijstand vraagt ter gedeeltelijke of volledige aflossing van een schuldenlast\\nArtikel 13, sectie 1 g IN Participatiewet\\n\\nbelanghebbende die overigens bij het ontstaan van de schuldenlast dan wel nadien, beschikte of beschikt over de middelen om in de noodzakelijke kosten van het bestaan te voorzien\\nArtikel 13, sectie 1 g IN Participatiewet\\n\\nbelanghebbende die een uitreiziger is\\nArtikel 13, sectie 1 h IN Participatiewet\\n\\nbelanghebbende van 18, 19 of 20 jaar die in een inrichting verblijft\\nArtikel 13, sectie 2 a IN Participatiewet\\n\\nbelanghebbende die jonger is dan 27 jaar en uit ’s Rijks kas bekostigd onderwijs kan volgen\\nArtikel 13, sectie 2 c IN Participatiewet\\n\\nbelanghebbende heeft in verband met volgen onderwijs aanspraak op studiefinanciering op grond van de Wet studiefinanciering 2000\\nArtikel 13, sectie 2 c 1 IN Participatiewet\\n\\nbelanghebbende heeft geen aanspraak op studiefinanciering omdat hij onderwijs dat hij kan volgen onderwijs, niet volgt\\nArtikel 13, sectie 2 c 2 IN Participatiewet\\n\\nbelanghebbende die jonger is dan 27 jaar en uit wiens houding en gedragingen ondubbelzinnig blijkt dat hij de verplichtingen, bedoeld in artikel 9, eerste lid, of artikel 55 niet wil nakomen\\nArtikel 13, sectie 2 d IN Participatiewet\\n\\nbijstand wordt toegekend voor de dag waarop de belanghebbende zich heeft gemeld om bijstand aan te vragen\\nArtikel 44, sectie 1 IN Participatiewet\\n\\nNOT het in aanmerking te nemen inkomen lager is dan de bijstandsnorm\\nArtikel 19 sectie 1 a IN Participatiewet\\n\\ner is in aanmerking te nemen vermogen\\nArtikel 19 sectie 1 b IN Participatiewet\\n\\n']\n"
     ]
    }
   ],
   "source": [
    "candidates_new = df['response_new_model'].tolist()\n",
    "candidates_base = df['response_base_model'].tolist()\n",
    "\n",
    "precon_text_list = df['precondition_texts'].to_list()\n",
    "precon_pos_list = df[\"precondition_positions\"].to_list()\n",
    "\n",
    "references = []\n",
    "for dict1, dict2 in zip(precon_text_list, precon_pos_list):\n",
    "    dict1 = ast.literal_eval(dict1)\n",
    "    dict2 = ast.literal_eval(dict2)\n",
    "    combined = []\n",
    "    for key in dict1.keys():  # or use sorted(dict1.keys()) if key order isn't guaranteed\n",
    "        combined.append(str(dict1[key]) + '\\n')\n",
    "        combined.append(str(dict2[key]) + '\\n\\n')\n",
    "    references.append(''.join(combined))\n",
    "\n",
    "print(references)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1edeb3",
   "metadata": {},
   "source": [
    "## ROUGE/BLEU on relevant sequence from the answer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8b32cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results new: {'rouge1': np.float64(0.2565681920151968), 'rouge2': np.float64(0.06891495296075426), 'rougeL': np.float64(0.15538943880354444), 'rougeLsum': np.float64(0.2423120915525212)}\n",
      "Results base: {'rouge1': np.float64(0.3537931177336315), 'rouge2': np.float64(0.20274294527799072), 'rougeL': np.float64(0.23936522250038622), 'rougeLsum': np.float64(0.3421915218579997)}\n"
     ]
    }
   ],
   "source": [
    "# Evaluation metric 1: ROUGE on relevant sequences\n",
    "# WHy not BLEU --> penalizes missing ngrams, not something I m looking for here\n",
    "\n",
    "rouge = evaluate.load('rouge')\n",
    "\n",
    "results_new = rouge.compute(predictions=candidates_new, references=references)\n",
    "results_base = rouge.compute(predictions=candidates_base, references=references)\n",
    "\n",
    "print(f\"Results new: {results_new}\")\n",
    "print(f\"Results base: {results_base}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db770b90",
   "metadata": {},
   "source": [
    "## BERT Score --> based on embedding similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5360138",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# Maybe add BERTScore --> semantic similarity based on sentencetransformer\n",
    "P_new, R_new, F1_new = score(\n",
    "    candidates_new, \n",
    "    references, \n",
    "    model_type='answerdotai/ModernBERT-base', \n",
    "    num_layers=22,\n",
    "    lang='nl')\n",
    "\n",
    "P_base, R_base, F1_base = score(\n",
    "    candidates_base, \n",
    "    references, \n",
    "    model_type='answerdotai/ModernBERT-base', \n",
    "    num_layers=22,\n",
    "    lang='nl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "afdb33c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Score metrics new: (tensor([0.7910, 0.7866, 0.6702, 0.7962]), tensor([0.8069, 0.8561, 0.7887, 0.8184]), tensor([0.7988, 0.8199, 0.7247, 0.8071]))\n",
      "BERT Score metrics base: (tensor([0.8020, 0.7591, 0.7961, 0.8030]), tensor([0.8674, 0.8447, 0.8619, 0.8218]), tensor([0.8334, 0.7996, 0.8277, 0.8123]))\n",
      "F1 base: 0.8182516098022461, F1 new: 0.7876150608062744\n"
     ]
    }
   ],
   "source": [
    "print(f\"BERT Score metrics new: {P_new, R_new, F1_new}\")\n",
    "print(f\"BERT Score metrics base: {P_base, R_base, F1_base}\")\n",
    "\n",
    "print(f\"F1 base: {F1_base.mean()}, F1 new: {F1_new.mean()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78332f58",
   "metadata": {},
   "source": [
    "## G-Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02388cd4",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Unsupported type for model: <class 'openai.lib.azure.AzureOpenAI'>. Expected None, str, DeepEvalBaseLLM, GPTModel, AzureOpenAIModel, LiteLLMModel, OllamaModel, LocalModel.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 43\u001b[0m\n\u001b[1;32m     17\u001b[0m azure_model \u001b[38;5;241m=\u001b[39m AzureOpenAI(\n\u001b[1;32m     18\u001b[0m     api_version\u001b[38;5;241m=\u001b[39mapi_version,\n\u001b[1;32m     19\u001b[0m     azure_endpoint\u001b[38;5;241m=\u001b[39mendpoint,\n\u001b[1;32m     20\u001b[0m     api_key\u001b[38;5;241m=\u001b[39msubscription_key,\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     24\u001b[0m criteria \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     25\u001b[0m \n\u001b[1;32m     26\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     40\u001b[0m \n\u001b[1;32m     41\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m \u001b[38;5;66;03m#TODO: think about criteria...\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m precondition_extraction_metric \u001b[38;5;241m=\u001b[39m \u001b[43mGEval\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPrecondition_Extraction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluation_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mLLMTestCaseParams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mINPUT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLLMTestCaseParams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mACTUAL_OUTPUT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLLMTestCaseParams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEXPECTED_OUTPUT\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mazure_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \n\u001b[1;32m     49\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Now define your test case, actual_output is your LLM output\u001b[39;00m\n\u001b[1;32m     52\u001b[0m g_eval_new \u001b[38;5;241m=\u001b[39m LLMTestCase(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mprompts, actual_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtracetd Preconditions: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcandidates_new\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, expected_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected preconditions: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreferences\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/Evaluation/lib/python3.9/site-packages/deepeval/metrics/g_eval/g_eval.py:56\u001b[0m, in \u001b[0;36mGEval.__init__\u001b[0;34m(self, name, evaluation_params, criteria, evaluation_steps, rubric, model, threshold, top_logprobs, async_mode, strict_mode, verbose_mode, _include_g_eval_suffix)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriteria \u001b[38;5;241m=\u001b[39m criteria\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrubric \u001b[38;5;241m=\u001b[39m validate_and_sort_rubrics(rubric)\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39musing_native_model \u001b[38;5;241m=\u001b[39m \u001b[43minitialize_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mget_model_name()\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_steps \u001b[38;5;241m=\u001b[39m evaluation_steps\n",
      "File \u001b[0;32m~/miniconda3/envs/Evaluation/lib/python3.9/site-packages/deepeval/metrics/utils.py:312\u001b[0m, in \u001b[0;36minitialize_model\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m GPTModel(model\u001b[38;5;241m=\u001b[39mmodel), \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;66;03m# Otherwise (the model is a wrong type), we raise an error\u001b[39;00m\n\u001b[0;32m--> 312\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    313\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported type for model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(model)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Expected None, str, DeepEvalBaseLLM, GPTModel, AzureOpenAIModel, LiteLLMModel, OllamaModel, LocalModel.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    314\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: Unsupported type for model: <class 'openai.lib.azure.AzureOpenAI'>. Expected None, str, DeepEvalBaseLLM, GPTModel, AzureOpenAIModel, LiteLLMModel, OllamaModel, LocalModel."
     ]
    }
   ],
   "source": [
    "# Evaluation metric 2: Evaluate whole answer on G-Eval\n",
    "\n",
    "prompts = df['prompt'].tolist()\n",
    "\n",
    "# setup variables for the Azure OpenAI API\n",
    "endpoint = \"https://openai-ds-instance-sweden.openai.azure.com/\"\n",
    "model_name = \"gpt-4.1\"\n",
    "deployment = \"deze-voor-alles\"\n",
    "\n",
    "\n",
    "\n",
    "subscription_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "api_version = \"2024-12-01-preview\"\n",
    "\n",
    "\n",
    "\n",
    "azure_model = AzureOpenAIModel(\n",
    "    api_version=api_version,\n",
    "    azure_endpoint=endpoint,\n",
    "    api_key=subscription_key,\n",
    ")\n",
    "\n",
    "\n",
    "criteria = \"\"\"\n",
    "\n",
    "\n",
    "Evalueer hoe goed een taalmodel presteerde in de taak van voorwaarde-extractie uit Nederlandse juridische teksten.\n",
    "\n",
    "Voor elke lijst van precondities/subfacts werd de act/fact aan een taalmodel gegeven als onderdeel van een prompt, met de opdracht om alle bijbehorende subfact/preconditie(s) en hun respectieve positie(s) in de tekst terug te geven.\n",
    "\n",
    "Uw taak is om per paar te evalueren hoe goed het model presteerde op twee punten (op een 4-punt Likert-schaal):\n",
    "\n",
    "1. **Het vinden van alle relevante precondities in de tekst**\n",
    "2. **Hoe duidelijk de positie in de tekst is die het model aanduidde**\n",
    "\n",
    "Voor het modelantwoord krijgt u het hele antwoord voor een act/fact (die dus meredere precondities/subfacts kan beïnhouden) en moet u voor elke ground truth preconditie afzonderlijk evalueren of deze \n",
    "    a) aanwezig is in het antwoord en \n",
    "    b) of de positie ervan ook goed is aangegeven in het antwoord. \n",
    "\n",
    "\n",
    "\"\"\" #TODO: think about criteria...\n",
    "\n",
    "precondition_extraction_metric = GEval(\n",
    "    name=\"Precondition_Extraction\",\n",
    "    criteria=criteria,\n",
    "    evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT, LLMTestCaseParams.EXPECTED_OUTPUT],\n",
    "    model=azure_model,\n",
    "\n",
    ")\n",
    "\n",
    "# Now define your test case, actual_output is your LLM output\n",
    "g_eval_new = LLMTestCase(input=prompts, actual_output=f\"Extracetd Preconditions: {candidates_new}\", expected_output=f\"Expected preconditions: {references}\")\n",
    "g_eval_base = LLMTestCase(input=prompts, actual_output=f\"Extracetd Preconditions: {candidates_base}\", expected_output=f\"Expected preconditions: {references}\")\n",
    "\n",
    "\n",
    "# Use G-Eval metric\n",
    "precondition_extraction_metric.measure(g_eval_new)\n",
    "print(\"Score:\", precondition_extraction_metric.score)\n",
    "print(\"Reason:\", precondition_extraction_metric.reason)\n",
    "\n",
    "precondition_extraction_metric.measure(g_eval_base)\n",
    "print(\"Score:\", precondition_extraction_metric.score)\n",
    "print(\"Reason:\", precondition_extraction_metric.reason)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1585bcb3",
   "metadata": {},
   "source": [
    "Run this in the proper environment tomorrow:\n",
    "\n",
    "https://deepeval.com/docs/metrics-introduction#using-azure-openai\n",
    "\n",
    "deepeval set-azure-openai \\\n",
    "    --openai-endpoint=<endpoint> \\ # e.g. https://example-resource.azure.openai.com/\n",
    "    --openai-api-key=<api_key> \\\n",
    "    --openai-model-name=<model_name> \\ # e.g. gpt-4o\n",
    "    --deployment-name=<deployment_name> \\  # e.g. Test Deployment\n",
    "    --openai-api-version=<openai_api_version> \\ # e.g. 2025-01-01-preview\n",
    "    --model-version=<model_version> # e.g. 2024-11-20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6086f2",
   "metadata": {},
   "source": [
    "## Personal evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f34ef6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate about 10 answers with both models --> load them into the user interface and get reward score\n",
    "# Also maybe do qualitative evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Evaluation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
