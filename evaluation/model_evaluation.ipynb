{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e99baab",
   "metadata": {},
   "source": [
    "# Evaluate finetuned Mistral Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d0599d",
   "metadata": {},
   "source": [
    "## Imports and environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7235bb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from peft import PeftModel\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, AutoModelForCausalLM\n",
    "\n",
    "from rl_training_new.utils import find_best_window\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Add the parent directory to the Python path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname(os.getcwd()), '..')))\n",
    "\n",
    "# load the relevant devices available on the server\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = os.getenv(\"AVAILABLE_DEVICES\")\n",
    "\n",
    "# Enable expandable CUDA segments\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# load cuda\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print(\"CUDA is available. Using GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA is not available. Using CPU.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329c4a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = os.getenv(\"GENERATION_MODEL_NAME\")\n",
    "ALGORITHM = os.getenv(\"EVAL_MODEL_ALGORITHM\")\n",
    "RL_TRAINED_ADAPTERS = os.getenv(\"EVAL_MODEL_FOLDER\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db917af8",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ca7f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_test = AutoModelForSequenceClassification.from_pretrained(MODEL, num_labels=1)\n",
    "new_model = PeftModel.from_pretrained(base_model_test, RL_TRAINED_ADAPTERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a729e82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f1edeb3",
   "metadata": {},
   "source": [
    "## ROUGE/BLEU on relevant sequence from the answer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b32cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation metric 1: ROUGE on relevant sequences\n",
    "# WHy not BLEU --> penalizes missing ngrams, not something I m looking for here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db770b90",
   "metadata": {},
   "source": [
    "## BERT Score --> based on embedding similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5360138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maybe add BERTScore --> semantic similarity based on sentencetransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78332f58",
   "metadata": {},
   "source": [
    "## G-Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02388cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation metric 2: Evaluate whole answer on G-Eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6086f2",
   "metadata": {},
   "source": [
    "## Personal evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34ef6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate about 10 answers with both models --> load them into the user interface and get reward score\n",
    "# Also maybe do qualitative evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
