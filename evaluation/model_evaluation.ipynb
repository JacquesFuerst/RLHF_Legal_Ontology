{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e99baab",
   "metadata": {},
   "source": [
    "# Evaluate finetuned Mistral Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d0599d",
   "metadata": {},
   "source": [
    "## Imports and environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7235bb20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacques.furst/miniconda3/envs/Evaluation/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "CUDA is available. Using GPU: NVIDIA L40S\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "import ast\n",
    "import evaluate\n",
    "\n",
    "from deepeval.metrics import GEval\n",
    "from deepeval.test_case import LLMTestCaseParams, LLMTestCase\n",
    "\n",
    "from distutils.util import strtobool\n",
    "\n",
    "# from deepeval.models.azure_openai import AzureOpenAIModel\n",
    "\n",
    "from peft import PeftModel\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, AutoModelForCausalLM\n",
    "\n",
    "from deepeval.models import AzureOpenAIModel\n",
    "# from deepeval.metrics import AnswerRelevancyMetric\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Add the parent directory to the Python path\n",
    "# __file__\n",
    "\n",
    "\n",
    "# Adjust this path to point to the directory containing rl_training_new\n",
    "module_path = os.path.abspath(os.path.join('..')) # or another relative path\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from rl_training_new.utils import find_best_window\n",
    "\n",
    "\n",
    "from bert_score import score\n",
    "\n",
    "# load the relevant devices available on the server\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = os.getenv(\"AVAILABLE_DEVICES\")\n",
    "\n",
    "# Enable expandable CUDA segments\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# load cuda\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print(\"CUDA is available. Using GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA is not available. Using CPU.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d1260ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_bool(s):\n",
    "    return bool(strtobool(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "329c4a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = os.getenv(\"GENERATION_MODEL_NAME\")\n",
    "ALGORITHM = os.getenv(\"EVAL_MODEL_ALGORITHM\")\n",
    "RL_TRAINED_ADAPTERS = os.getenv(\"EVAL_MODEL_FOLDER\")\n",
    "EVAL_ANSWERS_CSV = os.getenv(\"EVAL_ANSWERS_CSV\")\n",
    "GENERATE_RESPONSES = str_to_bool(os.getenv(\"GENERATE_RESPONSES\"))\n",
    "OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "# RL_DATA_PATH = os.getenv(\"RL_DATA_PATH\")\n",
    "EVAL_FILE = os.getenv(\"EVAL_FILE\")\n",
    "NUM_RESPONSES_EVAL = int(os.getenv(\"NUM_RESPONSES_EVAL\"))  # Number of responses per model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db917af8",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2ca7f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:11<00:00,  3.82s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:11<00:00,  3.67s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): MistralForCausalLM(\n",
       "      (model): MistralModel(\n",
       "        (embed_tokens): Embedding(32768, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x MistralDecoderLayer(\n",
       "            (self_attn): MistralAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (mlp): MistralMLP(\n",
       "              (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "            (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "        (rotary_emb): MistralRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=32768, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# base_model = AutoModelForCausalLM.from_pretrained(MODEL)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(MODEL,  \n",
    "                                            #  device_map=\"auto\",  # For GPU/TPU acceleration\n",
    "                                            device_map=None,\n",
    "                                            torch_dtype=torch.bfloat16,\n",
    "                                            #  load_in_4bit=True,\n",
    "                                            quantization_config={\n",
    "                                                \"load_in_4bit\": True,\n",
    "                                                \"bnb_4bit_compute_dtype\": torch.bfloat16,\n",
    "                                                \"bnb_4bit_use_double_quant\": True,\n",
    "                                                \"bnb_4bit_quant_type\": \"nf4\"\n",
    "                                                }\n",
    "                                            )   # Optimize precision)\n",
    "# base_model_new = AutoModelForCausalLM.from_pretrained(MODEL)\n",
    "base_model_new = AutoModelForCausalLM.from_pretrained(MODEL,  \n",
    "                                            #  device_map=\"auto\",  # For GPU/TPU acceleration\n",
    "                                            device_map=None,\n",
    "                                            torch_dtype=torch.bfloat16,\n",
    "                                            #  load_in_4bit=True,\n",
    "                                            quantization_config={\n",
    "                                                \"load_in_4bit\": True,\n",
    "                                                \"bnb_4bit_compute_dtype\": torch.bfloat16,\n",
    "                                                \"bnb_4bit_use_double_quant\": True,\n",
    "                                                \"bnb_4bit_quant_type\": \"nf4\"\n",
    "                                                }\n",
    "                                            )   # Optimize precision)\n",
    "new_model = PeftModel.from_pretrained(base_model_new, RL_TRAINED_ADAPTERS)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "base_model.eval()\n",
    "new_model.eval()\n",
    "\n",
    "base_model.to(device)\n",
    "new_model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7ace9b",
   "metadata": {},
   "source": [
    "## Get test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a729e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(EVAL_FILE, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afccf5c",
   "metadata": {},
   "source": [
    "## Generate Model responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab9ff172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate response\n",
    "def generate_response(prompt, tokenizer, model, max_length=1024):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    input_length = inputs['input_ids'].shape[1]\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_length=input_length + max_length, do_sample=True, top_k=50)\n",
    "        generated_ids = outputs[0][input_length:]\n",
    "    return tokenizer.decode(generated_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fe4e660",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              prompt  \\\n",
      "0  \\n\\n\\n                --- Definitie ---\\n\\n   ...   \n",
      "1  \\n\\n\\n                --- Definitie ---\\n\\n   ...   \n",
      "2  \\n\\n\\n                --- Definitie ---\\n\\n   ...   \n",
      "3  \\n\\n\\n                --- Definitie ---\\n\\n   ...   \n",
      "\n",
      "                                response_new_model_1  \\\n",
      "0   Subfact: budgettaire totaalbeeld \\n  Positie:...   \n",
      "1   Preconditie: de aanvrager moet beschikken ove...   \n",
      "2  ----------------------------------------------...   \n",
      "3   Preconditie:  Wezen ongehuwde moeder of gehuw...   \n",
      "\n",
      "                                response_new_model_2  \\\n",
      "0   Subfact: Budgettaire totaalbeeld\\n           ...   \n",
      "1   Preconditie: De vreemdeling moet beschikken o...   \n",
      "2  --------------------------------------------\\n...   \n",
      "3   Preconditie: De voorstellen van wet tot vasts...   \n",
      "\n",
      "                                response_new_model_3  \\\n",
      "0  ----------\\n\\n                Subfact: Budgett...   \n",
      "1   Preconditie: degene overeenkomstig artikel 2a...   \n",
      "2  1. Preconditie: Ieder minister stelt een begro...   \n",
      "3   Preconditie: In het betreffende begrotingsjaa...   \n",
      "\n",
      "                                response_new_model_4  \\\n",
      "0   Subfact: Budgettaire totaalbeeld\\n           ...   \n",
      "1   Preconditie: de vreemdeling beschikt over een...   \n",
      "2   Preconditie: De Minister stelt voor de aanvan...   \n",
      "3   Preconditie: Om recht op algemene bijstand te...   \n",
      "\n",
      "                                response_new_model_5  \\\n",
      "0   Subfact: budgettaire totaalbeeld  \\n  Positie...   \n",
      "1   Preconditie: de vreemdeling beschikt over een...   \n",
      "2   Preconditie: De begrotingen voor elk minister...   \n",
      "3   Preconditie: Het aangaan is gezorgd\\n        ...   \n",
      "\n",
      "                                response_new_model_6  \\\n",
      "0   Subfact: \"Budgettaire totaalbeeld\"\\n  Positio...   \n",
      "1   Verblijfsvergunning regulier\\n\\n             ...   \n",
      "2  Preconditie: De ministers verantwoordelijk zij...   \n",
      "3   Preconditie: De act moet uitgevoerd worden do...   \n",
      "\n",
      "                                response_new_model_7  \\\n",
      "0   Subfact: Het budgettaire totaalbeeld\\n       ...   \n",
      "1   - Preconditie: De aanvrager moet geschikte mi...   \n",
      "2   Preconditie: De Ministers, ieder met betrekki...   \n",
      "3   Preconditie: Geen aandachtspunt in de uitvoer...   \n",
      "\n",
      "                                response_new_model_8  \\\n",
      "0   Subfact: Budgettaire totaalbeeld \\n\\n        ...   \n",
      "1  ---------------------\\n\\n                Preco...   \n",
      "2   Preconditie: Onze Ministers, ieder met betrek...   \n",
      "3   Preconditie: Wij dienen de voorstellen van we...   \n",
      "\n",
      "                                response_new_model_9  ...  \\\n",
      "0   Subfact: budgettaire beschouwingen\\n         ...  ...   \n",
      "1   Preconditie: de vreemdeling beschikt over een...  ...   \n",
      "2   Preconditie: ministers voorstellen voor een b...  ...   \n",
      "3  --------\\n\\n                Preconditie: Wij d...  ...   \n",
      "\n",
      "                               response_base_model_1  \\\n",
      "0  <inhoud subfact>\\n\\n\\n               Subfact: ...   \n",
      "1  ----------------------------------------------...   \n",
      "2  -- Voorbeeld --\\n\\n                 Preconditi...   \n",
      "3  ----------------------------------------------...   \n",
      "\n",
      "                               response_base_model_2  \\\n",
      "0   Inhoud: <inhoud van de subfact, genoteerd na ...   \n",
      "1         Preconditie: De vreemdeling heeft in Ne...   \n",
      "2  1. Vorige stempel: De Ministers, elke voor het...   \n",
      "3  -------------------------------------\\n\\n\\n   ...   \n",
      "\n",
      "                               response_base_model_3  \\\n",
      "0   Inhoud: <Inhoud>\\n --------------------------...   \n",
      "1  1. Er is geen geldige machtiging tot voorlopig...   \n",
      "2  ------\\n\\n                Preconditie: Ieder m...   \n",
      "3   Inhoud: <Inhoud van de preconditie>\\n\\n      ...   \n",
      "\n",
      "                               response_base_model_4  \\\n",
      "0  Inhoud: Op basis van het budgettaire totaalbee...   \n",
      "1  1. De vreemdeling beschikt over een geldige ma...   \n",
      "2   Voorbeeld:\\n                  * Preconditie: ...   \n",
      "3  1. Weigeren recht op algemene bijstand:\\n\\n   ...   \n",
      "\n",
      "                               response_base_model_5  \\\n",
      "0  -------\\n                Budgettaire totaalbee...   \n",
      "1  ------\\n                Preconditie: De vreemd...   \n",
      "2   Inhoud: <definitie> \\n \\n\\n                Op...   \n",
      "3   <inhoud preconditie>\\n\\n\\n                Pre...   \n",
      "\n",
      "                               response_base_model_6  \\\n",
      "0   Inhoud: <tekst waar de subfact wordt opgeteke...   \n",
      "1   Voorbeeld:\\n\\n                Preconditie: de...   \n",
      "2         - Preconditie: Onze Ministers, ieder me...   \n",
      "3  <inhoud>\\n \\n\\n                 --- Informatie...   \n",
      "\n",
      "                               response_base_model_7  \\\n",
      "0  --------------\\n\\n                Subfact: het...   \n",
      "1   Verblijfsvergunning reguliere Verlenen:\\n    ...   \n",
      "2   --- Voorbeeld ---\\n\\n                  Precon...   \n",
      "3  Preconditie: De handeling om een afkondiging t...   \n",
      "\n",
      "                               response_base_model_8  \\\n",
      "0   Inhoud: <Factueel omdat inhoud informatie voo...   \n",
      "1  1. Beschikt over een geldige machtiging tot vo...   \n",
      "2  1. Preconditie: elke minister stelt voor de aa...   \n",
      "3   - Preconditie: Een indienaar van een recht op...   \n",
      "\n",
      "                               response_base_model_9  \\\n",
      "0   Subfact: budgettaire totaalbeeld \\n          ...   \n",
      "1   Zie de brontekst voor extra informatie.\\n\\n  ...   \n",
      "2  1. Minister verantwoordelijk: <minister>\\n\\n  ...   \n",
      "3   Eigenschap: <eigenschap> \\n\\n                ...   \n",
      "\n",
      "                              response_base_model_10  \n",
      "0  Inhoud: <descriptie> \\n\\n\\n\\n                S...  \n",
      "1  ----------------------------------------------...  \n",
      "2  1. Preconditie: Het aantal ministers moet er e...  \n",
      "3   * Preconditie: Recht op algemene bijstand kan...  \n",
      "\n",
      "[4 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "if GENERATE_RESPONSES:\n",
    "    # Generate multiple responses for each prompt\n",
    "    for i in range(NUM_RESPONSES_EVAL):\n",
    "        df[f'response_base_model_{i+1}'] = df['prompt'].apply(lambda x: generate_response(x, tokenizer, base_model))\n",
    "        df[f'response_new_model_{i+1}'] = df['prompt'].apply(lambda x: generate_response(x, tokenizer, new_model))\n",
    "\n",
    "    # Show result: displaying first response columns for brevity\n",
    "    response_cols = [f'response_new_model_{i+1}' for i in range(NUM_RESPONSES_EVAL)] + \\\n",
    "                    [f'response_base_model_{i+1}' for i in range(NUM_RESPONSES_EVAL)]\n",
    "    print(df[['prompt'] + response_cols])\n",
    "\n",
    "    # Store in CSV\n",
    "    df.to_csv(EVAL_ANSWERS_CSV, index=False, sep=';')\n",
    "\n",
    "else:\n",
    "    df = pd.read_csv(EVAL_ANSWERS_CSV, sep=';')\n",
    "\n",
    "\n",
    "\n",
    "# if GENERATE_RESPONSES:\n",
    "#     # Apply both models\n",
    "#     df['response_base_model'] = df['prompt'].apply(lambda x: generate_response(x, tokenizer, base_model))\n",
    "#     df['response_new_model'] = df['prompt'].apply(lambda x: generate_response(x, tokenizer, new_model))\n",
    "\n",
    "#     # Show result\n",
    "#     print(df[['prompt', 'response_new_model', 'response_base_model']])\n",
    "\n",
    "#     # store response df in csv\n",
    "#     df.to_csv(EVAL_ANSWERS_CSV, index=False, sep=';')\n",
    "# else:\n",
    "#     pd.read_csv(EVAL_ANSWERS_CSV, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0658429a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df['response_base_model'][0])\n",
    "# print(df['response_new_model'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0cb0c503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['een overzicht van de uitgaven en de ontvangsten in de begrotingen voor het begrotingsjaar en de vier daarop aansluitende jaren.\\nArtikel 2.23 sectie 4c IN Comptabiliteitswet 2016\\n\\nde budgettaire beschouwingen over het voorgenomen beleid voor de collectieve sector\\nArtikel 2.23 sectie 4b IN Comptabiliteitswet 2016\\n\\nhet budgettaire totaalbeeld voor het betrokken begrotingsjaar en de vier daaropvolgende jaren van de rijksbegroting en de niet tot de rijksbegroting behorende budgetdisciplinesectoren\\nArtikel 2.23 sectie 4a IN Comptabiliteitswet 2016\\n\\n', 'de verblijfsvergunning wordt verleend met ingang van de dag waarop de vreemdeling heeft aangetoond dat hij aan alle voorwaarden voldoet\\nArtikel 26, sectie 1 IN Vreemdelingenwet 2024\\n\\nde verblijfsvergunning wordt verleend met ingang van een dag eerder dan de dag waarop de aanvraag is ontvangen\\nArtikel 26, sectie 1 IN Vreemdelingenwet 2024\\n\\n', 'Begroting bevat begrotingsstaat\\nArtikel 2.2 IN Comptabiliteitswet 2016\\n\\n', 'college stelt het recht op bijstand vast op schriftelijke aanvraag\\nArtikel 43, sectie 1 IN Participatiewet\\n\\nNOT bijstand wordt door de echtgenoten gezamenlijk aangevraagd dan wel door een van hen met schriftelijke toestemming van de ander\\nArtikel 43, sectie 2 IN Participatiewet\\n\\nNOT belanghebbende is een in Nederland woonachtige Nederlander\\nArtikel 11, sectie 1 IN Participatiewet\\n\\nNOT belanghebbende is een hier te lande woonachtige vreemdeling die rechtmatig in Nederland verblijf houdt in de zin van artikel 8, onderdelen a tot en met e en l, van de Vreemdelingenwet 2000\\nArtikel 11, sectie 2 IN Participatiewet\\n\\n\\nbelanghebbende aan wie rechtens zijn vrijheid is ontnomen\\nArtikel 13, sectie 1 a IN Participatiewet\\n\\nbelanghebbende die zich onttrekt aan de tenuitvoerlegging van een vrijheidsstraf of vrijheidsbenemende maatregel\\nArtikel 13, sectie 1 b IN Participatiewet\\n\\nbelanghebbende die zijn militaire of vervangende dienstplicht vervult\\nArtikel 13, sectie 1 c IN Participatiewet\\n\\nbelanghebbende die wegens werkstaking of uitsluiting niet deelneemt aan de arbeid, voorzover diens gebrek aan middelen daarvan het gevolg is\\nArtikel 13, sectie 1 d IN Participatiewet\\n\\nbelanghebbende die per kalenderjaar langer dan vier weken verblijf houdt buiten Nederland\\nArtikel 13, sectie 1 e IN Participatiewet\\n\\nbelanghebbende die bijstand vraagt ter gedeeltelijke of volledige aflossing van een schuldenlast\\nArtikel 13, sectie 1 g IN Participatiewet\\n\\nbelanghebbende die overigens bij het ontstaan van de schuldenlast dan wel nadien, beschikte of beschikt over de middelen om in de noodzakelijke kosten van het bestaan te voorzien\\nArtikel 13, sectie 1 g IN Participatiewet\\n\\nbelanghebbende die een uitreiziger is\\nArtikel 13, sectie 1 h IN Participatiewet\\n\\nbelanghebbende van 18, 19 of 20 jaar die in een inrichting verblijft\\nArtikel 13, sectie 2 a IN Participatiewet\\n\\nbelanghebbende die jonger is dan 27 jaar en uit ’s Rijks kas bekostigd onderwijs kan volgen\\nArtikel 13, sectie 2 c IN Participatiewet\\n\\nbelanghebbende heeft in verband met volgen onderwijs aanspraak op studiefinanciering op grond van de Wet studiefinanciering 2000\\nArtikel 13, sectie 2 c 1 IN Participatiewet\\n\\nbelanghebbende heeft geen aanspraak op studiefinanciering omdat hij onderwijs dat hij kan volgen onderwijs, niet volgt\\nArtikel 13, sectie 2 c 2 IN Participatiewet\\n\\nbelanghebbende die jonger is dan 27 jaar en uit wiens houding en gedragingen ondubbelzinnig blijkt dat hij de verplichtingen, bedoeld in artikel 9, eerste lid, of artikel 55 niet wil nakomen\\nArtikel 13, sectie 2 d IN Participatiewet\\n\\nbijstand wordt toegekend voor de dag waarop de belanghebbende zich heeft gemeld om bijstand aan te vragen\\nArtikel 44, sectie 1 IN Participatiewet\\n\\nNOT het in aanmerking te nemen inkomen lager is dan de bijstandsnorm\\nArtikel 19 sectie 1 a IN Participatiewet\\n\\ner is in aanmerking te nemen vermogen\\nArtikel 19 sectie 1 b IN Participatiewet\\n\\n']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create list of column names\n",
    "new_cols = [f'response_new_model_{j+1}' for j in range(NUM_RESPONSES_EVAL)]\n",
    "base_cols = [f'response_base_model_{j+1}' for j in range(NUM_RESPONSES_EVAL)]\n",
    "\n",
    "# Select columns and convert to list of lists (rows)\n",
    "candidates_new = df[new_cols].values.tolist()\n",
    "candidates_base = df[base_cols].values.tolist()\n",
    "\n",
    "precon_text_list = df['precondition_texts'].to_list()\n",
    "precon_pos_list = df[\"precondition_positions\"].to_list()\n",
    "\n",
    "references = []\n",
    "for dict1, dict2 in zip(precon_text_list, precon_pos_list):\n",
    "    dict1 = ast.literal_eval(dict1)\n",
    "    dict2 = ast.literal_eval(dict2)\n",
    "    combined = []\n",
    "    for key in dict1.keys():  # or use sorted(dict1.keys()) if key order isn't guaranteed\n",
    "        combined.append(str(dict1[key]) + '\\n')\n",
    "        combined.append(str(dict2[key]) + '\\n\\n')\n",
    "    references.append(''.join(combined))\n",
    "\n",
    "print(references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69cdaded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the candidate lists\n",
    "candidates_new_flat = [resp for row in candidates_new for resp in row]\n",
    "candidates_base_flat = [resp for row in candidates_base for resp in row]\n",
    "\n",
    "# Repeat each reference NUM_RESPONSES times to match the flattened predictions\n",
    "references_flat = [ref for ref in references for _ in range(NUM_RESPONSES_EVAL)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1edeb3",
   "metadata": {},
   "source": [
    "## ROUGE/BLEU on relevant sequence from the answer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8b32cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results new: {'rouge1': np.float64(0.2702238591440755), 'rouge2': np.float64(0.08079582354168015), 'rougeL': np.float64(0.19583391721252524), 'rougeLsum': np.float64(0.25952731659008055)}\n",
      "Results base: {'rouge1': np.float64(0.23873150764732406), 'rouge2': np.float64(0.09433176706032653), 'rougeL': np.float64(0.152806037841491), 'rougeLsum': np.float64(0.2264665626435638)}\n"
     ]
    }
   ],
   "source": [
    "# Evaluation metric 1: ROUGE on relevant sequences\n",
    "# WHy not BLEU --> penalizes missing ngrams, not something I m looking for here\n",
    "\n",
    "rouge = evaluate.load('rouge')\n",
    "\n",
    "\n",
    "results_new = rouge.compute(predictions=candidates_new_flat, references=references_flat)\n",
    "results_base = rouge.compute(predictions=candidates_base_flat, references=references_flat)\n",
    "\n",
    "\n",
    "print(f\"Results new: {results_new}\")\n",
    "print(f\"Results base: {results_base}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db770b90",
   "metadata": {},
   "source": [
    "## BERT Score --> based on embedding similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5360138",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# Maybe add BERTScore --> semantic similarity based on sentencetransformer\n",
    "P_new, R_new, F1_new = score(\n",
    "    candidates_new_flat, \n",
    "    references_flat, \n",
    "    model_type='answerdotai/ModernBERT-base', \n",
    "    num_layers=22,\n",
    "    lang='nl')\n",
    "\n",
    "P_base, R_base, F1_base = score(\n",
    "    candidates_base_flat, \n",
    "    references_flat, \n",
    "    model_type='answerdotai/ModernBERT-base', \n",
    "    num_layers=22,\n",
    "    lang='nl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "afdb33c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Score metrics new: (tensor([0.8326, 0.8375, 0.8130, 0.8244, 0.7785, 0.8020, 0.8106, 0.8212, 0.8264,\n",
      "        0.8142, 0.7983, 0.7946, 0.8134, 0.8024, 0.7708, 0.8099, 0.8149, 0.7864,\n",
      "        0.7789, 0.7603, 0.7087, 0.7537, 0.7002, 0.8246, 0.6824, 0.6269, 0.7587,\n",
      "        0.7403, 0.6926, 0.7183, 0.8008, 0.8092, 0.7641, 0.7999, 0.7648, 0.7609,\n",
      "        0.7746, 0.8002, 0.7682, 0.7822]), tensor([0.8096, 0.8064, 0.8182, 0.8040, 0.7976, 0.7822, 0.8075, 0.8123, 0.7996,\n",
      "        0.8117, 0.8598, 0.8527, 0.8541, 0.8567, 0.8374, 0.8513, 0.8310, 0.8402,\n",
      "        0.8516, 0.8377, 0.8530, 0.8259, 0.8119, 0.8758, 0.7947, 0.7866, 0.8298,\n",
      "        0.8683, 0.7876, 0.8056, 0.8115, 0.7494, 0.7933, 0.7474, 0.8008, 0.7892,\n",
      "        0.8143, 0.7981, 0.8053, 0.8137]), tensor([0.8210, 0.8217, 0.8156, 0.8141, 0.7880, 0.7920, 0.8091, 0.8167, 0.8128,\n",
      "        0.8130, 0.8279, 0.8226, 0.8332, 0.8287, 0.8027, 0.8301, 0.8229, 0.8124,\n",
      "        0.8136, 0.7971, 0.7742, 0.7882, 0.7519, 0.8494, 0.7342, 0.6977, 0.7927,\n",
      "        0.7992, 0.7371, 0.7595, 0.8061, 0.7781, 0.7784, 0.7728, 0.7824, 0.7748,\n",
      "        0.7939, 0.7992, 0.7864, 0.7976]))\n",
      "BERT Score metrics base: (tensor([0.8501, 0.8197, 0.7641, 0.7790, 0.7558, 0.7669, 0.8009, 0.7329, 0.7446,\n",
      "        0.8135, 0.7276, 0.7391, 0.8106, 0.7749, 0.7377, 0.7700, 0.7361, 0.7395,\n",
      "        0.7704, 0.7709, 0.6999, 0.6580, 0.7010, 0.6989, 0.6437, 0.6920, 0.6958,\n",
      "        0.7061, 0.6458, 0.7324, 0.8147, 0.8229, 0.8138, 0.7842, 0.8103, 0.8139,\n",
      "        0.7886, 0.7994, 0.8072, 0.8253]), tensor([0.8838, 0.8797, 0.7953, 0.8174, 0.7865, 0.7870, 0.8078, 0.8147, 0.8016,\n",
      "        0.8520, 0.8380, 0.8389, 0.8412, 0.8461, 0.8375, 0.8451, 0.8419, 0.8462,\n",
      "        0.8496, 0.8425, 0.8030, 0.7954, 0.7986, 0.7994, 0.7816, 0.7878, 0.8090,\n",
      "        0.8100, 0.8043, 0.8470, 0.8185, 0.8118, 0.7923, 0.8048, 0.8255, 0.8238,\n",
      "        0.6940, 0.8227, 0.8224, 0.8072]), tensor([0.8667, 0.8486, 0.7794, 0.7977, 0.7708, 0.7768, 0.8044, 0.7716, 0.7721,\n",
      "        0.8323, 0.7790, 0.7858, 0.8256, 0.8089, 0.7844, 0.8058, 0.7855, 0.7893,\n",
      "        0.8080, 0.8051, 0.7479, 0.7202, 0.7466, 0.7458, 0.7060, 0.7368, 0.7482,\n",
      "        0.7545, 0.7164, 0.7855, 0.8166, 0.8173, 0.8029, 0.7944, 0.8178, 0.8189,\n",
      "        0.7383, 0.8109, 0.8147, 0.8161]))\n",
      "F1 base: 0.7863398790359497, F1 new: 0.7962180376052856\n"
     ]
    }
   ],
   "source": [
    "print(f\"BERT Score metrics new: {P_new, R_new, F1_new}\")\n",
    "print(f\"BERT Score metrics base: {P_base, R_base, F1_base}\")\n",
    "\n",
    "print(f\"F1 base: {F1_base.mean()}, F1 new: {F1_new.mean()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78332f58",
   "metadata": {},
   "source": [
    "## G-Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02388cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation metric 2: Evaluate whole answer on G-Eval\n",
    "\n",
    "prompts = df['prompt'].tolist()\n",
    "\n",
    "# setup variables for the Azure OpenAI API\n",
    "endpoint = \"https://openai-ds-instance-sweden.openai.azure.com/\"\n",
    "model_name = \"gpt-4.1\"\n",
    "deployment = \"deze-voor-alles\"\n",
    "\n",
    "\n",
    "\n",
    "subscription_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "api_version = \"2024-12-01-preview\"\n",
    "\n",
    "\n",
    "\n",
    "azure_model = AzureOpenAIModel(\n",
    "    openai_api_version=api_version,\n",
    "    azure_endpoint=endpoint,\n",
    "    api_key=subscription_key,\n",
    "    deployment_name=deployment\n",
    ")\n",
    "\n",
    "\n",
    "criteria = \"\"\"\n",
    "\n",
    "\n",
    "Evalueer hoe goed een taalmodel presteerde in de taak van voorwaarde-extractie uit Nederlandse juridische teksten.\n",
    "\n",
    "Voor elke lijst van precondities/subfacts werd de act/fact aan een taalmodel gegeven als onderdeel van een prompt, met de opdracht om alle bijbehorende subfact/preconditie(s) en hun respectieve positie(s) in de tekst terug te geven.\n",
    "\n",
    "Uw taak is om per paar te evalueren hoe goed het model presteerde op twee punten (op een 4-punt Likert-schaal):\n",
    "\n",
    "1. **Het vinden van alle relevante precondities in de tekst**\n",
    "2. **Hoe duidelijk de positie in de tekst is die het model aanduidde**\n",
    "\n",
    "Voor het modelantwoord krijgt u het hele antwoord voor een act/fact (die dus meredere precondities/subfacts kan beïnhouden) en moet u voor elke ground truth preconditie afzonderlijk evalueren of deze \n",
    "    a) aanwezig is in het antwoord en \n",
    "    b) of de positie ervan ook goed is aangegeven in het antwoord. \n",
    "\n",
    "\n",
    "\"\"\" #TODO: think about criteria...\n",
    "\n",
    "precondition_extraction_metric = GEval(\n",
    "    name=\"Precondition_Extraction\",\n",
    "    criteria=criteria,\n",
    "    evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT, LLMTestCaseParams.EXPECTED_OUTPUT],\n",
    "    model=azure_model,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffd8ea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/jacques.furst/miniconda3/envs/Evaluation/lib/python3.9/site-packages/rich/live.py:231: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/jacques.furst/miniconda3/envs/Evaluation/lib/python3.9/site-packages/rich/live.py:231: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Model Scores: [0.19770226320134773, 0.05, 0.2578758973185694, 0.37276161622778764]\n",
      "Base Model Scores: [0.27130695079342926, 0.002297737237712758, 0.3088617474797687, 0.2681893797918597]\n"
     ]
    }
   ],
   "source": [
    "#TODO: the flat version does not make tons of sense at the moment, double-check\n",
    "\n",
    "# Store results\n",
    "scores_new, reasons_new = [], []\n",
    "scores_base, reasons_base = [], []\n",
    "\n",
    "# Evaluate each prompt individually\n",
    "for prompt, new_out, base_out, ref in zip(prompts, candidates_new_flat, candidates_base_flat, references_flat):\n",
    " \n",
    "    # New model evaluation\n",
    "    g_eval_new = LLMTestCase(\n",
    "        input=prompt,\n",
    "        actual_output=f\"Extracted Preconditions: {new_out}\",\n",
    "        expected_output=f\"Expected Preconditions: {ref}\"\n",
    "    )\n",
    "    precondition_extraction_metric.measure(g_eval_new)\n",
    "    scores_new.append(precondition_extraction_metric.score)\n",
    "    reasons_new.append(precondition_extraction_metric.reason)\n",
    "\n",
    "    # Base model evaluation\n",
    "    g_eval_base = LLMTestCase(\n",
    "        input=prompt,\n",
    "        actual_output=f\"Extracted Preconditions: {base_out}\",\n",
    "        expected_output=f\"Expected Preconditions: {ref}\"\n",
    "    )   \n",
    "    precondition_extraction_metric.measure(g_eval_base)\n",
    "    scores_base.append(precondition_extraction_metric.score)\n",
    "    reasons_base.append(precondition_extraction_metric.reason)\n",
    "\n",
    "# Optionally: print or analyze results\n",
    "print(\"New Model Scores:\", scores_new)\n",
    "print(\"Base Model Scores:\", scores_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fbd333f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Model Scores average: 0.2195849441869262\n",
      "Base Model Scores average: 0.2126639538256926\n",
      "['De drie verwachte subfacts zijn inhoudelijk herkend in het modelantwoord, maar de posities zijn volledig onjuist: alle subfacts worden geplaatst in Artikel 4.3, vierde lid van de Rijksbegroting 2016, terwijl ze volgens de Expected Output in Artikel 2.23, sectie 4a-c van de Comptabiliteitswet 2016 horen. De volledigheid van preconditie-detectie is redelijk, maar de nauwkeurigheid van de positie-aanduiding is zeer slecht.', 'Geen van de verwachte precondities uit de Expected Output zijn aanwezig in het Actual Output. De precondities in het Actual Output verwijzen naar Artikel 4.4 van de Comptabiliteitswet 2016, terwijl de Expected Output precondities uit Artikel 2.23, sectie 4a-c van dezelfde wet verwacht. Zowel de inhoud van de precondities als de positie-aanduidingen komen niet overeen met de test case parameters. Er is dus geen volledigheid of nauwkeurigheid in preconditie-detectie of positie-aanduiding.', 'De modeloutput noemt de drie verwachte precondities (budgettaire totaalbeeld, budgettaire beschouwingen, overzicht uitgaven en ontvangsten), maar de posities zijn onjuist: alle worden aan Artikel 4.4 gekoppeld, terwijl ze volgens de Expected Output in Artikel 2.23 lid 4a-c van de Comptabiliteitswet 2016 staan. De preconditie-detectie is volledig, maar de nauwkeurigheid van de positie-aanduiding is laag.', \"Het modelantwoord detecteert alle drie de verwachte precondities inhoudelijk, maar benoemt ze als 'subfact' in plaats van 'preconditie'. De inhoud van de precondities komt grotendeels overeen met de Expected Output. Echter, de positie-aanduidingen zijn onjuist: het model noemt 'Artikel 4.3.4.(b), sectie 4, IN Rijksbegrotingwet 2016', terwijl de juiste positie 'Artikel 2.23 sectie 4a/b/c IN Comptabiliteitswet 2016' is. Hierdoor is de volledigheid van preconditie-detectie voldoende, maar de nauwkeurigheid van de positie-aanduiding onvoldoende.\"]\n",
      "[\"Alle drie de verwachte subfacts zijn inhoudelijk herkend in het modelantwoord, maar de posities zijn onjuist: het model noemt 'Artikel 4.4.4, sectie 1 IN Rijksbegroting 2016' in plaats van 'Artikel 2.23 sectie 4a/b/c IN Comptabiliteitswet 2016'. De inhoud van de subfacts is grotendeels correct, maar de aanduiding van de positie is onduidelijk en onnauwkeurig. Dit leidt tot een lage score voor nauwkeurigheid van de positie-aanduiding, ondanks volledige detectie van de precondities.\", 'Geen van de verwachte precondities uit de Comptabiliteitswet 2016 zijn aanwezig in het modelantwoord. In plaats daarvan worden irrelevante precondities uit de Rijksbegroting 2016 genoemd, die niet overeenkomen met de gevraagde act of de verwachte posities. Zowel de volledigheid van preconditie-detectie als de nauwkeurigheid van de positie-aanduiding zijn volledig afwezig.', 'De modeloutput detecteert alle drie de verwachte precondities inhoudelijk (budgettaire totaalbeeld, budgettaire beschouwingen, overzicht uitgaven/ontvangsten), maar de positieaanduidingen zijn onjuist: de output noemt Artikel 4.4.4 in de Rijksbegrotingsvoorschriften 2024, terwijl de correcte posities Artikel 2.23, sectie 4a-c in de Comptabiliteitswet 2016 zijn. Dit betekent volledige preconditie-detectie, maar onjuiste en onduidelijke positieaanduiding, wat resulteert in een lage totaalscore.', 'De modeloutput detecteert de drie verwachte precondities inhoudelijk, maar benoemt ze als subfacts en geeft de inhoud niet altijd exact weer zoals in de wetstekst. De posities zijn onjuist: de output verwijst naar Artikel 4.4.4 in de Rijksbegrotingsvoorschriften 2024, terwijl de verwachte posities Artikel 2.23 sectie 4a-c in de Comptabiliteitswet 2016 zijn. De volledigheid van preconditie-detectie is redelijk, maar de nauwkeurigheid van de positie-aanduiding is onvoldoende.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geval_scores_new_average = sum(scores_new) / len(scores_new)\n",
    "geval_scores_base_average = sum(scores_base) / len(scores_base)\n",
    "\n",
    "print(\"New Model Scores average:\", geval_scores_new_average)\n",
    "print(\"Base Model Scores average:\", geval_scores_base_average)\n",
    "\n",
    "\n",
    "print(reasons_new)\n",
    "print(reasons_base)\n",
    "\n",
    "len(reasons_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a98bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ############ OLD VERSION ######################\n",
    "\n",
    "# # Now define your test case, actual_output is your LLM output\n",
    "# g_eval_new = LLMTestCase(input=prompts, actual_output=f\"Extracetd Preconditions: {candidates_new}\", expected_output=f\"Expected preconditions: {references}\")\n",
    "# g_eval_base = LLMTestCase(input=prompts, actual_output=f\"Extracetd Preconditions: {candidates_base}\", expected_output=f\"Expected preconditions: {references}\")\n",
    "\n",
    "\n",
    "# # Use G-Eval metric\n",
    "# precondition_extraction_metric.measure(g_eval_new)\n",
    "# print(\"Score:\", precondition_extraction_metric.score)\n",
    "# print(\"Reason:\", precondition_extraction_metric.reason)\n",
    "\n",
    "# precondition_extraction_metric.measure(g_eval_base)\n",
    "# print(\"Score:\", precondition_extraction_metric.score)\n",
    "# print(\"Reason:\", precondition_extraction_metric.reason)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6086f2",
   "metadata": {},
   "source": [
    "## Personal evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34ef6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate about 10 answers with both models --> load them into the user interface and get reward score\n",
    "# Also maybe do qualitative evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Evaluation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
