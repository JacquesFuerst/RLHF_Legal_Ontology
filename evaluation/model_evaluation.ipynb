{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e99baab",
   "metadata": {},
   "source": [
    "# Evaluate finetuned Mistral Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d0599d",
   "metadata": {},
   "source": [
    "## Imports and environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7235bb20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacques.furst/miniconda3/envs/Evaluation/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "CUDA is available. Using GPU: NVIDIA L40S\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "import ast\n",
    "import evaluate\n",
    "\n",
    "from deepeval.metrics import GEval\n",
    "from deepeval.test_case import LLMTestCaseParams, LLMTestCase\n",
    "\n",
    "from distutils.util import strtobool\n",
    "\n",
    "# from deepeval.models.azure_openai import AzureOpenAIModel\n",
    "\n",
    "from peft import PeftModel\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, AutoModelForCausalLM\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Add the parent directory to the Python path\n",
    "# __file__\n",
    "\n",
    "\n",
    "# Adjust this path to point to the directory containing rl_training_new\n",
    "module_path = os.path.abspath(os.path.join('..')) # or another relative path\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from rl_training_new.utils import find_best_window\n",
    "\n",
    "\n",
    "from bert_score import score\n",
    "\n",
    "# load the relevant devices available on the server\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = os.getenv(\"AVAILABLE_DEVICES\")\n",
    "\n",
    "# Enable expandable CUDA segments\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# load cuda\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print(\"CUDA is available. Using GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA is not available. Using CPU.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d1260ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_bool(s):\n",
    "    return bool(strtobool(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329c4a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = os.getenv(\"GENERATION_MODEL_NAME\")\n",
    "ALGORITHM = os.getenv(\"EVAL_MODEL_ALGORITHM\")\n",
    "RL_TRAINED_ADAPTERS = os.getenv(\"EVAL_MODEL_FOLDER\")\n",
    "EVAL_ANSWERS_CSV = os.getenv(\"EVAL_ANSWERS_CSV\")\n",
    "GENERATE_RESPONSES = str_to_bool(os.getenv(\"GENERATE_RESPONSES\"))\n",
    "OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "# RL_DATA_PATH = os.getenv(\"RL_DATA_PATH\")\n",
    "EVAL_FILE = os.getenv(\"EVAL_FILE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db917af8",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2ca7f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:10<00:00,  3.52s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:10<00:00,  3.41s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): MistralForCausalLM(\n",
       "      (model): MistralModel(\n",
       "        (embed_tokens): Embedding(32768, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x MistralDecoderLayer(\n",
       "            (self_attn): MistralAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (mlp): MistralMLP(\n",
       "              (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "            (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "        (rotary_emb): MistralRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=32768, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# base_model = AutoModelForCausalLM.from_pretrained(MODEL)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(MODEL,  \n",
    "                                            #  device_map=\"auto\",  # For GPU/TPU acceleration\n",
    "                                            device_map=None,\n",
    "                                            torch_dtype=torch.bfloat16,\n",
    "                                            #  load_in_4bit=True,\n",
    "                                            quantization_config={\n",
    "                                                \"load_in_4bit\": True,\n",
    "                                                \"bnb_4bit_compute_dtype\": torch.bfloat16,\n",
    "                                                \"bnb_4bit_use_double_quant\": True,\n",
    "                                                \"bnb_4bit_quant_type\": \"nf4\"\n",
    "                                                }\n",
    "                                            )   # Optimize precision)\n",
    "# base_model_new = AutoModelForCausalLM.from_pretrained(MODEL)\n",
    "base_model_new = AutoModelForCausalLM.from_pretrained(MODEL,  \n",
    "                                            #  device_map=\"auto\",  # For GPU/TPU acceleration\n",
    "                                            device_map=None,\n",
    "                                            torch_dtype=torch.bfloat16,\n",
    "                                            #  load_in_4bit=True,\n",
    "                                            quantization_config={\n",
    "                                                \"load_in_4bit\": True,\n",
    "                                                \"bnb_4bit_compute_dtype\": torch.bfloat16,\n",
    "                                                \"bnb_4bit_use_double_quant\": True,\n",
    "                                                \"bnb_4bit_quant_type\": \"nf4\"\n",
    "                                                }\n",
    "                                            )   # Optimize precision)\n",
    "new_model = PeftModel.from_pretrained(base_model_new, RL_TRAINED_ADAPTERS)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "base_model.eval()\n",
    "new_model.eval()\n",
    "\n",
    "base_model.to(device)\n",
    "new_model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7ace9b",
   "metadata": {},
   "source": [
    "## Get test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a729e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv(EVAL_FILE, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afccf5c",
   "metadata": {},
   "source": [
    "## Generate Model responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab9ff172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate response\n",
    "def generate_response(prompt, tokenizer, model, max_length=1024):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    input_length = inputs['input_ids'].shape[1]\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_length=input_length + max_length, do_sample=True, top_k=50)\n",
    "        generated_ids = outputs[0][input_length:]\n",
    "    return tokenizer.decode(generated_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fe4e660",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              prompt  \\\n",
      "0  \\n\\n\\n                --- Definitie ---\\n\\n   ...   \n",
      "1  \\n\\n\\n                --- Definitie ---\\n\\n   ...   \n",
      "2  \\n\\n\\n                --- Definitie ---\\n\\n   ...   \n",
      "3  \\n\\n\\n                --- Definitie ---\\n\\n   ...   \n",
      "\n",
      "                                  response_new_model  \\\n",
      "0  ----------------------------------------------...   \n",
      "1  ---------------------------------------\\n\\n   ...   \n",
      "2   Preconditie: Ministers stellen voor de aanvan...   \n",
      "3   Preconditie: Wij hebben recht op de bijstand....   \n",
      "\n",
      "                                 response_base_model  \n",
      "0  Inhoud: <inhoud>\\n\\n\\n                Subfact:...  \n",
      "1  1. De vreemdeling moet beschikken over een gel...  \n",
      "2  1. Preconditie: De ministers moeten een begrot...  \n",
      "3   Het geld dat de minister voor de algemene bij...  \n"
     ]
    }
   ],
   "source": [
    "if GENERATE_RESPONSES:\n",
    "    # Apply both models\n",
    "    df['response_base_model'] = df['prompt'].apply(lambda x: generate_response(x, tokenizer, base_model))\n",
    "    df['response_new_model'] = df['prompt'].apply(lambda x: generate_response(x, tokenizer, new_model))\n",
    "\n",
    "    # Show result\n",
    "    print(df[['prompt', 'response_new_model', 'response_base_model']])\n",
    "\n",
    "    # store response df in csv\n",
    "    df.to_csv(EVAL_ANSWERS_CSV, index=False, sep=';')\n",
    "else:\n",
    "    pd.read_csv(EVAL_ANSWERS_CSV, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0658429a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inhoud: <inhoud>\n",
      "\n",
      "\n",
      "                Subfact: het budgettaire totaalbeeld\n",
      "                  Positie: Artikel 2.24, sectie 4, van de Rijksbegrotingsvoorschriften 2024 \n",
      "                  Inhoud: De miljoenennota bevat in elk geval het budgettaire totaalbeeld voor het betrokken begrotingsjaar en de vier daaropvolgende jaren van de rijksbegroting en de niet tot de rijksbegroting behorende budgetdisciplinesectoren.\n",
      "\n",
      "                Subfact: de budgettaire beschouwingen\n",
      "                  Positie: Artikel 2.24, sectie 4, van de Rijksbegrotingsvoorschriften 2024\n",
      "                  Inhoud: De miljoenennota bevat in elk geval de budgettaire beschouwingen over het voorgenomen beleid voor de collectieve sector.\n",
      "\n",
      "                Subfact: een overzicht van de uitgaven en ontvangsten\n",
      "                  Positie: Artikel 2.24, sectie 4, van de Rijksbegrotingsvoorschriften 2024\n",
      "                  Inhoud: De miljoenennota bevat in elk geval een overzicht van de uitgaven en de ontvangsten in de begrotingen voor het begrotingsjaar en de vier daarop aansluitende jaren.\n",
      "------------------------------------------------------------\n",
      "\n",
      "                 Subfact: budgettaire totaalbeeld \n",
      "                 Position: bij het woord ``budgettaire'' in Artikel 2.4.4\n",
      "                 Source text: Hoofdstuk 4. Begrotingsbeheer en financieel beheer: verantwoordelijkheden\n",
      "                 Wet: Rijksbegroting 2016\n",
      "\n",
      "                 ------------------------------------------------------------\n",
      "\n",
      "                 Subfact: budgettaire beschouwingen\n",
      "                 Position: bij de hoofdletter B in het woord ``budgettaire'' in Artikel 2.4.4\n",
      "                 Source text: Hoofdstuk 4. Begrotingsbeheer en financieel beheer: verantwoordelijkheden\n",
      "                 Wet: Rijksbegroting 2016\n",
      "\n",
      "                 ------------------------------------------------------------\n",
      "\n",
      "                 Subfact: een overzicht van de uitgaven en ontvangsten\n",
      "                 Position: bij het woord ``overzicht'' in Artikel 2.4.4\n",
      "                 Source text: Hoofdstuk 4. Begrotingsbeheer en financieel beheer: verantwoordelijkheden\n",
      "                 Wet: Rijksbegroting 2016\n"
     ]
    }
   ],
   "source": [
    "print(df['response_base_model'][0])\n",
    "print(df['response_new_model'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0cb0c503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['een overzicht van de uitgaven en de ontvangsten in de begrotingen voor het begrotingsjaar en de vier daarop aansluitende jaren.\\nArtikel 2.23 sectie 4c IN Comptabiliteitswet 2016\\n\\nde budgettaire beschouwingen over het voorgenomen beleid voor de collectieve sector\\nArtikel 2.23 sectie 4b IN Comptabiliteitswet 2016\\n\\nhet budgettaire totaalbeeld voor het betrokken begrotingsjaar en de vier daaropvolgende jaren van de rijksbegroting en de niet tot de rijksbegroting behorende budgetdisciplinesectoren\\nArtikel 2.23 sectie 4a IN Comptabiliteitswet 2016\\n\\n', 'de verblijfsvergunning wordt verleend met ingang van de dag waarop de vreemdeling heeft aangetoond dat hij aan alle voorwaarden voldoet\\nArtikel 26, sectie 1 IN Vreemdelingenwet 2024\\n\\nde verblijfsvergunning wordt verleend met ingang van een dag eerder dan de dag waarop de aanvraag is ontvangen\\nArtikel 26, sectie 1 IN Vreemdelingenwet 2024\\n\\n', 'Begroting bevat begrotingsstaat\\nArtikel 2.2 IN Comptabiliteitswet 2016\\n\\n', 'college stelt het recht op bijstand vast op schriftelijke aanvraag\\nArtikel 43, sectie 1 IN Participatiewet\\n\\nNOT bijstand wordt door de echtgenoten gezamenlijk aangevraagd dan wel door een van hen met schriftelijke toestemming van de ander\\nArtikel 43, sectie 2 IN Participatiewet\\n\\nNOT belanghebbende is een in Nederland woonachtige Nederlander\\nArtikel 11, sectie 1 IN Participatiewet\\n\\nNOT belanghebbende is een hier te lande woonachtige vreemdeling die rechtmatig in Nederland verblijf houdt in de zin van artikel 8, onderdelen a tot en met e en l, van de Vreemdelingenwet 2000\\nArtikel 11, sectie 2 IN Participatiewet\\n\\n\\nbelanghebbende aan wie rechtens zijn vrijheid is ontnomen\\nArtikel 13, sectie 1 a IN Participatiewet\\n\\nbelanghebbende die zich onttrekt aan de tenuitvoerlegging van een vrijheidsstraf of vrijheidsbenemende maatregel\\nArtikel 13, sectie 1 b IN Participatiewet\\n\\nbelanghebbende die zijn militaire of vervangende dienstplicht vervult\\nArtikel 13, sectie 1 c IN Participatiewet\\n\\nbelanghebbende die wegens werkstaking of uitsluiting niet deelneemt aan de arbeid, voorzover diens gebrek aan middelen daarvan het gevolg is\\nArtikel 13, sectie 1 d IN Participatiewet\\n\\nbelanghebbende die per kalenderjaar langer dan vier weken verblijf houdt buiten Nederland\\nArtikel 13, sectie 1 e IN Participatiewet\\n\\nbelanghebbende die bijstand vraagt ter gedeeltelijke of volledige aflossing van een schuldenlast\\nArtikel 13, sectie 1 g IN Participatiewet\\n\\nbelanghebbende die overigens bij het ontstaan van de schuldenlast dan wel nadien, beschikte of beschikt over de middelen om in de noodzakelijke kosten van het bestaan te voorzien\\nArtikel 13, sectie 1 g IN Participatiewet\\n\\nbelanghebbende die een uitreiziger is\\nArtikel 13, sectie 1 h IN Participatiewet\\n\\nbelanghebbende van 18, 19 of 20 jaar die in een inrichting verblijft\\nArtikel 13, sectie 2 a IN Participatiewet\\n\\nbelanghebbende die jonger is dan 27 jaar en uit ’s Rijks kas bekostigd onderwijs kan volgen\\nArtikel 13, sectie 2 c IN Participatiewet\\n\\nbelanghebbende heeft in verband met volgen onderwijs aanspraak op studiefinanciering op grond van de Wet studiefinanciering 2000\\nArtikel 13, sectie 2 c 1 IN Participatiewet\\n\\nbelanghebbende heeft geen aanspraak op studiefinanciering omdat hij onderwijs dat hij kan volgen onderwijs, niet volgt\\nArtikel 13, sectie 2 c 2 IN Participatiewet\\n\\nbelanghebbende die jonger is dan 27 jaar en uit wiens houding en gedragingen ondubbelzinnig blijkt dat hij de verplichtingen, bedoeld in artikel 9, eerste lid, of artikel 55 niet wil nakomen\\nArtikel 13, sectie 2 d IN Participatiewet\\n\\nbijstand wordt toegekend voor de dag waarop de belanghebbende zich heeft gemeld om bijstand aan te vragen\\nArtikel 44, sectie 1 IN Participatiewet\\n\\nNOT het in aanmerking te nemen inkomen lager is dan de bijstandsnorm\\nArtikel 19 sectie 1 a IN Participatiewet\\n\\ner is in aanmerking te nemen vermogen\\nArtikel 19 sectie 1 b IN Participatiewet\\n\\n']\n"
     ]
    }
   ],
   "source": [
    "candidates_new = df['response_new_model'].tolist()\n",
    "candidates_base = df['response_base_model'].tolist()\n",
    "\n",
    "precon_text_list = df['precondition_texts'].to_list()\n",
    "precon_pos_list = df[\"precondition_positions\"].to_list()\n",
    "\n",
    "references = []\n",
    "for dict1, dict2 in zip(precon_text_list, precon_pos_list):\n",
    "    dict1 = ast.literal_eval(dict1)\n",
    "    dict2 = ast.literal_eval(dict2)\n",
    "    combined = []\n",
    "    for key in dict1.keys():  # or use sorted(dict1.keys()) if key order isn't guaranteed\n",
    "        combined.append(str(dict1[key]) + '\\n')\n",
    "        combined.append(str(dict2[key]) + '\\n\\n')\n",
    "    references.append(''.join(combined))\n",
    "\n",
    "print(references)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1edeb3",
   "metadata": {},
   "source": [
    "## ROUGE/BLEU on relevant sequence from the answer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8b32cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results new: {'rouge1': np.float64(0.2780076523299725), 'rouge2': np.float64(0.06619500340555914), 'rougeL': np.float64(0.18790881994033837), 'rougeLsum': np.float64(0.24725295891627647)}\n",
      "Results base: {'rouge1': np.float64(0.37068163443947116), 'rouge2': np.float64(0.1861653641071485), 'rougeL': np.float64(0.24803561200207852), 'rougeLsum': np.float64(0.36372823553283323)}\n"
     ]
    }
   ],
   "source": [
    "# Evaluation metric 1: ROUGE on relevant sequences\n",
    "# WHy not BLEU --> penalizes missing ngrams, not something I m looking for here\n",
    "\n",
    "rouge = evaluate.load('rouge')\n",
    "\n",
    "results_new = rouge.compute(predictions=candidates_new, references=references)\n",
    "results_base = rouge.compute(predictions=candidates_base, references=references)\n",
    "\n",
    "print(f\"Results new: {results_new}\")\n",
    "print(f\"Results base: {results_base}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db770b90",
   "metadata": {},
   "source": [
    "## BERT Score --> based on embedding similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5360138",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# Maybe add BERTScore --> semantic similarity based on sentencetransformer\n",
    "P_new, R_new, F1_new = score(\n",
    "    candidates_new, \n",
    "    references, \n",
    "    model_type='answerdotai/ModernBERT-base', \n",
    "    num_layers=22,\n",
    "    lang='nl')\n",
    "\n",
    "P_base, R_base, F1_base = score(\n",
    "    candidates_base, \n",
    "    references, \n",
    "    model_type='answerdotai/ModernBERT-base', \n",
    "    num_layers=22,\n",
    "    lang='nl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "afdb33c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Score metrics new: (tensor([0.7599, 0.8310, 0.6469, 0.7415]), tensor([0.8138, 0.8560, 0.7944, 0.7951]), tensor([0.7859, 0.8433, 0.7131, 0.7674]))\n",
      "BERT Score metrics base: (tensor([0.8214, 0.8130, 0.7563, 0.8311]), tensor([0.8711, 0.8471, 0.8500, 0.8002]), tensor([0.8455, 0.8297, 0.8004, 0.8153]))\n",
      "F1 base: 0.8227373361587524, F1 new: 0.7774291038513184\n"
     ]
    }
   ],
   "source": [
    "print(f\"BERT Score metrics new: {P_new, R_new, F1_new}\")\n",
    "print(f\"BERT Score metrics base: {P_base, R_base, F1_base}\")\n",
    "\n",
    "print(f\"F1 base: {F1_base.mean()}, F1 new: {F1_new.mean()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78332f58",
   "metadata": {},
   "source": [
    "## G-Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02388cd4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AzureOpenAIModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 17\u001b[0m\n\u001b[1;32m     12\u001b[0m subscription_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAZURE_OPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m api_version \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2024-12-01-preview\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 17\u001b[0m azure_model \u001b[38;5;241m=\u001b[39m \u001b[43mAzureOpenAIModel\u001b[49m(\n\u001b[1;32m     18\u001b[0m     api_version\u001b[38;5;241m=\u001b[39mapi_version,\n\u001b[1;32m     19\u001b[0m     azure_endpoint\u001b[38;5;241m=\u001b[39mendpoint,\n\u001b[1;32m     20\u001b[0m     api_key\u001b[38;5;241m=\u001b[39msubscription_key,\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     24\u001b[0m criteria \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     25\u001b[0m \n\u001b[1;32m     26\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     40\u001b[0m \n\u001b[1;32m     41\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m \u001b[38;5;66;03m#TODO: think about criteria...\u001b[39;00m\n\u001b[1;32m     43\u001b[0m precondition_extraction_metric \u001b[38;5;241m=\u001b[39m GEval(\n\u001b[1;32m     44\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrecondition_Extraction\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     45\u001b[0m     criteria\u001b[38;5;241m=\u001b[39mcriteria,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m \n\u001b[1;32m     49\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'AzureOpenAIModel' is not defined"
     ]
    }
   ],
   "source": [
    "# Evaluation metric 2: Evaluate whole answer on G-Eval\n",
    "\n",
    "prompts = df['prompt'].tolist()\n",
    "\n",
    "# setup variables for the Azure OpenAI API\n",
    "endpoint = \"https://openai-ds-instance-sweden.openai.azure.com/\"\n",
    "model_name = \"gpt-4.1\"\n",
    "deployment = \"deze-voor-alles\"\n",
    "\n",
    "\n",
    "\n",
    "subscription_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "api_version = \"2024-12-01-preview\"\n",
    "\n",
    "\n",
    "\n",
    "azure_model = AzureOpenAIModel(\n",
    "    api_version=api_version,\n",
    "    azure_endpoint=endpoint,\n",
    "    api_key=subscription_key,\n",
    ")\n",
    "\n",
    "\n",
    "criteria = \"\"\"\n",
    "\n",
    "\n",
    "Evalueer hoe goed een taalmodel presteerde in de taak van voorwaarde-extractie uit Nederlandse juridische teksten.\n",
    "\n",
    "Voor elke lijst van precondities/subfacts werd de act/fact aan een taalmodel gegeven als onderdeel van een prompt, met de opdracht om alle bijbehorende subfact/preconditie(s) en hun respectieve positie(s) in de tekst terug te geven.\n",
    "\n",
    "Uw taak is om per paar te evalueren hoe goed het model presteerde op twee punten (op een 4-punt Likert-schaal):\n",
    "\n",
    "1. **Het vinden van alle relevante precondities in de tekst**\n",
    "2. **Hoe duidelijk de positie in de tekst is die het model aanduidde**\n",
    "\n",
    "Voor het modelantwoord krijgt u het hele antwoord voor een act/fact (die dus meredere precondities/subfacts kan beïnhouden) en moet u voor elke ground truth preconditie afzonderlijk evalueren of deze \n",
    "    a) aanwezig is in het antwoord en \n",
    "    b) of de positie ervan ook goed is aangegeven in het antwoord. \n",
    "\n",
    "\n",
    "\"\"\" #TODO: think about criteria...\n",
    "\n",
    "precondition_extraction_metric = GEval(\n",
    "    name=\"Precondition_Extraction\",\n",
    "    criteria=criteria,\n",
    "    evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT, LLMTestCaseParams.EXPECTED_OUTPUT],\n",
    "    model=azure_model,\n",
    "\n",
    ")\n",
    "\n",
    "# Now define your test case, actual_output is your LLM output\n",
    "g_eval_new = LLMTestCase(input=prompts, actual_output=f\"Extracetd Preconditions: {candidates_new}\", expected_output=f\"Expected preconditions: {references}\")\n",
    "g_eval_base = LLMTestCase(input=prompts, actual_output=f\"Extracetd Preconditions: {candidates_base}\", expected_output=f\"Expected preconditions: {references}\")\n",
    "\n",
    "\n",
    "# Use G-Eval metric\n",
    "precondition_extraction_metric.measure(g_eval_new)\n",
    "print(\"Score:\", precondition_extraction_metric.score)\n",
    "print(\"Reason:\", precondition_extraction_metric.reason)\n",
    "\n",
    "precondition_extraction_metric.measure(g_eval_base)\n",
    "print(\"Score:\", precondition_extraction_metric.score)\n",
    "print(\"Reason:\", precondition_extraction_metric.reason)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1585bcb3",
   "metadata": {},
   "source": [
    "Run this in the proper environment tomorrow:\n",
    "\n",
    "https://deepeval.com/docs/metrics-introduction#using-azure-openai\n",
    "\n",
    "deepeval set-azure-openai \\\n",
    "    --openai-endpoint=<endpoint> \\ # e.g. https://example-resource.azure.openai.com/\n",
    "    --openai-api-key=<api_key> \\\n",
    "    --openai-model-name=<model_name> \\ # e.g. gpt-4o\n",
    "    --deployment-name=<deployment_name> \\  # e.g. Test Deployment\n",
    "    --openai-api-version=<openai_api_version> \\ # e.g. 2025-01-01-preview\n",
    "    --model-version=<model_version> # e.g. 2024-11-20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6086f2",
   "metadata": {},
   "source": [
    "## Personal evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34ef6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate about 10 answers with both models --> load them into the user interface and get reward score\n",
    "# Also maybe do qualitative evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Evaluation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
