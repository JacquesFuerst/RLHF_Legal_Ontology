{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e99baab",
   "metadata": {},
   "source": [
    "# Evaluate finetuned Mistral Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d0599d",
   "metadata": {},
   "source": [
    "## Imports and environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7235bb20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacques.furst/miniconda3/envs/Evaluation/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "CUDA is available. Using GPU: NVIDIA L40S\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "import ast\n",
    "import evaluate\n",
    "\n",
    "from deepeval.metrics import GEval\n",
    "from deepeval.test_case import LLMTestCaseParams, LLMTestCase\n",
    "\n",
    "from distutils.util import strtobool\n",
    "\n",
    "# from deepeval.models.azure_openai import AzureOpenAIModel\n",
    "\n",
    "from peft import PeftModel\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, AutoModelForCausalLM\n",
    "\n",
    "from deepeval.models import AzureOpenAIModel\n",
    "# from deepeval.metrics import AnswerRelevancyMetric\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Add the parent directory to the Python path\n",
    "# __file__\n",
    "\n",
    "\n",
    "# Adjust this path to point to the directory containing rl_training_new\n",
    "module_path = os.path.abspath(os.path.join('..')) # or another relative path\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from rl_training_new.utils import find_best_window\n",
    "\n",
    "\n",
    "from bert_score import score\n",
    "\n",
    "# load the relevant devices available on the server\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = os.getenv(\"AVAILABLE_DEVICES\")\n",
    "\n",
    "# Enable expandable CUDA segments\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# load cuda\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print(\"CUDA is available. Using GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA is not available. Using CPU.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d1260ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_bool(s):\n",
    "    return bool(strtobool(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "329c4a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = os.getenv(\"GENERATION_MODEL_NAME\")\n",
    "ALGORITHM = os.getenv(\"EVAL_MODEL_ALGORITHM\")\n",
    "RL_TRAINED_ADAPTERS = os.getenv(\"EVAL_MODEL_FOLDER\")\n",
    "EVAL_ANSWERS_CSV = os.getenv(\"EVAL_ANSWERS_CSV\")\n",
    "GENERATE_RESPONSES = str_to_bool(os.getenv(\"GENERATE_RESPONSES\"))\n",
    "OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "# RL_DATA_PATH = os.getenv(\"RL_DATA_PATH\")\n",
    "EVAL_FILE = os.getenv(\"EVAL_FILE\")\n",
    "NUM_RESPONSES_EVAL = int(os.getenv(\"NUM_RESPONSES_EVAL\"))  # Number of responses per model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db917af8",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2ca7f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:10<00:00,  3.50s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:10<00:00,  3.40s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): MistralForCausalLM(\n",
       "      (model): MistralModel(\n",
       "        (embed_tokens): Embedding(32768, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x MistralDecoderLayer(\n",
       "            (self_attn): MistralAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (mlp): MistralMLP(\n",
       "              (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "            (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "        (rotary_emb): MistralRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=32768, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# base_model = AutoModelForCausalLM.from_pretrained(MODEL)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(MODEL,  \n",
    "                                            #  device_map=\"auto\",  # For GPU/TPU acceleration\n",
    "                                            device_map=None,\n",
    "                                            torch_dtype=torch.bfloat16,\n",
    "                                            #  load_in_4bit=True,\n",
    "                                            quantization_config={\n",
    "                                                \"load_in_4bit\": True,\n",
    "                                                \"bnb_4bit_compute_dtype\": torch.bfloat16,\n",
    "                                                \"bnb_4bit_use_double_quant\": True,\n",
    "                                                \"bnb_4bit_quant_type\": \"nf4\"\n",
    "                                                }\n",
    "                                            )   # Optimize precision)\n",
    "# base_model_new = AutoModelForCausalLM.from_pretrained(MODEL)\n",
    "base_model_new = AutoModelForCausalLM.from_pretrained(MODEL,  \n",
    "                                            #  device_map=\"auto\",  # For GPU/TPU acceleration\n",
    "                                            device_map=None,\n",
    "                                            torch_dtype=torch.bfloat16,\n",
    "                                            #  load_in_4bit=True,\n",
    "                                            quantization_config={\n",
    "                                                \"load_in_4bit\": True,\n",
    "                                                \"bnb_4bit_compute_dtype\": torch.bfloat16,\n",
    "                                                \"bnb_4bit_use_double_quant\": True,\n",
    "                                                \"bnb_4bit_quant_type\": \"nf4\"\n",
    "                                                }\n",
    "                                            )   # Optimize precision)\n",
    "new_model = PeftModel.from_pretrained(base_model_new, RL_TRAINED_ADAPTERS)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "base_model.eval()\n",
    "new_model.eval()\n",
    "\n",
    "base_model.to(device)\n",
    "new_model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7ace9b",
   "metadata": {},
   "source": [
    "## Get test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a729e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(EVAL_FILE, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afccf5c",
   "metadata": {},
   "source": [
    "## Generate Model responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab9ff172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate response\n",
    "def generate_response(prompt, tokenizer, model, max_length=1024):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    input_length = inputs['input_ids'].shape[1]\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_length=input_length + max_length, do_sample=True, top_k=50)\n",
    "        generated_ids = outputs[0][input_length:]\n",
    "    return tokenizer.decode(generated_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fe4e660",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'NUM_RESPONSES' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m GENERATE_RESPONSES:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# Generate multiple responses for each prompt\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[43mNUM_RESPONSES\u001b[49m):\n\u001b[1;32m      4\u001b[0m         df[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresponse_base_model_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: generate_response(x, tokenizer, base_model))\n\u001b[1;32m      5\u001b[0m         df[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresponse_new_model_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: generate_response(x, tokenizer, new_model))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'NUM_RESPONSES' is not defined"
     ]
    }
   ],
   "source": [
    "if GENERATE_RESPONSES:\n",
    "    # Generate multiple responses for each prompt\n",
    "    for i in range(NUM_RESPONSES):\n",
    "        df[f'response_base_model_{i+1}'] = df['prompt'].apply(lambda x: generate_response(x, tokenizer, base_model))\n",
    "        df[f'response_new_model_{i+1}'] = df['prompt'].apply(lambda x: generate_response(x, tokenizer, new_model))\n",
    "\n",
    "    # Show result: displaying first response columns for brevity\n",
    "    response_cols = [f'response_new_model_{i+1}' for i in range(NUM_RESPONSES)] + \\\n",
    "                    [f'response_base_model_{i+1}' for i in range(NUM_RESPONSES)]\n",
    "    print(df[['prompt'] + response_cols])\n",
    "\n",
    "    # Store in CSV\n",
    "    df.to_csv(EVAL_ANSWERS_CSV, index=False, sep=';')\n",
    "\n",
    "else:\n",
    "    df = pd.read_csv(EVAL_ANSWERS_CSV, sep=';')\n",
    "\n",
    "\n",
    "\n",
    "# if GENERATE_RESPONSES:\n",
    "#     # Apply both models\n",
    "#     df['response_base_model'] = df['prompt'].apply(lambda x: generate_response(x, tokenizer, base_model))\n",
    "#     df['response_new_model'] = df['prompt'].apply(lambda x: generate_response(x, tokenizer, new_model))\n",
    "\n",
    "#     # Show result\n",
    "#     print(df[['prompt', 'response_new_model', 'response_base_model']])\n",
    "\n",
    "#     # store response df in csv\n",
    "#     df.to_csv(EVAL_ANSWERS_CSV, index=False, sep=';')\n",
    "# else:\n",
    "#     pd.read_csv(EVAL_ANSWERS_CSV, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0658429a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'response_base_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/Evaluation/lib/python3.9/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3811\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mpandas/_libs/index.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/index.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'response_base_model'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mresponse_base_model\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresponse_new_model\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/envs/Evaluation/lib/python3.9/site-packages/pandas/core/frame.py:4107\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4107\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4109\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/miniconda3/envs/Evaluation/lib/python3.9/site-packages/pandas/core/indexes/base.py:3819\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3815\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3816\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3817\u001b[0m     ):\n\u001b[1;32m   3818\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3819\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3820\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3821\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3822\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3823\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'response_base_model'"
     ]
    }
   ],
   "source": [
    "# print(df['response_base_model'][0])\n",
    "# print(df['response_new_model'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb0c503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['een overzicht van de uitgaven en de ontvangsten in de begrotingen voor het begrotingsjaar en de vier daarop aansluitende jaren.\\nArtikel 2.23 sectie 4c IN Comptabiliteitswet 2016\\n\\nde budgettaire beschouwingen over het voorgenomen beleid voor de collectieve sector\\nArtikel 2.23 sectie 4b IN Comptabiliteitswet 2016\\n\\nhet budgettaire totaalbeeld voor het betrokken begrotingsjaar en de vier daaropvolgende jaren van de rijksbegroting en de niet tot de rijksbegroting behorende budgetdisciplinesectoren\\nArtikel 2.23 sectie 4a IN Comptabiliteitswet 2016\\n\\n', 'de verblijfsvergunning wordt verleend met ingang van de dag waarop de vreemdeling heeft aangetoond dat hij aan alle voorwaarden voldoet\\nArtikel 26, sectie 1 IN Vreemdelingenwet 2024\\n\\nde verblijfsvergunning wordt verleend met ingang van een dag eerder dan de dag waarop de aanvraag is ontvangen\\nArtikel 26, sectie 1 IN Vreemdelingenwet 2024\\n\\n', 'Begroting bevat begrotingsstaat\\nArtikel 2.2 IN Comptabiliteitswet 2016\\n\\n', 'college stelt het recht op bijstand vast op schriftelijke aanvraag\\nArtikel 43, sectie 1 IN Participatiewet\\n\\nNOT bijstand wordt door de echtgenoten gezamenlijk aangevraagd dan wel door een van hen met schriftelijke toestemming van de ander\\nArtikel 43, sectie 2 IN Participatiewet\\n\\nNOT belanghebbende is een in Nederland woonachtige Nederlander\\nArtikel 11, sectie 1 IN Participatiewet\\n\\nNOT belanghebbende is een hier te lande woonachtige vreemdeling die rechtmatig in Nederland verblijf houdt in de zin van artikel 8, onderdelen a tot en met e en l, van de Vreemdelingenwet 2000\\nArtikel 11, sectie 2 IN Participatiewet\\n\\n\\nbelanghebbende aan wie rechtens zijn vrijheid is ontnomen\\nArtikel 13, sectie 1 a IN Participatiewet\\n\\nbelanghebbende die zich onttrekt aan de tenuitvoerlegging van een vrijheidsstraf of vrijheidsbenemende maatregel\\nArtikel 13, sectie 1 b IN Participatiewet\\n\\nbelanghebbende die zijn militaire of vervangende dienstplicht vervult\\nArtikel 13, sectie 1 c IN Participatiewet\\n\\nbelanghebbende die wegens werkstaking of uitsluiting niet deelneemt aan de arbeid, voorzover diens gebrek aan middelen daarvan het gevolg is\\nArtikel 13, sectie 1 d IN Participatiewet\\n\\nbelanghebbende die per kalenderjaar langer dan vier weken verblijf houdt buiten Nederland\\nArtikel 13, sectie 1 e IN Participatiewet\\n\\nbelanghebbende die bijstand vraagt ter gedeeltelijke of volledige aflossing van een schuldenlast\\nArtikel 13, sectie 1 g IN Participatiewet\\n\\nbelanghebbende die overigens bij het ontstaan van de schuldenlast dan wel nadien, beschikte of beschikt over de middelen om in de noodzakelijke kosten van het bestaan te voorzien\\nArtikel 13, sectie 1 g IN Participatiewet\\n\\nbelanghebbende die een uitreiziger is\\nArtikel 13, sectie 1 h IN Participatiewet\\n\\nbelanghebbende van 18, 19 of 20 jaar die in een inrichting verblijft\\nArtikel 13, sectie 2 a IN Participatiewet\\n\\nbelanghebbende die jonger is dan 27 jaar en uit ’s Rijks kas bekostigd onderwijs kan volgen\\nArtikel 13, sectie 2 c IN Participatiewet\\n\\nbelanghebbende heeft in verband met volgen onderwijs aanspraak op studiefinanciering op grond van de Wet studiefinanciering 2000\\nArtikel 13, sectie 2 c 1 IN Participatiewet\\n\\nbelanghebbende heeft geen aanspraak op studiefinanciering omdat hij onderwijs dat hij kan volgen onderwijs, niet volgt\\nArtikel 13, sectie 2 c 2 IN Participatiewet\\n\\nbelanghebbende die jonger is dan 27 jaar en uit wiens houding en gedragingen ondubbelzinnig blijkt dat hij de verplichtingen, bedoeld in artikel 9, eerste lid, of artikel 55 niet wil nakomen\\nArtikel 13, sectie 2 d IN Participatiewet\\n\\nbijstand wordt toegekend voor de dag waarop de belanghebbende zich heeft gemeld om bijstand aan te vragen\\nArtikel 44, sectie 1 IN Participatiewet\\n\\nNOT het in aanmerking te nemen inkomen lager is dan de bijstandsnorm\\nArtikel 19 sectie 1 a IN Participatiewet\\n\\ner is in aanmerking te nemen vermogen\\nArtikel 19 sectie 1 b IN Participatiewet\\n\\n']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create list of column names\n",
    "new_cols = [f'response_new_model_{j+1}' for j in range(NUM_RESPONSES)]\n",
    "base_cols = [f'response_base_model_{j+1}' for j in range(NUM_RESPONSES)]\n",
    "\n",
    "# Select columns and convert to list of lists (rows)\n",
    "candidates_new = df[new_cols].values.tolist()\n",
    "candidates_base = df[base_cols].values.tolist()\n",
    "\n",
    "precon_text_list = df['precondition_texts'].to_list()\n",
    "precon_pos_list = df[\"precondition_positions\"].to_list()\n",
    "\n",
    "references = []\n",
    "for dict1, dict2 in zip(precon_text_list, precon_pos_list):\n",
    "    dict1 = ast.literal_eval(dict1)\n",
    "    dict2 = ast.literal_eval(dict2)\n",
    "    combined = []\n",
    "    for key in dict1.keys():  # or use sorted(dict1.keys()) if key order isn't guaranteed\n",
    "        combined.append(str(dict1[key]) + '\\n')\n",
    "        combined.append(str(dict2[key]) + '\\n\\n')\n",
    "    references.append(''.join(combined))\n",
    "\n",
    "print(references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cdaded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the candidate lists\n",
    "candidates_new_flat = [resp for row in candidates_new for resp in row]\n",
    "candidates_base_flat = [resp for row in candidates_base for resp in row]\n",
    "\n",
    "# Repeat each reference NUM_RESPONSES times to match the flattened predictions\n",
    "references_flat = [ref for ref in references for _ in range(NUM_RESPONSES)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1edeb3",
   "metadata": {},
   "source": [
    "## ROUGE/BLEU on relevant sequence from the answer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b32cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results new: {'rouge1': np.float64(0.27001352892246916), 'rouge2': np.float64(0.09322759038840496), 'rougeL': np.float64(0.19553374880287844), 'rougeLsum': np.float64(0.2602304569036782)}\n",
      "Results base: {'rouge1': np.float64(0.24465001162862643), 'rouge2': np.float64(0.0811503655720357), 'rougeL': np.float64(0.16359792413074326), 'rougeLsum': np.float64(0.23334599359548214)}\n"
     ]
    }
   ],
   "source": [
    "# Evaluation metric 1: ROUGE on relevant sequences\n",
    "# WHy not BLEU --> penalizes missing ngrams, not something I m looking for here\n",
    "\n",
    "rouge = evaluate.load('rouge')\n",
    "\n",
    "\n",
    "results_new = rouge.compute(predictions=candidates_new_flat, references=references_flat)\n",
    "results_base = rouge.compute(predictions=candidates_base_flat, references=references_flat)\n",
    "\n",
    "\n",
    "print(f\"Results new: {results_new}\")\n",
    "print(f\"Results base: {results_base}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db770b90",
   "metadata": {},
   "source": [
    "## BERT Score --> based on embedding similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5360138",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# Maybe add BERTScore --> semantic similarity based on sentencetransformer\n",
    "P_new, R_new, F1_new = score(\n",
    "    candidates_new_flat, \n",
    "    references_flat, \n",
    "    model_type='answerdotai/ModernBERT-base', \n",
    "    num_layers=22,\n",
    "    lang='nl')\n",
    "\n",
    "P_base, R_base, F1_base = score(\n",
    "    candidates_base_flat, \n",
    "    references_flat, \n",
    "    model_type='answerdotai/ModernBERT-base', \n",
    "    num_layers=22,\n",
    "    lang='nl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdb33c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Score metrics new: (tensor([0.7933, 0.8294, 0.8098, 0.8270, 0.8279, 0.8057, 0.7394, 0.7518, 0.7185,\n",
      "        0.7795, 0.7721, 0.7247, 0.7345, 0.7336, 0.7939, 0.8140, 0.7858, 0.7998,\n",
      "        0.7510, 0.7587]), tensor([0.7802, 0.8213, 0.7809, 0.8345, 0.7905, 0.8432, 0.8291, 0.8384, 0.8564,\n",
      "        0.8490, 0.8264, 0.8140, 0.8213, 0.8025, 0.8578, 0.8010, 0.8116, 0.8046,\n",
      "        0.7838, 0.7920]), tensor([0.7867, 0.8253, 0.7951, 0.8307, 0.8088, 0.8240, 0.7816, 0.7927, 0.7814,\n",
      "        0.8127, 0.7983, 0.7668, 0.7755, 0.7665, 0.8246, 0.8074, 0.7985, 0.8022,\n",
      "        0.7671, 0.7750]))\n",
      "BERT Score metrics base: (tensor([0.7348, 0.7843, 0.8411, 0.7709, 0.7690, 0.8050, 0.7043, 0.7389, 0.7662,\n",
      "        0.7754, 0.7376, 0.6555, 0.6495, 0.7245, 0.6337, 0.7457, 0.8181, 0.8178,\n",
      "        0.7632, 0.7629]), tensor([0.7778, 0.7803, 0.8784, 0.8060, 0.7952, 0.8540, 0.8341, 0.8437, 0.8502,\n",
      "        0.8580, 0.8220, 0.7818, 0.7714, 0.8182, 0.8074, 0.7769, 0.8027, 0.8191,\n",
      "        0.8001, 0.8091]), tensor([0.7557, 0.7823, 0.8594, 0.7881, 0.7819, 0.8287, 0.7637, 0.7878, 0.8060,\n",
      "        0.8146, 0.7775, 0.7131, 0.7052, 0.7685, 0.7101, 0.7610, 0.8104, 0.8185,\n",
      "        0.7813, 0.7853]))\n",
      "F1 base: 0.779949963092804, F1 new: 0.7960512042045593\n"
     ]
    }
   ],
   "source": [
    "print(f\"BERT Score metrics new: {P_new, R_new, F1_new}\")\n",
    "print(f\"BERT Score metrics base: {P_base, R_base, F1_base}\")\n",
    "\n",
    "print(f\"F1 base: {F1_base.mean()}, F1 new: {F1_new.mean()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78332f58",
   "metadata": {},
   "source": [
    "## G-Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02388cd4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AzureOpenAIModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 17\u001b[0m\n\u001b[1;32m     12\u001b[0m subscription_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAZURE_OPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m api_version \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2024-12-01-preview\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 17\u001b[0m azure_model \u001b[38;5;241m=\u001b[39m \u001b[43mAzureOpenAIModel\u001b[49m(\n\u001b[1;32m     18\u001b[0m     api_version\u001b[38;5;241m=\u001b[39mapi_version,\n\u001b[1;32m     19\u001b[0m     azure_endpoint\u001b[38;5;241m=\u001b[39mendpoint,\n\u001b[1;32m     20\u001b[0m     api_key\u001b[38;5;241m=\u001b[39msubscription_key,\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     24\u001b[0m criteria \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     25\u001b[0m \n\u001b[1;32m     26\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     40\u001b[0m \n\u001b[1;32m     41\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m \u001b[38;5;66;03m#TODO: think about criteria...\u001b[39;00m\n\u001b[1;32m     43\u001b[0m precondition_extraction_metric \u001b[38;5;241m=\u001b[39m GEval(\n\u001b[1;32m     44\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrecondition_Extraction\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     45\u001b[0m     criteria\u001b[38;5;241m=\u001b[39mcriteria,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m \n\u001b[1;32m     49\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'AzureOpenAIModel' is not defined"
     ]
    }
   ],
   "source": [
    "# Evaluation metric 2: Evaluate whole answer on G-Eval\n",
    "\n",
    "prompts = df['prompt'].tolist()\n",
    "\n",
    "# setup variables for the Azure OpenAI API\n",
    "endpoint = \"https://openai-ds-instance-sweden.openai.azure.com/\"\n",
    "model_name = \"gpt-4.1\"\n",
    "deployment = \"deze-voor-alles\"\n",
    "\n",
    "\n",
    "\n",
    "subscription_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "api_version = \"2024-12-01-preview\"\n",
    "\n",
    "\n",
    "\n",
    "azure_model = AzureOpenAIModel(\n",
    "    api_version=api_version,\n",
    "    azure_endpoint=endpoint,\n",
    "    api_key=subscription_key,\n",
    ")\n",
    "\n",
    "\n",
    "criteria = \"\"\"\n",
    "\n",
    "\n",
    "Evalueer hoe goed een taalmodel presteerde in de taak van voorwaarde-extractie uit Nederlandse juridische teksten.\n",
    "\n",
    "Voor elke lijst van precondities/subfacts werd de act/fact aan een taalmodel gegeven als onderdeel van een prompt, met de opdracht om alle bijbehorende subfact/preconditie(s) en hun respectieve positie(s) in de tekst terug te geven.\n",
    "\n",
    "Uw taak is om per paar te evalueren hoe goed het model presteerde op twee punten (op een 4-punt Likert-schaal):\n",
    "\n",
    "1. **Het vinden van alle relevante precondities in de tekst**\n",
    "2. **Hoe duidelijk de positie in de tekst is die het model aanduidde**\n",
    "\n",
    "Voor het modelantwoord krijgt u het hele antwoord voor een act/fact (die dus meredere precondities/subfacts kan beïnhouden) en moet u voor elke ground truth preconditie afzonderlijk evalueren of deze \n",
    "    a) aanwezig is in het antwoord en \n",
    "    b) of de positie ervan ook goed is aangegeven in het antwoord. \n",
    "\n",
    "\n",
    "\"\"\" #TODO: think about criteria...\n",
    "\n",
    "precondition_extraction_metric = GEval(\n",
    "    name=\"Precondition_Extraction\",\n",
    "    criteria=criteria,\n",
    "    evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT, LLMTestCaseParams.EXPECTED_OUTPUT],\n",
    "    model=azure_model,\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffd8ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results\n",
    "scores_new, reasons_new = [], []\n",
    "scores_base, reasons_base = [], []\n",
    "\n",
    "# Evaluate each prompt individually\n",
    "for prompt, new_out, base_out, ref in zip(prompts, candidates_new_flat, candidates_base_flat, references_flat):\n",
    " \n",
    "    # New model evaluation\n",
    "    g_eval_new = LLMTestCase(\n",
    "        input=prompt,\n",
    "        actual_output=f\"Extracted Preconditions: {new_out}\",\n",
    "        expected_output=f\"Expected Preconditions: {ref}\"\n",
    "    )\n",
    "    precondition_extraction_metric.measure(g_eval_new)\n",
    "    scores_new.append(precondition_extraction_metric.score)\n",
    "    reasons_new.append(precondition_extraction_metric.reason)\n",
    "\n",
    "    # Base model evaluation\n",
    "    g_eval_base = LLMTestCase(\n",
    "        input=prompt,\n",
    "        actual_output=f\"Extracted Preconditions: {base_out}\",\n",
    "        expected_output=f\"Expected Preconditions: {ref}\"\n",
    "    )   \n",
    "    precondition_extraction_metric.measure(g_eval_base)\n",
    "    scores_base.append(precondition_extraction_metric.score)\n",
    "    reasons_base.append(precondition_extraction_metric.reason)\n",
    "\n",
    "# Optionally: print or analyze results\n",
    "print(\"New Model Scores:\", scores_new)\n",
    "print(\"Base Model Scores:\", scores_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a98bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "############ OLD VERSION ######################\n",
    "\n",
    "# Now define your test case, actual_output is your LLM output\n",
    "g_eval_new = LLMTestCase(input=prompts, actual_output=f\"Extracetd Preconditions: {candidates_new}\", expected_output=f\"Expected preconditions: {references}\")\n",
    "g_eval_base = LLMTestCase(input=prompts, actual_output=f\"Extracetd Preconditions: {candidates_base}\", expected_output=f\"Expected preconditions: {references}\")\n",
    "\n",
    "\n",
    "# Use G-Eval metric\n",
    "precondition_extraction_metric.measure(g_eval_new)\n",
    "print(\"Score:\", precondition_extraction_metric.score)\n",
    "print(\"Reason:\", precondition_extraction_metric.reason)\n",
    "\n",
    "precondition_extraction_metric.measure(g_eval_base)\n",
    "print(\"Score:\", precondition_extraction_metric.score)\n",
    "print(\"Reason:\", precondition_extraction_metric.reason)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1585bcb3",
   "metadata": {},
   "source": [
    "Run this in the proper environment tomorrow:\n",
    "\n",
    "https://deepeval.com/docs/metrics-introduction#using-azure-openai\n",
    "\n",
    "deepeval set-azure-openai \\\n",
    "    --openai-endpoint=<endpoint> \\ # e.g. https://example-resource.azure.openai.com/\n",
    "    --openai-api-key=<api_key> \\\n",
    "    --openai-model-name=<model_name> \\ # e.g. gpt-4o\n",
    "    --deployment-name=<deployment_name> \\  # e.g. Test Deployment\n",
    "    --openai-api-version=<openai_api_version> \\ # e.g. 2025-01-01-preview\n",
    "    --model-version=<model_version> # e.g. 2024-11-20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6086f2",
   "metadata": {},
   "source": [
    "## Personal evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34ef6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate about 10 answers with both models --> load them into the user interface and get reward score\n",
    "# Also maybe do qualitative evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Evaluation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
