{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tune reward model from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODOs:\n",
    "\n",
    "#TODO: double-check that labels are not somehow misaligned...\n",
    "\n",
    "#TODO: check if you need to plot \n",
    "\n",
    "1. LoRA learns the position of the low rank adaptation matrix that is needed to finetune a model of a much higher rank\n",
    "\n",
    "#TODO: double check model performance, generate output, maybe adjust training metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports, setup, and global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "CUDA is available. Using GPU: NVIDIA L40S\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Add the parent directory to the Python path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname(os.getcwd()), '..')))\n",
    "\n",
    "from transformers import TrainingArguments, EarlyStoppingCallback\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from datasets import Dataset, DatasetDict, load_from_disk\n",
    "\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "\n",
    "from utils import parse_ratings, tokenize_fn_with_best_window, tokenize_fn_basic_batched, CustomRewardTrainer, find_best_window\n",
    "\n",
    "\n",
    "# from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# load the relevant devices available on the server\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = os.getenv(\"AVAILABLE_DEVICES\")\n",
    "\n",
    "# Enable expandable CUDA segments\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# load cuda\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print(\"CUDA is available. Using GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA is not available. Using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training variables\n",
    "FEEDBACK_TO_TRAIN_ON = os.getenv(\"FEEDBACK_TO_TRAIN_ON\")\n",
    "FEEDBACK_TO_REMOVE = os.getenv(\"FEEDBACK_TO_REMOVE\")\n",
    "MODEL = os.getenv(\"REWARD_MODEL\")\n",
    "DATASET = os.getenv(\"REWARD_DATASET\")\n",
    "TOKENIZE_FN = os.getenv(\"TOKENIZE_FN\")\n",
    "MAX_LENGTH = os.getenv(\"MAX_LENGTH\")\n",
    "STRIDE = os.getenv(\"STRIDE\")\n",
    "LORA_CHECKPOINTS_FOLDER = os.getenv(\"LORA_CHECKPOINTS_FOLDER\")\n",
    "FINAL_LORA_ADAPTERS = os.getenv(\"FINAL_LORA_ADAPTERS_FOLDER\") + f\"_{FEEDBACK_TO_TRAIN_ON}_{TOKENIZE_FN}_{DATASET}\"\n",
    "TOKENIZED_DATA = os.getenv(\"TOKENIZED_DATA\") + f\"_{FEEDBACK_TO_TRAIN_ON}_{TOKENIZE_FN}_{DATASET}\"\n",
    "\n",
    "# load training data\n",
    "FILE_1 = os.getenv(\"FILE_1\")\n",
    "FILE_5 = os.getenv(\"FILE_5\")\n",
    "FILE_7 = os.getenv(\"FILE_7\")\n",
    "FILE_9 = os.getenv(\"FILE_9\")\n",
    "FILE_10_1 = os.getenv(\"FILE_10_1\")\n",
    "FILE_10_2 = os.getenv(\"FILE_10_2\")\n",
    "FILE_SYNTH = os.getenv(\"FILE_SYNTH\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset loading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataframes\n",
    "df_1 = pd.read_csv(FILE_1, sep=\";\")\n",
    "df_5 = pd.read_csv(FILE_5, sep=\";\")\n",
    "df_7 = pd.read_csv(FILE_7, sep=\";\")\n",
    "df_9 = pd.read_csv(FILE_9, sep=\";\")\n",
    "df_10_1 = pd.read_csv(FILE_10_1, sep=\";\")\n",
    "df_10_2 = pd.read_csv(FILE_10_2, sep=\";\")\n",
    "df_synth = pd.read_csv(FILE_SYNTH, sep=\";\")\n",
    "\n",
    "df_human = pd.concat([df_1, df_5, df_7, df_9, df_10_1, df_10_2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Re-structure df synthetic to fit in training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic feedback shape: (563, 14)\n",
      "Index(['file', 'frame_ID', 'frame_type', 'frame_text', 'precondition_id',\n",
      "       'precondition_text', 'precondition_position', 'response_text',\n",
      "       'prompt_config_examples', 'prompt_config_chain_of_thought',\n",
      "       'feedback_extraction', 'feedback_detection', 'additional_feedback',\n",
      "       'synthetic_feedback'],\n",
      "      dtype='object')\n",
      "Synthetic feedback shape: (564, 14)\n"
     ]
    }
   ],
   "source": [
    "print(\"Synthetic feedback shape:\", df_synth.shape)\n",
    "\n",
    "\n",
    "# Save the current headers since forgot to store headers in csv file\n",
    "old_headers = df_synth.columns.tolist()\n",
    "\n",
    "# print(\"Old headers:\", old_headers)\n",
    "\n",
    "# Step 2: Insert the headers as the first row\n",
    "df_synth.loc[-1] = old_headers # Add headers as a new row\n",
    "df_synth.index = df_synth.index + 1 # Shift index\n",
    "df_synth = df_synth.sort_index() # Sort index to place the new row at the top\n",
    "\n",
    "\n",
    "# Step 3: Assign new headers (optional)\n",
    "df_synth.columns = ['file', \n",
    "                            'frame_ID', \n",
    "                            'frame_type', \n",
    "                            'frame_text', \n",
    "                            'precondition_id', \n",
    "                            'precondition_text', \n",
    "                            'precondition_position', \n",
    "                            'response_text', \n",
    "                            'prompt_config_examples', \n",
    "                            'prompt_config_chain_of_thought', \n",
    "                            'feedback_extraction', \n",
    "                            'feedback_detection', \n",
    "                            'additional_feedback',\n",
    "                            'synthetic_feedback',\n",
    "                ]\n",
    "\n",
    "print(df_synth.columns)\n",
    "\n",
    "\n",
    "df_synth['prompt_config_examples'] = (df_synth['prompt_config_examples']                                              \n",
    "                                                .astype(str)\n",
    "                                                .str.strip()\n",
    "                                                .str.lower()\n",
    "                                                .map({'true': True, 'false': False})\n",
    ")\n",
    "\n",
    "df_synth['prompt_config_chain_of_thought'] = (df_synth['prompt_config_chain_of_thought']\n",
    "                                                .astype(str)\n",
    "                                                .str.strip()\n",
    "                                                .str.lower()\n",
    "                                                .map({'true': True, 'false': False})\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print(\"Synthetic feedback shape:\", df_synth.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['file', 'frame_ID', 'frame_type', 'frame_text', 'precondition_id',\n",
       "       'precondition_text', 'precondition_position', 'response_text',\n",
       "       'prompt_config_examples', 'prompt_config_chain_of_thought',\n",
       "       'feedback_extraction', 'feedback_detection', 'additional_feedback'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if DATASET == \"human\":\n",
    "    df_train = df_human\n",
    "elif DATASET == \"synthetic\":\n",
    "    df_train = df_synth\n",
    "    \n",
    "    \n",
    "df_train.shape\n",
    "df_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. a) Parse ratings to numeric values for MSE Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed feedback for extraction: 0    2\n",
      "1    2\n",
      "2    2\n",
      "3    2\n",
      "4    3\n",
      "Name: feedback_extraction, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df_train[FEEDBACK_TO_TRAIN_ON] = [parse_ratings(feedback) for feedback in df_train[FEEDBACK_TO_TRAIN_ON]]\n",
    "print(\"Parsed feedback for extraction:\", df_train[FEEDBACK_TO_TRAIN_ON][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. b) look at biases in feedback to train on for weights in RL loop --> feedback_detection is very biased through way it was collected, so gets less weight overall..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feedback_extraction\n",
       "0    499\n",
       "3    231\n",
       "2    104\n",
       "1     95\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[FEEDBACK_TO_TRAIN_ON].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. c) keep only relevant feedback column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['file', 'frame_ID', 'frame_type', 'frame_text', 'precondition_id', 'precondition_text', 'precondition_position', 'response_text', 'prompt_config_examples', 'prompt_config_chain_of_thought', 'feedback_extraction', 'feedback_detection', 'additional_feedback'],\n",
      "    num_rows: 929\n",
      "})\n",
      "feedback_extraction\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset.from_pandas(df_train)\n",
    "\n",
    "print(dataset)\n",
    "print(FEEDBACK_TO_TRAIN_ON) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.remove_columns([FEEDBACK_TO_REMOVE])\n",
    "dataset = dataset.rename_column(FEEDBACK_TO_TRAIN_ON, \"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load model with LoRA layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at nlpaueb/legal-bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSdpaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Load the model and the tokenizer\n",
    "model_id = MODEL \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_id, num_labels=1) # num_labels = 1 since we want to prodict a single scalar (the rating)\n",
    "\n",
    "# Comment: Automodel for sequence classification with num_labels=1 already has a regression head\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n",
      "512\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.model_max_length)\n",
    "print(model.config.max_position_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 443,137 || all params: 109,926,146 || trainable%: 0.4031\n"
     ]
    }
   ],
   "source": [
    "# Define LoRA config\n",
    "lora_config = LoraConfig(\n",
    "    r=8,           # Rank of the LoRA matrices (smaller = less memory)\n",
    "    lora_alpha=16, # Scaling factor (higher = stronger adaptation)\n",
    "    target_modules=[\"query\", \"key\", \"value\"], # Apply LoRA to attention layers\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_CLS\"  # classify each answer \n",
    ")\n",
    "\n",
    "# Freeze base model\n",
    "\n",
    "# Freeze base model\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "# Convert the model to a PEFT (LoRA) model\n",
    "model = get_peft_model(model, lora_config)\n",
    "# model.gradient_checkpointing_enable()\n",
    "model.print_trainable_parameters()  # Check trainable params (~0.1% of full model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 1067, 223, 207, 580, 210, 1335, 124, 102, 0, 0, 0], [101, 1067, 223, 207, 5601, 190, 580, 213, 207, 1727, 124, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test tokenizer\n",
    "sample_data = [\"What is the capital of France?\", \"What is the largest capital in the world?\"]\n",
    "tokenizer(sample_data, padding=True, truncation=True, max_length=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Encode dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['file', 'frame_ID', 'frame_type', 'frame_text', 'precondition_id', 'precondition_text', 'precondition_position', 'response_text', 'prompt_config_examples', 'prompt_config_chain_of_thought', 'label', 'additional_feedback']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 929/929 [00:00<00:00, 16769.13 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2, 2, 2, 3]\n",
      "['1. Subfact: Onze Minister\\n                2. Positie: Artikel 1, sectie 1 IN Vreemdelingenwet geldig vanaf 2024\\n                3. Subfact: Onze Minister\\n                4. Positie: Artikel 8, sectie 1 IN Vreemdelingenwet geldig vanaf 2024\\n                5. Subfact: Onze Minister\\n                6. Positie: Artikel 14, sectie 1 IN Vreemdelingenwet geldig vanaf 2024\\n                7. Subfact: Onze Minister\\n                8. Positie: Artikel 16, sectie 1 IN Vreemdelingenwet geldig vanaf 2024\\n                9. Subfact: Onze Minister\\n                10. Positie: Artikel 17, sectie 1 IN Vreemdelingenwet geldig vanaf 2024\\n                11. Subfact: Onze Minister\\n                12. Positie: Artikel 17a, sectie 1 IN Vreemdelingenwet geldig vanaf 2024\\n                13. Subfact: Onze Minister\\n                14. Positie: Artikel 26, sectie 1 IN Vreemdelingenwet geldig vanaf 2024\\n                15. Subfact: Onze Minister\\n                16. Positie: Artikel 27, sectie 1 IN Vreemdelingenwet geldig vanaf 2024\\n                17. Subfact: Onze Minister\\n                18. Positie: Artikel 66a, sectie 6 IN Vreemdelingenwet geldig vanaf 2024\\n                19. Subfact: Onze Minister\\n                20. Positie: Artikel 67, sectie 3 IN Vreemdelingenwet geldig vanaf 2024\\n', '1. Subfact: Onze Minister\\n                2. Positie: Artikel 1, sectie 1 IN Vreemdelingenwet geldig vanaf 2024\\n                3. Subfact: Onze Minister\\n                4. Positie: Artikel 8, sectie 1 IN Vreemdelingenwet geldig vanaf 2024\\n                5. Subfact: Onze Minister\\n                6. Positie: Artikel 14, sectie 1 IN Vreemdelingenwet geldig vanaf 2024\\n                7. Subfact: Onze Minister\\n                8. Positie: Artikel 16, sectie 1 IN Vreemdelingenwet geldig vanaf 2024\\n                9. Subfact: Onze Minister\\n                10. Positie: Artikel 17, sectie 1 IN Vreemdelingenwet geldig vanaf 2024\\n                11. Subfact: Onze Minister\\n                12. Positie: Artikel 17a, sectie 1 IN Vreemdelingenwet geldig vanaf 2024\\n                13. Subfact: Onze Minister\\n                14. Positie: Artikel 26, sectie 1 IN Vreemdelingenwet geldig vanaf 2024\\n                15. Subfact: Onze Minister\\n                16. Positie: Artikel 27, sectie 1 IN Vreemdelingenwet geldig vanaf 2024\\n                17. Subfact: Onze Minister\\n                18. Positie: Artikel 66a, sectie 6 IN Vreemdelingenwet geldig vanaf 2024\\n                19. Subfact: Onze Minister\\n                20. Positie: Artikel 67, sectie 3 IN Vreemdelingenwet geldig vanaf 2024\\n', '1. Subfact: Onze Minister\\n                2. Positie: Artikel 1, sectie 1 IN Vreemdelingenwet geldig vanaf 2024\\n                3. Subfact: Onze Minister\\n                4. Positie: Artikel 8, sectie 1 IN Vreemdelingenwet geldig vanaf 2024\\n                5. Subfact: Onze Minister\\n                6. Positie: Artikel 14, sectie 1 IN Vreemdelingenwet geldig vanaf 2024\\n                7. Subfact: Onze Minister\\n                8. Positie: Artikel 16, sectie 1 IN Vreemdelingenwet geldig vanaf 2024\\n                9. Subfact: Onze Minister\\n                10. Positie: Artikel 17, sectie 1 IN Vreemdelingenwet geldig vanaf 2024\\n                11. Subfact: Onze Minister\\n                12. Positie: Artikel 17a, sectie 1 IN Vreemdelingenwet geldig vanaf 2024\\n                13. Subfact: Onze Minister\\n                14. Positie: Artikel 26, sectie 1 IN Vreemdelingenwet geldig vanaf 2024\\n                15. Subfact: Onze Minister\\n                16. Positie: Artikel 27, sectie 1 IN Vreemdelingenwet geldig vanaf 2024\\n                17. Subfact: Onze Minister\\n                18. Positie: Artikel 66a, sectie 6 IN Vreemdelingenwet geldig vanaf 2024\\n                19. Subfact: Onze Minister\\n                20. Positie: Artikel 67, sectie 3 IN Vreemdelingenwet geldig vanaf 2024\\n', '1. Subfact: Onze Minister\\n\\n                2. Positie: Artikel 1, sectie 1 IN Algemene wet bestuursrecht\\n\\n                3. Positie: Artikel 1, sectie 1 IN Vreemdelingenwet geldig vanaf 2024\\n\\n                4. Positie: Artikel 14, 1 IN Vreemdelingenwet geldig vanaf 2024\\n\\n                5. Positie: Artikel 1, sub 2° IN Vreemdelingenwet geldig vanaf 2024\\n\\n                6. Positie: Artikel 1, sub 4° IN Vreemdelingenwet geldig vanaf 2024\\n\\n                7. Positie: Artikel 1, sub 6° IN Vreemdelingenwet geldig vanaf 2024\\n\\n                8. Positie: Artikel 1, sub 2° IN Vreemdelingenwet geldig vanaf 2024, Afdeling 3. De verblijfsvergunning regulier\\n\\n                9. Positie: Artikel 9, 1 IN Vreemdelingenwet geldig vanaf 2024\\n\\n                10. Positie: Artikel 14, 1a IN Vreemdelingenwet geldig vanaf 2024\\n\\n                11. Positie: Artikel 14, 1b IN Vreemdelingenwet geldig vanaf 2024\\n\\n                12. Positie: Artikel 14, 1c IN Vreemdelingenwet geldig vanaf 2024\\n\\n                13. Positie: Artikel 14, 1d IN Vreemdelingenwet geldig vanaf 2024\\n\\n                14. Positie: Artikel 14, 1e IN Vreemdel', '1. Subfact: Onze Minister\\n\\n                2. Positie: Artikel 1, sectie 1 IN Algemene wet bestuursrecht\\n\\n                3. Positie: Artikel 1, sectie 1 IN Vreemdelingenwet geldig vanaf 2024\\n\\n                4. Positie: Artikel 14, 1 IN Vreemdelingenwet geldig vanaf 2024\\n\\n                5. Positie: Artikel 1, sub 2° IN Vreemdelingenwet geldig vanaf 2024\\n\\n                6. Positie: Artikel 1, sub 4° IN Vreemdelingenwet geldig vanaf 2024\\n\\n                7. Positie: Artikel 1, sub 6° IN Vreemdelingenwet geldig vanaf 2024\\n\\n                8. Positie: Artikel 1, sub 2° IN Vreemdelingenwet geldig vanaf 2024, Afdeling 3. De verblijfsvergunning regulier\\n\\n                9. Positie: Artikel 9, 1 IN Vreemdelingenwet geldig vanaf 2024\\n\\n                10. Positie: Artikel 14, 1a IN Vreemdelingenwet geldig vanaf 2024\\n\\n                11. Positie: Artikel 14, 1b IN Vreemdelingenwet geldig vanaf 2024\\n\\n                12. Positie: Artikel 14, 1c IN Vreemdelingenwet geldig vanaf 2024\\n\\n                13. Positie: Artikel 14, 1d IN Vreemdelingenwet geldig vanaf 2024\\n\\n                14. Positie: Artikel 14, 1e IN Vreemdel']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# if labels are not integers, convert them to integers\n",
    "def convert_label_to_int(data):\n",
    "    data[\"label\"] = int(data[\"label\"])\n",
    "    return data\n",
    "\n",
    "\n",
    "print(dataset.column_names)\n",
    "# mao string labels to integers\n",
    "dataset = dataset.map(convert_label_to_int)  # Assuming 'text' is the column with the text data\n",
    "\n",
    "print(dataset[\"label\"][:5])  # Check labels\n",
    "print(dataset[\"response_text\"][:5])  # Check labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comment\n",
    "\n",
    "1. Needed for feedback extraction: precondition_text, response_text, label(rating feedback extraction)\n",
    "2. Needed for feedback detection: precondition_text, precondition_position, response_text, label (rating feedback detection)\n",
    "3. For the precondition position to be found well, it is a crucial for the model to find the precondition text (at least to a recognizable degree) as well, otherwise the precondition is not found at all..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1127 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "de juiste keuzes hebben gemaakt, of ze trouw zijn gebleven aan zichzelf. anderen proberen simpelweg de dag door te komen, met hoop op iets beters. in die momenten van stilte komt vaak het besef dat, hoewel we allemaal verschillende paden bewandelen, we een waarheid delen : dat het leven, ondanks al onze inspanningen en verlangens, nooit gemakkelijk is. of, zoals mijn grootmoeder het ooit zei terwijl ze haar handen vouwde na een lange dag werken op het land : [UNK] je moet weten, kind, het leven is nooit gemakkelijk, maar het is wel de moeite waard. [UNK] we worden gevormd door onze ervaringen, door de mensen die we ontmoeten en de obstakels die we overwinnen. elke fout, elk succes, elke traan en elke glimlach draagt bij aan wie we zijn. en toch, ondanks al die ervaringen, blijven we zoeken. naar betekenis. naar verbinding. naar rust. soms lijkt het alsof de wereld te snel draait. technologie verandert ons leven in een razend tempo, verwachtingen worden hoger, en de druk om te presteren neemt toe. in die chaos vergeten we soms stil te staan. te ademen. te voelen. maar juist in die momenten van rust vinden we vaak de antwoorden die we zo hard nodig hebben. de liefde, bijvoorbeeld, is een van de krachtigste krachten die ons voortdrijft. liefde voor een partner, een kind, een vriend, of zelfs voor een passie. het is die liefde die ons helpt vol te houden wanneer alles tegen\n"
     ]
    }
   ],
   "source": [
    "# Code to test bestw indow function\n",
    "\n",
    "test_text = \"\"\"\n",
    "        Titel: De Weg Door Het Leven\n",
    "\n",
    "Het leven is een reis vol onverwachte wendingen, een pad dat zich zelden rechtlijnig ontvouwt. Vanaf het moment dat we onze eerste ademhaling nemen, worden we ondergedompeld in een wereld die we nog moeten leren begrijpen. Als kind lijkt alles eenvoudig: lachen, spelen, ontdekken. Maar naarmate we ouder worden, beginnen de lagen van complexiteit zich op te stapelen. We leren dat mensen niet altijd zeggen wat ze bedoelen, dat keuzes consequenties hebben, en dat geluk soms vluchtiger is dan we zouden willen.\n",
    "\n",
    "In de vroege ochtenden, wanneer de zon net boven de horizon verschijnt en de wereld nog stil is, denken velen na over hun plaats in het grotere geheel. Sommigen vragen zich af of ze de juiste keuzes hebben gemaakt, of ze trouw zijn gebleven aan zichzelf. Anderen proberen simpelweg de dag door te komen, met hoop op iets beters. In die momenten van stilte komt vaak het besef dat, hoewel we allemaal verschillende paden bewandelen, we één waarheid delen: dat het leven, ondanks al onze inspanningen en verlangens, nooit gemakkelijk is. Of, zoals mijn grootmoeder het ooit zei terwijl ze haar handen vouwde na een lange dag werken op het land: “Je moet weten, kind, het leven is nooit gemakkelijk, maar het is wel de moeite waard.”\n",
    "\n",
    "We worden gevormd door onze ervaringen, door de mensen die we ontmoeten en de obstakels die we overwinnen. Elke fout, elk succes, elke traan en elke glimlach draagt bij aan wie we zijn. En toch, ondanks al die ervaringen, blijven we zoeken. Naar betekenis. Naar verbinding. Naar rust.\n",
    "\n",
    "Soms lijkt het alsof de wereld te snel draait. Technologie verandert ons leven in een razend tempo, verwachtingen worden hoger, en de druk om te presteren neemt toe. In die chaos vergeten we soms stil te staan. Te ademen. Te voelen. Maar juist in die momenten van rust vinden we vaak de antwoorden die we zo hard nodig hebben.\n",
    "\n",
    "De liefde, bijvoorbeeld, is een van de krachtigste krachten die ons voortdrijft. Liefde voor een partner, een kind, een vriend, of zelfs voor een passie. Het is die liefde die ons helpt vol te houden wanneer alles tegenzit. Die ons eraan herinnert waarom we begonnen zijn, waarom we blijven proberen.\n",
    "\n",
    "En dan is er verlies. Een onvermijdelijk onderdeel van het leven. We verliezen mensen, kansen, dromen. Maar in dat verlies schuilt ook groei. We leren loslaten, opnieuw beginnen, sterker worden. Het is pijnlijk, ja, maar ook noodzakelijk.\n",
    "\n",
    "Wanneer we terugkijken op ons leven, zijn het zelden de materiële zaken die we herinneren. Het zijn de momenten. De gesprekken bij kaarslicht. De wandelingen in de regen. De onverwachte lachbuien. De stilte van een gedeeld verdriet. Die momenten vormen de essentie van ons bestaan.\n",
    "\n",
    "Dus ja, het leven is vol uitdagingen. Het is rommelig, verwarrend, soms oneerlijk. Maar het is ook prachtig, rijk aan betekenis, en gevuld met kansen om te groeien, te leren en lief te hebben. En misschien is dat wel de grootste les van allemaal: dat we, ondanks alles, blijven kiezen voor hoop. Voor verbinding. Voor het leven zelf.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "test_ground_truth = \"Het leven is nooit gemakkelijk.\"\n",
    "\n",
    "print(find_best_window(test_text, test_ground_truth, device, tokenizer))\n",
    "\n",
    "# Works as expectd, I am impressed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = dataset.select(range(5))\n",
    "# tokenized_sample = tokenize_fn(sample)\n",
    "# print([len(ids) for ids in tokenized_sample[\"input_ids\"]])\n",
    "\n",
    "if not os.path.exists(TOKENIZED_DATA):\n",
    "    if TOKENIZE_FN == \"best_window\":\n",
    "        dataset = dataset.map(tokenize_fn_with_best_window, \n",
    "                              fn_kwargs={\"feedback_train\": FEEDBACK_TO_TRAIN_ON, \n",
    "                                         \"tokenizer\": tokenizer, \n",
    "                                         \"max_length\": int(MAX_LENGTH), \n",
    "                                         \"stride\": int(STRIDE),\n",
    "                                         \"device\": device\n",
    "                                         },\n",
    "                              batched=False)\n",
    "    else:\n",
    "        dataset = dataset.map(tokenize_fn_basic_batched, \n",
    "                              fn_kwargs={\"feedback_train\": FEEDBACK_TO_TRAIN_ON, \n",
    "                                         \"tokenizer\": tokenizer \n",
    "                                         },\n",
    "                              batched=True)\n",
    "    \n",
    "    dataset.save_to_disk(TOKENIZED_DATA)\n",
    "else:\n",
    "    dataset = load_from_disk(TOKENIZED_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'Interpretatie_Vw_over_besluiten_op_aanvragen_voor_een_verblijfsvergunning_regulier_bepaalde_tijd.json': 432, 'Participatiewet_most_recent_public.json': 255, 'rijksbegrotingscyclus.json': 242})\n"
     ]
    }
   ],
   "source": [
    "print(Counter(dataset['file']))\n",
    "\n",
    "#TODO: maybe use rijksbegrotingscyclus purely as a test + eval set...\n",
    "# get indices and use dataset.select!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split dataset into train, test, eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train, test and eval sets\n",
    "train_test_split = dataset.train_test_split(test_size=0.3, seed=42)\n",
    "eval_test_split = train_test_split[\"test\"].train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "\n",
    "final_splits = DatasetDict({\n",
    "    'train': train_test_split['train'],\n",
    "    'validation': eval_test_split['train'],\n",
    "    'test': eval_test_split['test']\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train reward model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=LORA_CHECKPOINTS_FOLDER,\n",
    "    eval_strategy='steps',\n",
    "    save_strategy='steps',\n",
    "    save_steps=10,\n",
    "    eval_steps=10,\n",
    "    save_total_limit=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    learning_rate=3e-4,\n",
    "    num_train_epochs=20,\n",
    "    logging_steps=10,\n",
    "    label_names=[\"labels\"],\n",
    "    # report_to=\"none\",\n",
    "    logging_dir=\"./logs\",\n",
    "    fp16=True,  # Use mixed precision training\n",
    "    metric_for_best_model=\"eval_loss\", # or \"eval_loss\"\n",
    "    greater_is_better=False, # False if using loss\n",
    "    # gradient_accumulation_steps=4, # \n",
    "    # weight_decay=0.01\n",
    ")\n",
    "\n",
    "# Initialize custom trainer\n",
    "trainer = CustomRewardTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=final_splits['train'],\n",
    "    eval_dataset=final_splits['validation'],\n",
    "    # compute_metrics=trainer.compute_metrics,  # Use the custom metrics function\n",
    "    processing_class=tokenizer,\n",
    "    loss_type=\"huber\",  # \"mse\" or \"huber\"\n",
    "    weight_strategy=\"linear\",  # \"linear\", \"inverse\", or None\n",
    "    # callbacks=[EarlyStoppingCallback(early_stopping_patience=50)] # use early stopping since we are sing high amount of epochs\n",
    "    # data_collator=RewardDataCollator()\n",
    ")\n",
    "\n",
    "print(trainer.args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='820' max='820' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [820/820 01:37, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Model Preparation Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.051500</td>\n",
       "      <td>1.672177</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.703200</td>\n",
       "      <td>1.603025</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.562600</td>\n",
       "      <td>1.495673</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.299200</td>\n",
       "      <td>1.445699</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.336000</td>\n",
       "      <td>1.327551</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.386400</td>\n",
       "      <td>1.321038</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.994200</td>\n",
       "      <td>1.314813</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.042400</td>\n",
       "      <td>1.247218</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.887100</td>\n",
       "      <td>1.124598</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.048500</td>\n",
       "      <td>1.206574</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.074800</td>\n",
       "      <td>1.083644</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.909900</td>\n",
       "      <td>1.045692</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.117800</td>\n",
       "      <td>0.983789</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.993700</td>\n",
       "      <td>1.001207</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.888500</td>\n",
       "      <td>0.961907</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.811100</td>\n",
       "      <td>1.009468</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.942000</td>\n",
       "      <td>0.954867</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.753400</td>\n",
       "      <td>0.919418</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.765100</td>\n",
       "      <td>0.961227</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.652500</td>\n",
       "      <td>1.013993</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.826500</td>\n",
       "      <td>1.011746</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.844300</td>\n",
       "      <td>0.917139</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.674300</td>\n",
       "      <td>1.011811</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.723000</td>\n",
       "      <td>0.875939</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.857400</td>\n",
       "      <td>0.990813</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.621400</td>\n",
       "      <td>0.869268</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.703600</td>\n",
       "      <td>0.811687</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.651700</td>\n",
       "      <td>0.848945</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.898800</td>\n",
       "      <td>0.855176</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.743500</td>\n",
       "      <td>0.927746</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.773300</td>\n",
       "      <td>0.923280</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.768900</td>\n",
       "      <td>0.836765</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.506000</td>\n",
       "      <td>0.787331</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.577900</td>\n",
       "      <td>0.845129</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.736582</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.498900</td>\n",
       "      <td>0.650926</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.695900</td>\n",
       "      <td>0.612800</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.573500</td>\n",
       "      <td>0.583145</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.750200</td>\n",
       "      <td>0.543135</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.513200</td>\n",
       "      <td>0.704134</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.573300</td>\n",
       "      <td>0.620528</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.388700</td>\n",
       "      <td>0.583636</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.495000</td>\n",
       "      <td>0.619045</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.570300</td>\n",
       "      <td>0.676275</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.664300</td>\n",
       "      <td>0.528110</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.422900</td>\n",
       "      <td>0.518014</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.409300</td>\n",
       "      <td>0.509859</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.484000</td>\n",
       "      <td>0.567919</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.403000</td>\n",
       "      <td>0.562313</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.379600</td>\n",
       "      <td>0.627747</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.449800</td>\n",
       "      <td>0.584767</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.437300</td>\n",
       "      <td>0.562950</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>0.476700</td>\n",
       "      <td>0.607193</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.340000</td>\n",
       "      <td>0.687962</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.581600</td>\n",
       "      <td>0.598339</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.419400</td>\n",
       "      <td>0.596086</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>0.390400</td>\n",
       "      <td>0.589972</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.459300</td>\n",
       "      <td>0.586326</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>0.393100</td>\n",
       "      <td>0.577507</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.397400</td>\n",
       "      <td>0.600499</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>0.365400</td>\n",
       "      <td>0.603922</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.283600</td>\n",
       "      <td>0.599598</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>0.308500</td>\n",
       "      <td>0.606343</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.435900</td>\n",
       "      <td>0.633494</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.484300</td>\n",
       "      <td>0.615290</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.442100</td>\n",
       "      <td>0.615965</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>0.427500</td>\n",
       "      <td>0.615992</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.452600</td>\n",
       "      <td>0.621386</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>0.315700</td>\n",
       "      <td>0.605122</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.372900</td>\n",
       "      <td>0.591111</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>0.345700</td>\n",
       "      <td>0.588625</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.444900</td>\n",
       "      <td>0.588460</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>0.401100</td>\n",
       "      <td>0.583316</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>0.311800</td>\n",
       "      <td>0.576098</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.376800</td>\n",
       "      <td>0.565230</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>0.534300</td>\n",
       "      <td>0.574870</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>0.248700</td>\n",
       "      <td>0.577852</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>0.304700</td>\n",
       "      <td>0.577085</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.573772</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.408300</td>\n",
       "      <td>0.573476</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>0.385400</td>\n",
       "      <td>0.574567</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>0.346100</td>\n",
       "      <td>0.574765</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# if not os.path.exists(FINAL_LORA_ADAPTERS):\n",
    "# train model\n",
    "trainer.train()\n",
    "# # store final model parameters\n",
    "model.save_pretrained(FINAL_LORA_ADAPTERS)\n",
    "\n",
    "# #TODO: not storing this properly I suppose, need to change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reload saved LoRA adapter for inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at nlpaueb/legal-bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "base_model_test = AutoModelForSequenceClassification.from_pretrained(MODEL, num_labels=1)\n",
    "new_model = PeftModel.from_pretrained(base_model_test, FINAL_LORA_ADAPTERS)\n",
    "# new_model = new_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer with new model\n",
    "trainer = CustomRewardTrainer(\n",
    "    model=new_model,\n",
    "    args=training_args,\n",
    "    train_dataset=final_splits['train'],\n",
    "    eval_dataset=final_splits['validation'],\n",
    "    # compute_metrics=trainer.compute_metrics,  # Use the custom metrics function\n",
    "    processing_class=tokenizer,\n",
    "    loss_type=\"huber\",  # \"mse\" or \"huber\"\n",
    "    weight_strategy=\"linear\",  # \"linear\", \"inverse\", or None\n",
    "    # callbacks=[EarlyStoppingCallback(early_stopping_patience=50)] # use early stopping since we are sing high amount of epochs\n",
    "    # data_collator=RewardDataCollator()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results: {'eval_loss': 0.5358006358146667, 'eval_model_preparation_time': 0.008, 'eval_runtime': 0.2558, 'eval_samples_per_second': 547.295, 'eval_steps_per_second': 35.183}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_results = trainer.evaluate(eval_dataset=final_splits['test'])\n",
    "print(\"Test Results:\", test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1: Predicted Rating: 2.779296875, True Rating: 1\n",
      "Sample 2: Predicted Rating: -0.07684326171875, True Rating: 0\n",
      "Sample 3: Predicted Rating: 2.345703125, True Rating: 3\n",
      "Sample 4: Predicted Rating: -0.010162353515625, True Rating: 0\n",
      "Sample 5: Predicted Rating: 2.9140625, True Rating: 3\n",
      "Sample 6: Predicted Rating: 0.10626220703125, True Rating: 0\n",
      "Sample 7: Predicted Rating: 2.4453125, True Rating: 1\n",
      "Sample 8: Predicted Rating: 0.07196044921875, True Rating: 0\n",
      "Sample 9: Predicted Rating: 0.056854248046875, True Rating: 0\n",
      "Sample 10: Predicted Rating: -0.040679931640625, True Rating: 0\n",
      "Sample 11: Predicted Rating: 0.053863525390625, True Rating: 0\n",
      "Sample 12: Predicted Rating: 0.17333984375, True Rating: 0\n",
      "Sample 13: Predicted Rating: -0.03515625, True Rating: 0\n",
      "Sample 14: Predicted Rating: 1.3251953125, True Rating: 1\n",
      "Sample 15: Predicted Rating: 1.2685546875, True Rating: 0\n",
      "Sample 16: Predicted Rating: 0.18017578125, True Rating: 0\n",
      "Sample 17: Predicted Rating: 2.4453125, True Rating: 3\n",
      "Sample 18: Predicted Rating: 2.87109375, True Rating: 3\n",
      "Sample 19: Predicted Rating: 0.0848388671875, True Rating: 1\n",
      "Sample 20: Predicted Rating: 2.955078125, True Rating: 3\n"
     ]
    }
   ],
   "source": [
    "# evaluate model manually on some test cases\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "#TODO: change tokenization function here!\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(20):\n",
    "        sample = final_splits['test'][i]\n",
    "        inputs = tokenizer(sample['precondition_text'] + \" \" + sample['response_text'], return_tensors='pt', truncation=True, padding=\"max_length\").to(device)\n",
    "        outputs = model(**inputs)\n",
    "        prediction = outputs.logits.item()\n",
    "        print(f\"Sample {i+1}: Predicted Rating: {prediction}, True Rating: {sample['label']}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
