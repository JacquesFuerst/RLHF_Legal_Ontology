{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tune reward model from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODOs:\n",
    "\n",
    "#TODO: double-check that labels are not somehow misaligned...\n",
    "\n",
    "#TODO: check if you need to plot \n",
    "\n",
    "1. LoRA learns the position of the low rank adaptation matrix that is needed to finetune a model of a much higher rank\n",
    "\n",
    "#TODO: double check model performance, generate output, maybe adjust training metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports, setup, and global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "CUDA is available. Using GPU: NVIDIA L40S\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "from transformers import TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "from datasets import Dataset, DatasetDict, load_from_disk\n",
    "\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "\n",
    "from utils import parse_ratings\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "# from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# load the relevant devices available on the server\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = os.getenv(\"AVAILABLE_DEVICES\")\n",
    "\n",
    "# Enable expandable CUDA segments\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# load cuda\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print(\"CUDA is available. Using GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA is not available. Using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training variables\n",
    "FEEDBACK_TO_TRAIN_ON = os.getenv(\"FEEDBACK_TO_TRAIN_ON\")\n",
    "FEEDBACK_TO_REMOVE = os.getenv(\"FEEDBACK_TO_REMOVE\")\n",
    "MODEL = os.getenv(\"REWARD_MODEL\")\n",
    "LORA_CHECKPOINTS_FOLDER = os.getenv(\"LORA_CHECKPOINTS_FOLDER\")\n",
    "FINAL_LORA_ADAPTERS = os.getenv(\"FINAL_LORA_ADAPTERS_FOLDER\") + \"_\" + FEEDBACK_TO_TRAIN_ON\n",
    "\n",
    "# load training data\n",
    "FILE_1 = os.getenv(\"FILE_1\")\n",
    "FILE_5 = os.getenv(\"FILE_5\")\n",
    "FILE_7 = os.getenv(\"FILE_7\")\n",
    "FILE_9 = os.getenv(\"FILE_9\")\n",
    "FILE_10_1 = os.getenv(\"FILE_10_1\")\n",
    "FILE_10_2 = os.getenv(\"FILE_10_2\")\n",
    "FILE_SYNTH = os.getenv(\"FILE_SYNTH\")\n",
    "\n",
    "MAX_LENGTH = 512\n",
    "STRIDE = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset loading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataframes\n",
    "df_1 = pd.read_csv(FILE_1, sep=\";\")\n",
    "df_5 = pd.read_csv(FILE_5, sep=\";\")\n",
    "df_7 = pd.read_csv(FILE_7, sep=\";\")\n",
    "df_9 = pd.read_csv(FILE_9, sep=\";\")\n",
    "df_10_1 = pd.read_csv(FILE_10_1, sep=\";\")\n",
    "df_10_2 = pd.read_csv(FILE_10_2, sep=\";\")\n",
    "df_synth = pd.read_csv(FILE_SYNTH, sep=\";\")\n",
    "\n",
    "df_human = pd.concat([df_1, df_5, df_7, df_9, df_10_1, df_10_2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(929, 13)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = df_human\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. a) Parse ratings to numeric values for MSE Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed feedback for extraction: 0    2\n",
      "1    2\n",
      "2    2\n",
      "3    2\n",
      "4    3\n",
      "Name: feedback_extraction, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df_train[FEEDBACK_TO_TRAIN_ON] = [parse_ratings(feedback) for feedback in df_train[FEEDBACK_TO_TRAIN_ON]]\n",
    "print(\"Parsed feedback for extraction:\", df_train[FEEDBACK_TO_TRAIN_ON][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. b) keep only relevant feedback column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['file', 'frame_ID', 'frame_type', 'frame_text', 'precondition_id', 'precondition_text', 'precondition_position', 'response_text', 'prompt_config_examples', 'prompt_config_chain_of_thought', 'feedback_extraction', 'feedback_detection', 'additional_feedback'],\n",
      "    num_rows: 929\n",
      "})\n",
      "feedback_extraction\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset.from_pandas(df_train)\n",
    "\n",
    "print(dataset)\n",
    "print(FEEDBACK_TO_TRAIN_ON) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.remove_columns([FEEDBACK_TO_REMOVE])\n",
    "dataset = dataset.rename_column(FEEDBACK_TO_TRAIN_ON, \"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load model with LoRA layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at nlpaueb/legal-bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSdpaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Load the model and the tokenizer\n",
    "model_id = MODEL \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_id, num_labels=1) # num_labels = 1 since we want to prodict a single scalar (the rating)\n",
    "\n",
    "# Comment: Automodel for sequence classification with num_labels=1 already has a regression head\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n",
      "512\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.model_max_length)\n",
    "print(model.config.max_position_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 443,137 || all params: 109,926,146 || trainable%: 0.4031\n"
     ]
    }
   ],
   "source": [
    "# Define LoRA config\n",
    "lora_config = LoraConfig(\n",
    "    r=8,           # Rank of the LoRA matrices (smaller = less memory)\n",
    "    lora_alpha=16, # Scaling factor (higher = stronger adaptation)\n",
    "    target_modules=[\"query\", \"key\", \"value\"], # Apply LoRA to attention layers\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_CLS\"  # classify each answer \n",
    ")\n",
    "\n",
    "# Convert the model to a PEFT (LoRA) model\n",
    "model = get_peft_model(model, lora_config)\n",
    "# model.gradient_checkpointing_enable()\n",
    "model.print_trainable_parameters()  # Check trainable params (~0.1% of full model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 1067, 223, 207, 580, 210, 1335, 124, 102, 0, 0, 0], [101, 1067, 223, 207, 5601, 190, 580, 213, 207, 1727, 124, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test tokenizer\n",
    "sample_data = [\"What is the capital of France?\", \"What is the largest capital in the world?\"]\n",
    "tokenizer(sample_data, padding=True, truncation=True, max_length=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Custom Trainer to be used for the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRewardTrainer(Trainer):\n",
    "    def __init__(self, *args, loss_type=\"mse\", weight_strategy=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.loss_type = loss_type  # \"mse\", \"huber\", or custom\n",
    "        self.weight_strategy = weight_strategy  # \"linear\", \"inverse\", or None\n",
    "\n",
    "    def compute_loss(self, model, inputs, num_items_in_batch=None, return_outputs=False):\n",
    "        # Extract labels (ratings) and optional sample weights\n",
    "        labels = inputs.pop(\"labels\").float()  # Shape: (batch_size)\n",
    "        \n",
    "        # Optional: Compute sample weights dynamically\n",
    "        weights = self._get_sample_weights(labels) if self.weight_strategy else None\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits.squeeze()  # Shape: (batch_size) --> logits are the predicted rewards in this case\n",
    "        \n",
    "        # Custom loss calculation\n",
    "        loss = self._compute_custom_loss(logits, labels, weights)\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "    def _compute_custom_loss(self, logits, labels, weights=None):\n",
    "        if self.loss_type == \"mse\":\n",
    "            loss = F.mse_loss(logits, labels, reduction=\"none\") # --> MSE provides precise regression BUT sensitive to outliers\n",
    "        elif self.loss_type == \"huber\":\n",
    "            loss = F.huber_loss(logits, labels, reduction=\"none\", delta=1.0) #--> balances between MSE and MAE for data that has outliers/ noise\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported loss type: {self.loss_type}\")\n",
    "\n",
    "        # Apply sample weights if provided\n",
    "        if weights is not None:\n",
    "            loss = loss * weights\n",
    "            loss = loss.mean()  # Normalize by mean if weights are unnormalized\n",
    "        else:\n",
    "            loss = loss.mean()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def _get_sample_weights(self, labels):\n",
    "        \"\"\"\n",
    "        Generate sample weights based on rating values.\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        if self.weight_strategy == \"linear\":\n",
    "            # Linear weighting (e.g., emphasize extremes)\n",
    "            weights = torch.abs(labels - labels.mean()) + 1.0\n",
    "        elif self.weight_strategy == \"inverse\":\n",
    "            # Inverse frequency weighting (if ratings are skewed)\n",
    "            unique, counts = torch.unique(labels, return_counts=True)\n",
    "            freq = counts.float() / len(labels)\n",
    "            weight_map = 1.0 / (freq + 1e-6)  # Avoid division by zero\n",
    "            weights = torch.tensor([weight_map[(unique == lbl).nonzero().item()] for lbl in labels])\n",
    "        else:\n",
    "            weights = None\n",
    "        \n",
    "        return weights.to(labels.device) if weights is not None else None\n",
    "\n",
    "\n",
    "\n",
    "    def compute_metrics(self, eval_preds):\n",
    "        predictions, labels = eval_preds\n",
    "        predictions = predictions.squeeze()\n",
    "        \n",
    "        # Regression metrics\n",
    "        mse = mean_squared_error(labels, predictions)\n",
    "        pearson = pearsonr(labels, predictions)[0] # Pearson correlation coefficient\n",
    "        \n",
    "        # Threshold accuracy --> \n",
    "        tolerance_acc = (np.abs(predictions - labels) <= 0.5).mean()\n",
    "        \n",
    "        return {\"mse\": mse, \"pearson\": pearson, \"tolerance_acc\": tolerance_acc}\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    #TODO: evaluate whether the plotting should be done or whether it is redundant to add them\n",
    "\n",
    "    # def evaluation_loop(self, *args, **kwargs):\n",
    "    #     output = super().evaluation_loop(*args, **kwargs)\n",
    "    #     predictions = output.predictions.squeeze()\n",
    "    #     labels = output.label_ids\n",
    "        \n",
    "    #     # Generate plots (saved to disk or logged to W&B)\n",
    "    #     plot_distributions(predictions, labels, self.state.epoch)\n",
    "    #     plot_calibration(predictions, labels)\n",
    "        \n",
    "    #     return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: debug if this is truly needed...\n",
    "\n",
    "# add distributioncallback to trianing to evaluate \n",
    "\n",
    "# class DistributionCallback(TrainerCallback):\n",
    "#     def on_evaluate(self, args, state, control, metrics, **kwargs):\n",
    "#         # Get predictions and labels from the trainer's eval loop\n",
    "#         eval_results = trainer.evaluate()\n",
    "#         predictions = eval_results[\"eval_predictions\"]\n",
    "#         labels = eval_results[\"eval_labels\"]\n",
    "        \n",
    "#         # Log histogram to W&B\n",
    "#         wandb.log({\n",
    "#             \"reward_histogram\": wandb.Histogram(predictions),\n",
    "#             \"true_ratings_histogram\": wandb.Histogram(labels),\n",
    "#         })\n",
    "\n",
    "\n",
    "# def plot_distributions(predictions, labels, epoch):\n",
    "#     plt.figure(figsize=(10, 4))\n",
    "#     plt.subplot(1, 2, 1)\n",
    "#     plt.hist(predictions, bins=20, alpha=0.7, label=\"Predicted\")\n",
    "#     plt.title(\"Predicted Rewards\")\n",
    "    \n",
    "#     plt.subplot(1, 2, 2)\n",
    "#     plt.hist(labels, bins=20, alpha=0.7, label=\"True Ratings\", color=\"orange\")\n",
    "#     plt.title(\"True Ratings\")\n",
    "    \n",
    "#     plt.savefig(f\"distributions_epoch_{epoch}.png\")\n",
    "#     plt.close()\n",
    "\n",
    "# class PlotCallback(TrainerCallback):\n",
    "#     def on_evaluate(self, args, state, control, **kwargs):\n",
    "#         predictions = trainer.predict(test_dataset).predictions.squeeze()\n",
    "#         labels = test_dataset[\"ratings\"]\n",
    "#         plot_distributions(predictions, labels, state.epoch)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def plot_calibration(predictions, labels):\n",
    "#     \"\"\"\n",
    "#     Function to check if the sdfs\n",
    "\n",
    "    \n",
    "\n",
    "#     \"\"\"\n",
    "#     bin_means = np.linspace(1, 5, num=5)  # For 1-5 ratings\n",
    "#     bin_centers = []\n",
    "#     empirical_means = []\n",
    "    \n",
    "#     for i in range(len(bin_means) - 1):\n",
    "#         mask = (labels >= bin_means[i]) & (labels < bin_means[i+1])\n",
    "#         if mask.sum() > 0:\n",
    "#             bin_centers.append((bin_means[i] + bin_means[i+1]) / 2)\n",
    "#             empirical_means.append(predictions[mask].mean())\n",
    "    \n",
    "#     plt.plot(bin_centers, empirical_means, marker=\"o\")\n",
    "#     plt.plot([1, 5], [1, 5], linestyle=\"--\", color=\"gray\")  # Ideal line\n",
    "#     plt.xlabel(\"True Rating\")\n",
    "#     plt.ylabel(\"Predicted Reward\")\n",
    "#     plt.savefig(\"calibration_plot.png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. a) Define a custom data collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import DefaultDataCollator\n",
    "\n",
    "# #TODO: only do this if the labels fix does not work for some reason\n",
    "\n",
    "# class RewardDataCollator(DefaultDataCollator):\n",
    "#     def __call__(self, features):\n",
    "\n",
    "#         ratings = [f.pop(\"rating\") for f in features]  # Removes rating from features temporarily\n",
    "#         batch = super().__call__(features)\n",
    "#         # Explicitly ensure rating is included\n",
    "#         print(features)\n",
    "#         # Re-inject ratings into the batch\n",
    "#         batch[\"rating\"] = torch.tensor(ratings, dtype=torch.float32)\n",
    "#         return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Encode dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['file', 'frame_ID', 'frame_type', 'frame_text', 'precondition_id', 'precondition_text', 'precondition_position', 'response_text', 'prompt_config_examples', 'prompt_config_chain_of_thought', 'label', 'additional_feedback']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 929/929 [00:00<00:00, 4253.68 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2, 2, 2, 3]\n",
      "['1. Subfact: Onze Minister\\n                2. Positie: Artikel 1, sectie 1 IN Vreemdelingenwet geldig vanaf 2024\\n                3. Subfact: Onze Minister\\n                4. Positie: Artikel 8, sectie 1 IN Vreemdelingenwet geldig vanaf 2024\\n                5. Subfact: Onze Minister\\n                6. Positie: Artikel 14, sectie 1 IN Vreemdelingenwet geldig vanaf 2024\\n                7. Subfact: Onze Minister\\n                8. Positie: Artikel 16, sectie 1 IN Vreemdelingenwet geldig vanaf 2024\\n                9. Subfact: Onze Minister\\n                10. Positie: Artikel 17, sectie 1 IN Vreemdelingenwet geldig vanaf 2024\\n                11. Subfact: Onze Minister\\n                12. Positie: Artikel 17a, sectie 1 IN Vreemdelingenwet geldig vanaf 2024\\n                13. Subfact: Onze Minister\\n                14. Positie: Artikel 26, sectie 1 IN Vreemdelingenwet geldig vanaf 2024\\n                15. Subfact: Onze Minister\\n                16. Positie: Artikel 27, sectie 1 IN Vreemdelingenwet geldig vanaf 2024\\n                17. Subfact: Onze Minister\\n                18. Positie: Artikel 66a, sectie 6 IN Vreemdelingenwet geldig vanaf 2024\\n                19. Subfact: Onze Minister\\n                20. Positie: Artikel 67, sectie 3 IN Vreemdelingenwet geldig vanaf 2024\\n', '1. Subfact: Onze Minister\\n                2. Positie: Artikel 1, sectie 1 IN Vreemdelingenwet geldig vanaf 2024\\n                3. Subfact: Onze Minister\\n                4. Positie: Artikel 8, sectie 1 IN Vreemdelingenwet geldig vanaf 2024\\n                5. Subfact: Onze Minister\\n                6. Positie: Artikel 14, sectie 1 IN Vreemdelingenwet geldig vanaf 2024\\n                7. Subfact: Onze Minister\\n                8. Positie: Artikel 16, sectie 1 IN Vreemdelingenwet geldig vanaf 2024\\n                9. Subfact: Onze Minister\\n                10. Positie: Artikel 17, sectie 1 IN Vreemdelingenwet geldig vanaf 2024\\n                11. Subfact: Onze Minister\\n                12. Positie: Artikel 17a, sectie 1 IN Vreemdelingenwet geldig vanaf 2024\\n                13. Subfact: Onze Minister\\n                14. Positie: Artikel 26, sectie 1 IN Vreemdelingenwet geldig vanaf 2024\\n                15. Subfact: Onze Minister\\n                16. Positie: Artikel 27, sectie 1 IN Vreemdelingenwet geldig vanaf 2024\\n                17. Subfact: Onze Minister\\n                18. Positie: Artikel 66a, sectie 6 IN Vreemdelingenwet geldig vanaf 2024\\n                19. Subfact: Onze Minister\\n                20. Positie: Artikel 67, sectie 3 IN Vreemdelingenwet geldig vanaf 2024\\n', '1. Subfact: Onze Minister\\n                2. Positie: Artikel 1, sectie 1 IN Vreemdelingenwet geldig vanaf 2024\\n                3. Subfact: Onze Minister\\n                4. Positie: Artikel 8, sectie 1 IN Vreemdelingenwet geldig vanaf 2024\\n                5. Subfact: Onze Minister\\n                6. Positie: Artikel 14, sectie 1 IN Vreemdelingenwet geldig vanaf 2024\\n                7. Subfact: Onze Minister\\n                8. Positie: Artikel 16, sectie 1 IN Vreemdelingenwet geldig vanaf 2024\\n                9. Subfact: Onze Minister\\n                10. Positie: Artikel 17, sectie 1 IN Vreemdelingenwet geldig vanaf 2024\\n                11. Subfact: Onze Minister\\n                12. Positie: Artikel 17a, sectie 1 IN Vreemdelingenwet geldig vanaf 2024\\n                13. Subfact: Onze Minister\\n                14. Positie: Artikel 26, sectie 1 IN Vreemdelingenwet geldig vanaf 2024\\n                15. Subfact: Onze Minister\\n                16. Positie: Artikel 27, sectie 1 IN Vreemdelingenwet geldig vanaf 2024\\n                17. Subfact: Onze Minister\\n                18. Positie: Artikel 66a, sectie 6 IN Vreemdelingenwet geldig vanaf 2024\\n                19. Subfact: Onze Minister\\n                20. Positie: Artikel 67, sectie 3 IN Vreemdelingenwet geldig vanaf 2024\\n', '1. Subfact: Onze Minister\\n\\n                2. Positie: Artikel 1, sectie 1 IN Algemene wet bestuursrecht\\n\\n                3. Positie: Artikel 1, sectie 1 IN Vreemdelingenwet geldig vanaf 2024\\n\\n                4. Positie: Artikel 14, 1 IN Vreemdelingenwet geldig vanaf 2024\\n\\n                5. Positie: Artikel 1, sub 2° IN Vreemdelingenwet geldig vanaf 2024\\n\\n                6. Positie: Artikel 1, sub 4° IN Vreemdelingenwet geldig vanaf 2024\\n\\n                7. Positie: Artikel 1, sub 6° IN Vreemdelingenwet geldig vanaf 2024\\n\\n                8. Positie: Artikel 1, sub 2° IN Vreemdelingenwet geldig vanaf 2024, Afdeling 3. De verblijfsvergunning regulier\\n\\n                9. Positie: Artikel 9, 1 IN Vreemdelingenwet geldig vanaf 2024\\n\\n                10. Positie: Artikel 14, 1a IN Vreemdelingenwet geldig vanaf 2024\\n\\n                11. Positie: Artikel 14, 1b IN Vreemdelingenwet geldig vanaf 2024\\n\\n                12. Positie: Artikel 14, 1c IN Vreemdelingenwet geldig vanaf 2024\\n\\n                13. Positie: Artikel 14, 1d IN Vreemdelingenwet geldig vanaf 2024\\n\\n                14. Positie: Artikel 14, 1e IN Vreemdel', '1. Subfact: Onze Minister\\n\\n                2. Positie: Artikel 1, sectie 1 IN Algemene wet bestuursrecht\\n\\n                3. Positie: Artikel 1, sectie 1 IN Vreemdelingenwet geldig vanaf 2024\\n\\n                4. Positie: Artikel 14, 1 IN Vreemdelingenwet geldig vanaf 2024\\n\\n                5. Positie: Artikel 1, sub 2° IN Vreemdelingenwet geldig vanaf 2024\\n\\n                6. Positie: Artikel 1, sub 4° IN Vreemdelingenwet geldig vanaf 2024\\n\\n                7. Positie: Artikel 1, sub 6° IN Vreemdelingenwet geldig vanaf 2024\\n\\n                8. Positie: Artikel 1, sub 2° IN Vreemdelingenwet geldig vanaf 2024, Afdeling 3. De verblijfsvergunning regulier\\n\\n                9. Positie: Artikel 9, 1 IN Vreemdelingenwet geldig vanaf 2024\\n\\n                10. Positie: Artikel 14, 1a IN Vreemdelingenwet geldig vanaf 2024\\n\\n                11. Positie: Artikel 14, 1b IN Vreemdelingenwet geldig vanaf 2024\\n\\n                12. Positie: Artikel 14, 1c IN Vreemdelingenwet geldig vanaf 2024\\n\\n                13. Positie: Artikel 14, 1d IN Vreemdelingenwet geldig vanaf 2024\\n\\n                14. Positie: Artikel 14, 1e IN Vreemdel']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# if labels are not integers, convert them to integers\n",
    "def convert_label_to_int(data):\n",
    "    data[\"label\"] = int(data[\"label\"])\n",
    "    return data\n",
    "\n",
    "\n",
    "print(dataset.column_names)\n",
    "# mao string labels to integers\n",
    "dataset = dataset.map(convert_label_to_int)  # Assuming 'text' is the column with the text data\n",
    "\n",
    "print(dataset[\"label\"][:5])  # Check labels\n",
    "print(dataset[\"response_text\"][:5])  # Check labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comment\n",
    "\n",
    "1. Needed for feedback extraction: precondition_text, response_text, label(rating feedback extraction)\n",
    "2. Needed for feedback detection: precondition_text, precondition_position, response_text, label (rating feedback detection)\n",
    "3. For the precondition position to be found well, it is a crucial for the model to find the precondition text (at least to a recognizable degree) as well, otherwise the precondition is not found at all..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "def find_best_window(long_text, ground_truth, tokenizer, window_size=512, stride=256):\n",
    "    \"\"\"\n",
    "    Find the best sliding window of text that is most similar to the ground truth.\n",
    "    Uses a semantic similarity model to evaluate the windows.\n",
    "    \"\"\"\n",
    "    similarity_model = SentenceTransformer('NetherlandsForensicInstitute/robbert-2022-dutch-sentence-transformers')\n",
    "    similarity_model.to(device)\n",
    "    \n",
    "    # Tokenize the full text\n",
    "    tokens = tokenizer.tokenize(long_text)\n",
    "    \n",
    "    best_score = -1\n",
    "    best_window = None\n",
    "\n",
    "    \n",
    "    # Handle special cases in sequence length\n",
    "    if len(tokens) < window_size:\n",
    "        # Handle short sequences (optional: pad or skip)\n",
    "        start_indices = [0]\n",
    "    elif len(tokens) - window_size + 1 < stride:\n",
    "        # Not enough room for a second stride step\n",
    "        start_indices = [0, len(tokens) - window_size]\n",
    "    else:\n",
    "        # Standard sliding window\n",
    "        start_indices = list(range(0, len(tokens) - window_size + 1, stride))\n",
    "\n",
    "    \n",
    "    # Create sliding windows\n",
    "    for start in start_indices:\n",
    "        window_tokens = tokens[start:start + window_size]\n",
    "        window_text = tokenizer.convert_tokens_to_string(window_tokens)\n",
    "        \n",
    "        # Calculate cosine similarity with ground truth\n",
    "        embeddings = similarity_model.encode([window_text, ground_truth])\n",
    "        similarity = np.dot(embeddings[0], embeddings[1]) / (\n",
    "            np.linalg.norm(embeddings[0]) * np.linalg.norm(embeddings[1])\n",
    "        )\n",
    "        \n",
    "        if similarity > best_score:\n",
    "            best_score = similarity\n",
    "            best_window = window_tokens\n",
    "\n",
    "    # print(best_window)\n",
    "    if best_window == None:\n",
    "        print(similarity)\n",
    "        print(ground_truth)\n",
    "        print(long_text)\n",
    "    \n",
    "    return tokenizer.convert_tokens_to_string(best_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/929 [00:00<?, ? examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (900 > 512). Running this sequence through the model will result in indexing errors\n",
      "Map:   4%|▍         | 37/929 [00:25<10:15,  1.45 examples/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 73\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenized\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# sample = dataset.select(range(5))\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# tokenized_sample = tokenize_fn(sample)\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# print([len(ids) for ids in tokenized_sample[\"input_ids\"]])\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenize_fn_with_best_window\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/RL/lib/python3.10/site-packages/datasets/arrow_dataset.py:557\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    552\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    555\u001b[0m }\n\u001b[1;32m    556\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 557\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    558\u001b[0m datasets: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    559\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/RL/lib/python3.10/site-packages/datasets/arrow_dataset.py:3079\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc, try_original_type)\u001b[0m\n\u001b[1;32m   3073\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3074\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[1;32m   3075\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3076\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[1;32m   3077\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3078\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3079\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39m_map_single(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdataset_kwargs):\n\u001b[1;32m   3080\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m   3081\u001b[0m                 shards_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/RL/lib/python3.10/site-packages/datasets/arrow_dataset.py:3501\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, try_original_type)\u001b[0m\n\u001b[1;32m   3499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m batched:\n\u001b[1;32m   3500\u001b[0m     _time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m-> 3501\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, example \u001b[38;5;129;01min\u001b[39;00m iter_outputs(shard_iterable):\n\u001b[1;32m   3502\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m update_data:\n\u001b[1;32m   3503\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/RL/lib/python3.10/site-packages/datasets/arrow_dataset.py:3475\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.iter_outputs\u001b[0;34m(shard_iterable)\u001b[0m\n\u001b[1;32m   3473\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3474\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, example \u001b[38;5;129;01min\u001b[39;00m shard_iterable:\n\u001b[0;32m-> 3475\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m i, \u001b[43mapply_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/RL/lib/python3.10/site-packages/datasets/arrow_dataset.py:3398\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function\u001b[0;34m(pa_inputs, indices, offset)\u001b[0m\n\u001b[1;32m   3396\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Utility to apply the function on a selection of columns.\"\"\"\u001b[39;00m\n\u001b[1;32m   3397\u001b[0m inputs, fn_args, additional_args, fn_kwargs \u001b[38;5;241m=\u001b[39m prepare_inputs(pa_inputs, indices, offset\u001b[38;5;241m=\u001b[39moffset)\n\u001b[0;32m-> 3398\u001b[0m processed_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3399\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m prepare_outputs(pa_inputs, inputs, processed_inputs)\n",
      "Cell \u001b[0;32mIn[25], line 32\u001b[0m, in \u001b[0;36mtokenize_fn_with_best_window\u001b[0;34m(examples)\u001b[0m\n\u001b[1;32m     30\u001b[0m stride \u001b[38;5;241m=\u001b[39m window_size \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     31\u001b[0m ground_truth \u001b[38;5;241m=\u001b[39m precon_text\n\u001b[0;32m---> 32\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[43mfind_best_window\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mground_truth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwindow_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# print(text)\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# combined_texts = [f\"{t} {r}\" for t, r in zip(examples[\"precondition_text\"], text)] --> batched version\u001b[39;00m\n\u001b[1;32m     35\u001b[0m combined_texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprecon_text\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprecon_pos\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[24], line 9\u001b[0m, in \u001b[0;36mfind_best_window\u001b[0;34m(long_text, ground_truth, tokenizer, window_size, stride)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfind_best_window\u001b[39m(long_text, ground_truth, tokenizer, window_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m, stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m):\n\u001b[1;32m      5\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03m    Find the best sliding window of text that is most similar to the ground truth.\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03m    Uses a semantic similarity model to evaluate the windows.\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m     similarity_model \u001b[38;5;241m=\u001b[39m \u001b[43mSentenceTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mNetherlandsForensicInstitute/robbert-2022-dutch-sentence-transformers\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     similarity_model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# Tokenize the full text\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/RL/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:348\u001b[0m, in \u001b[0;36mSentenceTransformer.__init__\u001b[0;34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, similarity_fn_name, cache_folder, trust_remote_code, revision, local_files_only, token, use_auth_token, truncate_dim, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data, backend)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    346\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 348\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_hpu_graph_enabled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_prompt_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_prompt_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompts:\n",
      "File \u001b[0;32m~/miniconda3/envs/RL/lib/python3.10/site-packages/torch/nn/modules/module.py:1343\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1340\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1341\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1343\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/RL/lib/python3.10/site-packages/torch/nn/modules/module.py:903\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 903\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    906\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    907\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    908\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    913\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    914\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/RL/lib/python3.10/site-packages/torch/nn/modules/module.py:903\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 903\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    906\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    907\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    908\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    913\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    914\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 903 (4 times)]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/RL/lib/python3.10/site-packages/torch/nn/modules/module.py:903\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 903\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    906\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    907\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    908\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    913\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    914\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/RL/lib/python3.10/site-packages/torch/nn/modules/module.py:930\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    927\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    928\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    929\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 930\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    931\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    933\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/RL/lib/python3.10/site-packages/torch/nn/modules/module.py:1329\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1323\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1324\u001b[0m             device,\n\u001b[1;32m   1325\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1326\u001b[0m             non_blocking,\n\u001b[1;32m   1327\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1328\u001b[0m         )\n\u001b[0;32m-> 1329\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1333\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1334\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1335\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# tokenize queries and answers together to provide proper context to reward model\n",
    "def tokenize_fn_with_best_window(examples):\n",
    "\n",
    "    \"\"\"\n",
    "    Tokenization function choosing the best window in the answer using similarity score with ground truth.\n",
    "    \"\"\"\n",
    "\n",
    "    max_length = MAX_LENGTH\n",
    "    stride = STRIDE\n",
    "\n",
    "    #TODO: need to replace the tokenize function here, get the number fo tokens properly\n",
    "    response_tokens = len(tokenizer(examples[\"response_text\"], truncation=False)[0])\n",
    "    # print(response_tokens)\n",
    "    precond_tokens = len(tokenizer(examples[\"precondition_text\"], truncation=False)[0])\n",
    "    # print(precond_tokens)\n",
    "    pos_tokens = len(tokenizer(examples[\"precondition_position\"], truncation=False)[0])\n",
    "    # print(pos_tokens)\n",
    "\n",
    "    precon_text = examples[\"precondition_text\"]\n",
    "    precon_pos = examples[\"precondition_position\"]\n",
    "    response = examples[\"response_text\"]\n",
    "\n",
    "    # combine the text for each separate use case\n",
    "    if FEEDBACK_TO_TRAIN_ON == \"feedback_extraction\":\n",
    "        # print(\"Tokenizing for feedback extraction\")\n",
    "        # choose the best sequence if the number of tokens is more than 512\n",
    "        num_tokens = response_tokens + precond_tokens\n",
    "        if num_tokens > max_length:\n",
    "            window_size = max_length - precond_tokens\n",
    "            stride = window_size // 2\n",
    "            ground_truth = precon_text\n",
    "            text = find_best_window(response, ground_truth, tokenizer, window_size=window_size, stride=stride)\n",
    "            # print(text)\n",
    "            # combined_texts = [f\"{t} {r}\" for t, r in zip(examples[\"precondition_text\"], text)] --> batched version\n",
    "            combined_texts = f\"{precon_text} {precon_pos} {text}\"\n",
    "        else:\n",
    "            # combined_texts = [f\"{t} {r}\" for t, r in zip(examples[\"precondition_text\"], examples[\"response_text\"])] --> batched version\n",
    "            combined_texts = f\"{precon_text} {precon_pos} {response}\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    elif FEEDBACK_TO_TRAIN_ON == \"feedback_detection\":\n",
    "        # print(\"Tokenizing for feedback detection\")\n",
    "        num_tokens = response_tokens + precond_tokens + pos_tokens\n",
    "        # print(num_tokens)\n",
    "        # print(max_length)\n",
    "        # print(examples[\"response_text\"])\n",
    "        if num_tokens > max_length:\n",
    "            # print(\"We get here at all\")\n",
    "            window_size = max_length - (precond_tokens + pos_tokens)\n",
    "            stride = window_size // 2\n",
    "            ground_truth = precon_text + \" \" + precon_pos\n",
    "            text = find_best_window(response, ground_truth, tokenizer, window_size=window_size, stride=stride)\n",
    "\n",
    "            #combined_texts = [f\"{t} {p} {r}\" for t, p, r in zip(examples[\"precondition_text\"], examples[\"precondition_position\"], text)] --> batched version\n",
    "            \n",
    "            combined_texts = f\"{precon_text} {precon_pos} {text}\"\n",
    "        else:\n",
    "            #combined_texts = [f\"{t} {p} {r}\" for t, p, r in zip(examples[\"precondition_text\"], examples[\"precondition_position\"], examples[\"response_text\"])] --> batched version\n",
    "            combined_texts = f\"{precon_text} {precon_pos} {response}\"\n",
    "\n",
    "    tokenized = tokenizer(combined_texts, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "\n",
    "# sample = dataset.select(range(5))\n",
    "# tokenized_sample = tokenize_fn(sample)\n",
    "# print([len(ids) for ids in tokenized_sample[\"input_ids\"]])\n",
    "\n",
    "dataset = dataset.map(tokenize_fn_with_best_window, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 929/929 [00:00<00:00, 162280.14 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Assuming `encoded_dataset` is a Hugging Face Dataset object\n",
    "dataset.save_to_disk(f\"/home/jacques.furst/development/RAG/flintfiller-precondition-rl/training_files/tokenized_data_reward_model_{FEEDBACK_TO_TRAIN_ON}\")\n",
    "dataset = load_from_disk(f\"/home/jacques.furst/development/RAG/flintfiller-precondition-rl/training_files/tokenized_data_reward_model_{FEEDBACK_TO_TRAIN_ON}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split dataset into train, test, eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train, test and eval sets\n",
    "train_test_split = dataset.train_test_split(test_size=0.3, seed=42)\n",
    "eval_test_split = train_test_split[\"test\"].train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "\n",
    "final_splits = DatasetDict({\n",
    "    'train': train_test_split['train'],\n",
    "    'validation': eval_test_split['train'],\n",
    "    'test': eval_test_split['test']\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train reward model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=LORA_CHECKPOINTS_FOLDER,\n",
    "    eval_strategy='steps',\n",
    "    save_strategy='steps',\n",
    "    save_steps=100,\n",
    "    eval_steps=100,\n",
    "    save_total_limit=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    learning_rate=3e-4,\n",
    "    num_train_epochs=20,\n",
    "    logging_steps=10,\n",
    "    label_names=[\"labels\"],\n",
    "    # report_to=\"none\",\n",
    "    logging_dir=\"./logs\",\n",
    "    fp16=True,  # Use mixed precision training\n",
    "    metric_for_best_model=\"eval_loss\", # or \"eval_loss\"\n",
    "    greater_is_better=False, # False if using loss\n",
    "    # gradient_accumulation_steps=4 # \n",
    ")\n",
    "\n",
    "# Initialize custom trainer\n",
    "trainer = CustomRewardTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=final_splits['train'],\n",
    "    eval_dataset=final_splits['validation'],\n",
    "    # compute_metrics=trainer.compute_metrics,  # Use the custom metrics function\n",
    "    processing_class=tokenizer,\n",
    "    loss_type=\"huber\",  # \"mse\" or \"huber\"\n",
    "    weight_strategy=\"linear\",  # \"linear\", \"inverse\", or None\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)] # use early stopping since we are sing high amount of epochs\n",
    "    # data_collator=RewardDataCollator()\n",
    ")\n",
    "\n",
    "# # add distributioncallback to trainer TODO: only integrate if relevant\n",
    "# trainer.add_callback(DistributionCallback())\n",
    "\n",
    "print(trainer.args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='800' max='820' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [800/820 01:00 < 00:01, 13.28 it/s, Epoch 19/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.488800</td>\n",
       "      <td>0.435845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.414100</td>\n",
       "      <td>0.618522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.358900</td>\n",
       "      <td>0.293196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.178100</td>\n",
       "      <td>0.272691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.267500</td>\n",
       "      <td>0.264794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.204800</td>\n",
       "      <td>0.285815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.230100</td>\n",
       "      <td>0.289056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.282800</td>\n",
       "      <td>0.285740</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=800, training_loss=0.5441169528663159, metrics={'train_runtime': 60.1869, 'train_samples_per_second': 215.994, 'train_steps_per_second': 13.624, 'total_flos': 3355066545033216.0, 'train_loss': 0.5441169528663159, 'epoch': 19.51219512195122})"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store final model parameters\n",
    "model.save_pretrained(FINAL_LORA_ADAPTERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reload saved LoRA adapter for inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at nlpaueb/legal-bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "base_model = AutoModelForSequenceClassification.from_pretrained(MODEL, num_labels=1)\n",
    "model = PeftModel.from_pretrained(base_model, FINAL_LORA_ADAPTERS)\n",
    "model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results: {'eval_loss': 0.40426239371299744, 'eval_runtime': 0.2921, 'eval_samples_per_second': 479.328, 'eval_steps_per_second': 30.814, 'epoch': 19.51219512195122}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_results = trainer.evaluate(eval_dataset=final_splits['test'])\n",
    "print(\"Test Results:\", test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1: Predicted Rating: 4.385091781616211, True Rating: 6\n",
      "Sample 2: Predicted Rating: 6.064860820770264, True Rating: 6\n",
      "Sample 3: Predicted Rating: 5.961627960205078, True Rating: 4\n",
      "Sample 4: Predicted Rating: 6.023962497711182, True Rating: 6\n",
      "Sample 5: Predicted Rating: 4.425707817077637, True Rating: 4\n",
      "Sample 6: Predicted Rating: 6.039861679077148, True Rating: 6\n",
      "Sample 7: Predicted Rating: 4.0829291343688965, True Rating: 4\n",
      "Sample 8: Predicted Rating: 6.074625015258789, True Rating: 6\n",
      "Sample 9: Predicted Rating: 6.0770649909973145, True Rating: 6\n",
      "Sample 10: Predicted Rating: 6.0072479248046875, True Rating: 6\n",
      "Sample 11: Predicted Rating: 5.0286335945129395, True Rating: 6\n",
      "Sample 12: Predicted Rating: 5.976753234863281, True Rating: 6\n",
      "Sample 13: Predicted Rating: 6.047481536865234, True Rating: 6\n",
      "Sample 14: Predicted Rating: 6.014069557189941, True Rating: 6\n",
      "Sample 15: Predicted Rating: 4.109388828277588, True Rating: 6\n",
      "Sample 16: Predicted Rating: 5.910253047943115, True Rating: 6\n",
      "Sample 17: Predicted Rating: 4.0829291343688965, True Rating: 4\n",
      "Sample 18: Predicted Rating: 4.109566688537598, True Rating: 4\n",
      "Sample 19: Predicted Rating: 5.968040466308594, True Rating: 4\n",
      "Sample 20: Predicted Rating: 5.605971336364746, True Rating: 4\n"
     ]
    }
   ],
   "source": [
    "# evaluate model manually on some test cases\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(20):\n",
    "        sample = final_splits['test'][i]\n",
    "        inputs = tokenizer(sample['precondition_text'] + \" \" + sample['response_text'], return_tensors='pt', truncation=True, padding=\"max_length\").to(device)\n",
    "        outputs = model(**inputs)\n",
    "        prediction = outputs.logits.item()\n",
    "        print(f\"Sample {i+1}: Predicted Rating: {prediction}, True Rating: {sample['label']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feedback_detection\n"
     ]
    }
   ],
   "source": [
    "print(FEEDBACK_TO_TRAIN_ON) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
