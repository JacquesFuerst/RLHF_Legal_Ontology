{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tune reward model from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODOs:\n",
    "\n",
    "#TODO: double-check that labels are not somehow misaligned...\n",
    "\n",
    "#TODO: check if you need to plot \n",
    "\n",
    "1. LoRA learns the position of the low rank adaptation matrix that is needed to finetune a model of a much higher rank\n",
    "\n",
    "#TODO: double check model performance, generate output, maybe adjust training metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports, setup, and global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacques.furst/miniconda3/envs/RL/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "CUDA is available. Using GPU: NVIDIA L40S\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "from transformers import TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "\n",
    "from utils import parse_ratings\n",
    "\n",
    "# load the relevant devices available on the server\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = os.getenv(\"AVAILABLE_DEVICES\")\n",
    "\n",
    "# Enable expandable CUDA segments\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# load cuda\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print(\"CUDA is available. Using GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA is not available. Using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training variables\n",
    "FEEDBACK_TO_TRAIN_ON = os.getenv(\"FEEDBACK_TO_TRAIN_ON\")\n",
    "FEEDBACK_TO_REMOVE = os.getenv(\"FEEDBACK_TO_REMOVE\")\n",
    "MODEL = os.getenv(\"REWARD_MODEL\")\n",
    "LORA_CHECKPOINTS_FOLDER = os.getenv(\"LORA_CHECKPOINTS_FOLDER\")\n",
    "FINAL_LORA_ADAPTERS = os.getenv(\"FINAL_LORA_ADAPTERS_FOLDER\") + \"_\" + FEEDBACK_TO_TRAIN_ON\n",
    "\n",
    "# load training data\n",
    "FILE_1 = os.getenv(\"FILE_1\")\n",
    "FILE_5 = os.getenv(\"FILE_5\")\n",
    "FILE_7 = os.getenv(\"FILE_7\")\n",
    "FILE_9 = os.getenv(\"FILE_9\")\n",
    "FILE_10_1 = os.getenv(\"FILE_10_1\")\n",
    "FILE_10_2 = os.getenv(\"FILE_10_2\")\n",
    "FILE_SYNTH = os.getenv(\"FILE_SYNTH\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset loading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataframes\n",
    "df_1 = pd.read_csv(FILE_1, sep=\";\")\n",
    "df_5 = pd.read_csv(FILE_5, sep=\";\")\n",
    "df_7 = pd.read_csv(FILE_7, sep=\";\")\n",
    "df_9 = pd.read_csv(FILE_9, sep=\";\")\n",
    "df_10_1 = pd.read_csv(FILE_10_1, sep=\";\")\n",
    "df_10_2 = pd.read_csv(FILE_10_2, sep=\";\")\n",
    "df_synth = pd.read_csv(FILE_SYNTH, sep=\";\")\n",
    "\n",
    "df_human = pd.concat([df_1, df_5, df_7, df_9, df_10_1, df_10_2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(929, 13)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = df_human\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. a) Parse ratings to numeric values for MSE Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed feedback for extraction: 0    2\n",
      "1    2\n",
      "2    2\n",
      "3    2\n",
      "4    3\n",
      "Name: feedback_extraction, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df_train[FEEDBACK_TO_TRAIN_ON] = [parse_ratings(feedback) for feedback in df_train[FEEDBACK_TO_TRAIN_ON]]\n",
    "print(\"Parsed feedback for extraction:\", df_train[FEEDBACK_TO_TRAIN_ON][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. b) keep only relevant feedback column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['file', 'frame_ID', 'frame_type', 'frame_text', 'precondition_id', 'precondition_text', 'precondition_position', 'response_text', 'prompt_config_examples', 'prompt_config_chain_of_thought', 'feedback_extraction', 'feedback_detection', 'additional_feedback'],\n",
      "    num_rows: 929\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset.from_pandas(df_train)\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.remove_columns([FEEDBACK_TO_REMOVE])\n",
    "dataset = dataset.rename_column(FEEDBACK_TO_TRAIN_ON, \"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load model with LoRA layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LongformerForSequenceClassification(\n",
      "  (longformer): LongformerModel(\n",
      "    (embeddings): LongformerEmbeddings(\n",
      "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
      "      (token_type_embeddings): Embedding(1, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (position_embeddings): Embedding(4098, 768, padding_idx=1)\n",
      "    )\n",
      "    (encoder): LongformerEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x LongformerLayer(\n",
      "          (attention): LongformerAttention(\n",
      "            (self): LongformerSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
      "            )\n",
      "            (output): LongformerSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): LongformerIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): LongformerOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): LongformerClassificationHead(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (out_proj): Linear(in_features=768, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Load the model and the tokenizer\n",
    "model_id = MODEL \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_id, num_labels=1) # num_labels = 1 since we want to prodict a single scalar (the rating)\n",
    "\n",
    "# Comment: Automodel for sequence classification with num_labels=1 already has a regression head\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000000000000019884624838656\n",
      "4098\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.model_max_length)\n",
    "print(model.config.max_position_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,033,729 || all params: 149,693,954 || trainable%: 0.6906\n"
     ]
    }
   ],
   "source": [
    "# Define LoRA config\n",
    "lora_config = LoraConfig(\n",
    "    r=8,           # Rank of the LoRA matrices (smaller = less memory)\n",
    "    lora_alpha=16, # Scaling factor (higher = stronger adaptation)\n",
    "    target_modules=[\"query\", \"key\", \"value\"], # Apply LoRA to attention layers\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_CLS\"  # classify each answer \n",
    ")\n",
    "\n",
    "# Convert the model to a PEFT (LoRA) model\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.gradient_checkpointing_enable()\n",
    "model.print_trainable_parameters()  # Check trainable params (~0.1% of full model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[0, 2264, 16, 5, 812, 9, 1470, 116, 2, 1, 1], [0, 2264, 16, 5, 1154, 812, 11, 5, 232, 116, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test tokenizer\n",
    "sample_data = [\"What is the capital of France?\", \"What is the largest capital in the world?\"]\n",
    "tokenizer(sample_data, padding=True, truncation=True, max_length=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Custom Trainer to be used for the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRewardTrainer(Trainer):\n",
    "    def __init__(self, *args, loss_type=\"mse\", weight_strategy=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.loss_type = loss_type  # \"mse\", \"huber\", or custom\n",
    "        self.weight_strategy = weight_strategy  # \"linear\", \"inverse\", or None\n",
    "\n",
    "    def compute_loss(self, model, inputs, num_items_in_batch=None, return_outputs=False):\n",
    "        # Extract labels (ratings) and optional sample weights\n",
    "        labels = inputs.pop(\"labels\").float()  # Shape: (batch_size)\n",
    "        \n",
    "        # Optional: Compute sample weights dynamically\n",
    "        weights = self._get_sample_weights(labels) if self.weight_strategy else None\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits.squeeze()  # Shape: (batch_size) --> logits are the predicted rewards in this case\n",
    "        \n",
    "        # Custom loss calculation\n",
    "        loss = self._compute_custom_loss(logits, labels, weights)\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "    def _compute_custom_loss(self, logits, labels, weights=None):\n",
    "        if self.loss_type == \"mse\":\n",
    "            loss = F.mse_loss(logits, labels, reduction=\"none\") # --> MSE provides precise regression BUT sensitive to outliers\n",
    "        elif self.loss_type == \"huber\":\n",
    "            loss = F.huber_loss(logits, labels, reduction=\"none\", delta=1.0) #--> balances between MSE and MAE for data that has outliers/ noise\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported loss type: {self.loss_type}\")\n",
    "\n",
    "        # Apply sample weights if provided\n",
    "        if weights is not None:\n",
    "            loss = loss * weights\n",
    "            loss = loss.mean()  # Normalize by mean if weights are unnormalized\n",
    "        else:\n",
    "            loss = loss.mean()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def _get_sample_weights(self, labels):\n",
    "        \"\"\"\n",
    "        Generate sample weights based on rating values.\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        if self.weight_strategy == \"linear\":\n",
    "            # Linear weighting (e.g., emphasize extremes)\n",
    "            weights = torch.abs(labels - labels.mean()) + 1.0\n",
    "        elif self.weight_strategy == \"inverse\":\n",
    "            # Inverse frequency weighting (if ratings are skewed)\n",
    "            unique, counts = torch.unique(labels, return_counts=True)\n",
    "            freq = counts.float() / len(labels)\n",
    "            weight_map = 1.0 / (freq + 1e-6)  # Avoid division by zero\n",
    "            weights = torch.tensor([weight_map[(unique == lbl).nonzero().item()] for lbl in labels])\n",
    "        else:\n",
    "            weights = None\n",
    "        \n",
    "        return weights.to(labels.device) if weights is not None else None\n",
    "\n",
    "\n",
    "\n",
    "    def compute_metrics(self, eval_preds):\n",
    "        predictions, labels = eval_preds\n",
    "        predictions = predictions.squeeze()\n",
    "        \n",
    "        # Regression metrics\n",
    "        mse = mean_squared_error(labels, predictions)\n",
    "        pearson = pearsonr(labels, predictions)[0] # Pearson correlation coefficient\n",
    "        \n",
    "        # Threshold accuracy --> \n",
    "        tolerance_acc = (np.abs(predictions - labels) <= 0.5).mean()\n",
    "        \n",
    "        return {\"mse\": mse, \"pearson\": pearson, \"tolerance_acc\": tolerance_acc}\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    #TODO: evaluate whether the plotting should be done or whether it is redundant to add them\n",
    "\n",
    "    # def evaluation_loop(self, *args, **kwargs):\n",
    "    #     output = super().evaluation_loop(*args, **kwargs)\n",
    "    #     predictions = output.predictions.squeeze()\n",
    "    #     labels = output.label_ids\n",
    "        \n",
    "    #     # Generate plots (saved to disk or logged to W&B)\n",
    "    #     plot_distributions(predictions, labels, self.state.epoch)\n",
    "    #     plot_calibration(predictions, labels)\n",
    "        \n",
    "    #     return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: debug if this is truly needed...\n",
    "\n",
    "# add distributioncallback to trianing to evaluate \n",
    "\n",
    "# class DistributionCallback(TrainerCallback):\n",
    "#     def on_evaluate(self, args, state, control, metrics, **kwargs):\n",
    "#         # Get predictions and labels from the trainer's eval loop\n",
    "#         eval_results = trainer.evaluate()\n",
    "#         predictions = eval_results[\"eval_predictions\"]\n",
    "#         labels = eval_results[\"eval_labels\"]\n",
    "        \n",
    "#         # Log histogram to W&B\n",
    "#         wandb.log({\n",
    "#             \"reward_histogram\": wandb.Histogram(predictions),\n",
    "#             \"true_ratings_histogram\": wandb.Histogram(labels),\n",
    "#         })\n",
    "\n",
    "\n",
    "# def plot_distributions(predictions, labels, epoch):\n",
    "#     plt.figure(figsize=(10, 4))\n",
    "#     plt.subplot(1, 2, 1)\n",
    "#     plt.hist(predictions, bins=20, alpha=0.7, label=\"Predicted\")\n",
    "#     plt.title(\"Predicted Rewards\")\n",
    "    \n",
    "#     plt.subplot(1, 2, 2)\n",
    "#     plt.hist(labels, bins=20, alpha=0.7, label=\"True Ratings\", color=\"orange\")\n",
    "#     plt.title(\"True Ratings\")\n",
    "    \n",
    "#     plt.savefig(f\"distributions_epoch_{epoch}.png\")\n",
    "#     plt.close()\n",
    "\n",
    "# class PlotCallback(TrainerCallback):\n",
    "#     def on_evaluate(self, args, state, control, **kwargs):\n",
    "#         predictions = trainer.predict(test_dataset).predictions.squeeze()\n",
    "#         labels = test_dataset[\"ratings\"]\n",
    "#         plot_distributions(predictions, labels, state.epoch)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def plot_calibration(predictions, labels):\n",
    "#     \"\"\"\n",
    "#     Function to check if the sdfs\n",
    "\n",
    "    \n",
    "\n",
    "#     \"\"\"\n",
    "#     bin_means = np.linspace(1, 5, num=5)  # For 1-5 ratings\n",
    "#     bin_centers = []\n",
    "#     empirical_means = []\n",
    "    \n",
    "#     for i in range(len(bin_means) - 1):\n",
    "#         mask = (labels >= bin_means[i]) & (labels < bin_means[i+1])\n",
    "#         if mask.sum() > 0:\n",
    "#             bin_centers.append((bin_means[i] + bin_means[i+1]) / 2)\n",
    "#             empirical_means.append(predictions[mask].mean())\n",
    "    \n",
    "#     plt.plot(bin_centers, empirical_means, marker=\"o\")\n",
    "#     plt.plot([1, 5], [1, 5], linestyle=\"--\", color=\"gray\")  # Ideal line\n",
    "#     plt.xlabel(\"True Rating\")\n",
    "#     plt.ylabel(\"Predicted Reward\")\n",
    "#     plt.savefig(\"calibration_plot.png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. a) Define a custom data collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import DefaultDataCollator\n",
    "\n",
    "# #TODO: only do this if the labels fix does not work for some reason\n",
    "\n",
    "# class RewardDataCollator(DefaultDataCollator):\n",
    "#     def __call__(self, features):\n",
    "\n",
    "#         ratings = [f.pop(\"rating\") for f in features]  # Removes rating from features temporarily\n",
    "#         batch = super().__call__(features)\n",
    "#         # Explicitly ensure rating is included\n",
    "#         print(features)\n",
    "#         # Re-inject ratings into the batch\n",
    "#         batch[\"rating\"] = torch.tensor(ratings, dtype=torch.float32)\n",
    "#         return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Encode dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['file', 'frame_ID', 'frame_type', 'frame_text', 'precondition_id', 'precondition_text', 'precondition_position', 'response_text', 'prompt_config_examples', 'prompt_config_chain_of_thought', 'label', 'additional_feedback']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 929/929 [00:00<00:00, 15731.20 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2, 2, 2, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# if labels are not integers, convert them to integers\n",
    "def convert_label_to_int(data):\n",
    "    data[\"label\"] = int(data[\"label\"])\n",
    "    return data\n",
    "\n",
    "\n",
    "print(dataset.column_names)\n",
    "# mao string labels to integers\n",
    "dataset = dataset.map(convert_label_to_int)  # Assuming 'text' is the column with the text data\n",
    "\n",
    "print(dataset[\"label\"][:5])  # Check labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comment\n",
    "\n",
    "1. Needed for feedback extraction: precondition_text, response_text, label(rating feedback extraction)\n",
    "2. Needed for feedback detection: precondition_text, precondition_position, response_text, label (rating feedback detection)\n",
    "3. For the precondition position to be found well, it is a crucial for the model to find the precondition text (at least to a recognizable degree) as well, otherwise the precondition is not found at all..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/929 [00:00<?, ? examples/s]Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map: 100%|██████████| 929/929 [00:00<00:00, 2133.45 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# tokenize queries and answers together to provide proper context to reward model\n",
    "def tokenize_fn(examples):\n",
    "    combined_texts = [f\"{p} {a} {r}\" for p, a, r in zip(examples[\"precondition_text\"], examples[\"response_text\"], examples[\"label\"])]\n",
    "    # exceeded = 0\n",
    "    # not_exceeded = 0\n",
    "    # largest_context_window = 0\n",
    "    # for text in combined_texts:\n",
    "    #     encoded = tokenizer.encode(text, add_special_tokens=True, truncation=False)\n",
    "    #     if len(encoded) > tokenizer.model_max_length:\n",
    "    #         print(f\"Warning: Text exceeds max length ({len(encoded)} tokens). Truncating to {tokenizer.model_max_length} tokens.\")\n",
    "    #         # encoded = encoded[:tokenizer.model_max_length]\n",
    "    #         exceeded +=1\n",
    "    #         largest_context_window = max(largest_context_window, len(encoded))\n",
    "    #     else:\n",
    "    #         None\n",
    "    #         not_exceeded += 1\n",
    "    # print(f\"Total texts exceeding max length: {exceeded}\")\n",
    "    # print(f\"Total texts within max length: {not_exceeded}\")\n",
    "    # print(f\"Largest context window used: {largest_context_window} tokens\")\n",
    "    return tokenizer(combined_texts, truncation=True, padding=\"max_length\")\n",
    "\n",
    "dataset = dataset.map(tokenize_fn, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['file', 'frame_ID', 'frame_type', 'frame_text', 'precondition_id', 'precondition_text', 'precondition_position', 'response_text', 'prompt_config_examples', 'prompt_config_chain_of_thought', 'label', 'additional_feedback', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 929\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split dataset into train, test, eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train, test and eval sets\n",
    "train_test_split = dataset.train_test_split(test_size=0.3, seed=42)\n",
    "eval_test_split = train_test_split[\"test\"].train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "\n",
    "final_splits = DatasetDict({\n",
    "    'train': train_test_split['train'],\n",
    "    'validation': eval_test_split['train'],\n",
    "    'test': eval_test_split['test']\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train reward model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=LORA_CHECKPOINTS_FOLDER,\n",
    "    eval_strategy='steps',\n",
    "    save_strategy='steps',\n",
    "    save_steps=25,\n",
    "    eval_steps=25,\n",
    "    save_total_limit=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    learning_rate=4e-5,\n",
    "    num_train_epochs=20,\n",
    "    logging_steps=10,\n",
    "    label_names=[\"labels\"],\n",
    "    # report_to=\"none\",\n",
    "    logging_dir=\"./logs\",\n",
    "    fp16=True,  # Use mixed precision training\n",
    "    metric_for_best_model=\"eval_loss\", # or \"eval_loss\"\n",
    "    greater_is_better=False, # or False if using loss\n",
    "    gradient_accumulation_steps=4 # \n",
    ")\n",
    "\n",
    "# Initialize custom trainer\n",
    "trainer = CustomRewardTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=final_splits['train'],\n",
    "    eval_dataset=final_splits['validation'],\n",
    "    # compute_metrics=trainer.compute_metrics,  # Use the custom metrics function\n",
    "    processing_class=tokenizer,\n",
    "    loss_type=\"huber\",  # Try \"mse\" or \"huber\"\n",
    "    weight_strategy=\"linear\",  # Try \"linear\", \"inverse\", or None\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)] # use early stopping since we are sing high amount of epochs\n",
    "    # data_collator=RewardDataCollator()\n",
    ")\n",
    "\n",
    "# # add distributioncallback to trainer TODO: only integrate if relevant\n",
    "# trainer.add_callback(DistributionCallback())\n",
    "\n",
    "print(trainer.args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded to be a multiple of `config.attention_window`: 512\n",
      "/home/jacques.furst/miniconda3/envs/RL/lib/python3.10/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='675' max='820' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [675/820 16:59 < 03:39, 0.66 it/s, Epoch 16/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.556700</td>\n",
       "      <td>1.570197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.423000</td>\n",
       "      <td>1.533329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>1.505500</td>\n",
       "      <td>1.519240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.492900</td>\n",
       "      <td>1.515897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>1.522800</td>\n",
       "      <td>1.503899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.434800</td>\n",
       "      <td>1.503258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>1.490400</td>\n",
       "      <td>1.494745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.321700</td>\n",
       "      <td>1.490436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>1.385000</td>\n",
       "      <td>1.479945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.465200</td>\n",
       "      <td>1.486350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>1.181700</td>\n",
       "      <td>1.487106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.443200</td>\n",
       "      <td>1.479757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>1.502900</td>\n",
       "      <td>1.474601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.529500</td>\n",
       "      <td>1.472341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>1.275500</td>\n",
       "      <td>1.466285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.360300</td>\n",
       "      <td>1.463262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>1.434600</td>\n",
       "      <td>1.466928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.444000</td>\n",
       "      <td>1.461720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>1.368600</td>\n",
       "      <td>1.458223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.317300</td>\n",
       "      <td>1.462308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>525</td>\n",
       "      <td>1.446700</td>\n",
       "      <td>1.458889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.458800</td>\n",
       "      <td>1.457340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>575</td>\n",
       "      <td>1.195200</td>\n",
       "      <td>1.455970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.447900</td>\n",
       "      <td>1.453130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>625</td>\n",
       "      <td>1.243500</td>\n",
       "      <td>1.455746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>1.332600</td>\n",
       "      <td>1.456390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>675</td>\n",
       "      <td>1.569600</td>\n",
       "      <td>1.453590</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacques.furst/miniconda3/envs/RL/lib/python3.10/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/jacques.furst/miniconda3/envs/RL/lib/python3.10/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/jacques.furst/miniconda3/envs/RL/lib/python3.10/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/jacques.furst/miniconda3/envs/RL/lib/python3.10/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/jacques.furst/miniconda3/envs/RL/lib/python3.10/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/jacques.furst/miniconda3/envs/RL/lib/python3.10/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/jacques.furst/miniconda3/envs/RL/lib/python3.10/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/jacques.furst/miniconda3/envs/RL/lib/python3.10/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/jacques.furst/miniconda3/envs/RL/lib/python3.10/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/jacques.furst/miniconda3/envs/RL/lib/python3.10/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/jacques.furst/miniconda3/envs/RL/lib/python3.10/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/jacques.furst/miniconda3/envs/RL/lib/python3.10/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/jacques.furst/miniconda3/envs/RL/lib/python3.10/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/jacques.furst/miniconda3/envs/RL/lib/python3.10/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/jacques.furst/miniconda3/envs/RL/lib/python3.10/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/jacques.furst/miniconda3/envs/RL/lib/python3.10/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/jacques.furst/miniconda3/envs/RL/lib/python3.10/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/jacques.furst/miniconda3/envs/RL/lib/python3.10/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/jacques.furst/miniconda3/envs/RL/lib/python3.10/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/jacques.furst/miniconda3/envs/RL/lib/python3.10/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/jacques.furst/miniconda3/envs/RL/lib/python3.10/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/jacques.furst/miniconda3/envs/RL/lib/python3.10/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/jacques.furst/miniconda3/envs/RL/lib/python3.10/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/jacques.furst/miniconda3/envs/RL/lib/python3.10/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/jacques.furst/miniconda3/envs/RL/lib/python3.10/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/jacques.furst/miniconda3/envs/RL/lib/python3.10/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=675, training_loss=1.4234113325896087, metrics={'train_runtime': 1020.8527, 'train_samples_per_second': 12.734, 'train_steps_per_second': 0.803, 'total_flos': 1.7086001607942672e+16, 'train_loss': 1.4234113325896087, 'epoch': 16.466257668711656})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store final model parameters\n",
    "model.save_pretrained(FINAL_LORA_ADAPTERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reload saved LoRA adapter for inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "base_model = AutoModelForSequenceClassification.from_pretrained(MODEL, num_labels=1)\n",
    "model = PeftModel.from_pretrained(base_model, FINAL_LORA_ADAPTERS)\n",
    "model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='35' max='35' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [35/35 00:09]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results: {'eval_loss': 1.4142974615097046, 'eval_runtime': 9.2538, 'eval_samples_per_second': 15.129, 'eval_steps_per_second': 3.782, 'epoch': 16.466257668711656}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_results = trainer.evaluate(eval_dataset=final_splits['test'])\n",
    "print(\"Test Results:\", test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1: Predicted Rating: 0.6952641606330872, True Rating: 1\n",
      "Sample 2: Predicted Rating: 0.6451296806335449, True Rating: 0\n",
      "Sample 3: Predicted Rating: 0.7857125401496887, True Rating: 3\n",
      "Sample 4: Predicted Rating: 0.7282338738441467, True Rating: 0\n",
      "Sample 5: Predicted Rating: 1.0074602365493774, True Rating: 3\n",
      "Sample 6: Predicted Rating: 0.7218911051750183, True Rating: 0\n",
      "Sample 7: Predicted Rating: 0.8589215874671936, True Rating: 1\n",
      "Sample 8: Predicted Rating: 0.7128860354423523, True Rating: 0\n",
      "Sample 9: Predicted Rating: 1.4981894493103027, True Rating: 0\n",
      "Sample 10: Predicted Rating: 0.7824877500534058, True Rating: 0\n",
      "Sample 11: Predicted Rating: 0.698183000087738, True Rating: 0\n",
      "Sample 12: Predicted Rating: 1.5357125997543335, True Rating: 0\n",
      "Sample 13: Predicted Rating: 1.1745346784591675, True Rating: 0\n",
      "Sample 14: Predicted Rating: 1.4537135362625122, True Rating: 1\n",
      "Sample 15: Predicted Rating: 0.7609323263168335, True Rating: 0\n",
      "Sample 16: Predicted Rating: 0.750640869140625, True Rating: 0\n",
      "Sample 17: Predicted Rating: 0.8589215874671936, True Rating: 3\n",
      "Sample 18: Predicted Rating: 0.9474672079086304, True Rating: 3\n",
      "Sample 19: Predicted Rating: 0.6500369906425476, True Rating: 1\n",
      "Sample 20: Predicted Rating: 0.8249042630195618, True Rating: 3\n"
     ]
    }
   ],
   "source": [
    "# evaluate model manually on some test cases\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(20):\n",
    "        sample = final_splits['test'][i]\n",
    "        inputs = tokenizer(sample['precondition_text'] + \" \" + sample['response_text'], return_tensors='pt', truncation=True, padding=\"max_length\").to(device)\n",
    "        outputs = model(**inputs)\n",
    "        prediction = outputs.logits.item()\n",
    "        print(f\"Sample {i+1}: Predicted Rating: {prediction}, True Rating: {sample['label']}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
